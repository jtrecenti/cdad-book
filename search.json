[
  {
    "objectID": "02-organizacao-dados-taxas.html",
    "href": "02-organizacao-dados-taxas.html",
    "title": "2  Organização de dados, frequências, proporções e taxas",
    "section": "",
    "text": "2.1 Introdução\nNessa análise, nosso interesse é estudar as decisões do TJSP em apelações criminais. Vamos estudar as quantidades, proporções e distribuições de decisões de acordo com as categorias de decisões.\nPor que isso é importante? A maior parte dos dados no direito envolvem variáveis categóricas. Entender como esse tipo de dados funciona é fundamental para entender o que podemos e não podemos fazer nas nossas análises.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Organização de dados, frequências, proporções e taxas</span>"
    ]
  },
  {
    "objectID": "02-organizacao-dados-taxas.html#conceitos",
    "href": "02-organizacao-dados-taxas.html#conceitos",
    "title": "2  Organização de dados, frequências, proporções e taxas",
    "section": "2.2 Conceitos",
    "text": "2.2 Conceitos\nPara cada conceito, vamos mostrar a definição, a fórmula matemática e o código em python para calcular.\n\n2.2.1 Frequência\n\n2.2.1.1 Definição\nA frequência é a quantidade de ocorrências de uma observação, objeto, pessoa, processo, etc. Geralmente, está atrelada a variáveis categóricas, tanto nominais quanto ordinais.\n\n\n2.2.1.2 Fórmula\nA fórmula aqui é bem simples: basta contar a quantidade de vezes que cada categoria aparece. Geralmente, denotamos a frequência pela variável \\(n_i\\), sendo \\(i\\) a categoria.\n\\[n_i = \\text{número de vezes que a categoria } i \\text{ aparece}\\]\n\n\n2.2.1.3 Código\nA frequência de uma variável pode ser rapidamente obtida com value_counts()\n\ncamaras.decisao.value_counts()\n\ndecisao\nNegaram                 5273\nParcialmente            3691\nProvido                  716\nPunibilidade Extinta     238\nOutros                    66\nNão conhecido             16\nName: count, dtype: int64\n\n\nAqui, \\(n_1\\) é 5273, \\(n_2\\) é 3691 e assim por diante.\n\n\n\n2.2.2 Frequência relativa\n\n2.2.2.1 Definição\nA frequência relativa é a proporção de ocorrências de uma observação, objeto, pessoa, processo, etc. em relação ao total de ocorrências. Também está atrelada a variáveis categóricas, tanto nominais quanto ordinais.\n\n\n2.2.2.2 Fórmula\nBasta dividir a frequência da categoria pela quantidade total de observações na amostra (\\(N\\)). Denotamos a frequência relativa pela variável \\(f_i\\), sendo \\(i\\) a categoria.\n\\[f_i = \\frac{n_i}{N}\\]\n\n\n2.2.2.3 Código\nO argumento ‘normalize=’ permite obter a frequência relativa - não o total de ocorrências.\n\ncamaras.decisao.value_counts(normalize=True)\n\ndecisao\nNegaram                 0.5273\nParcialmente            0.3691\nProvido                 0.0716\nPunibilidade Extinta    0.0238\nOutros                  0.0066\nNão conhecido           0.0016\nName: proportion, dtype: float64\n\n\nA frequência relativa, quando expressa na forma de porcentagem, também é chamada de proporção. A proporção de decisões negadas dentre todas as decisões é de 0.5273 (frequência relativa) ou 52.73% (proporção).\nObs: Você não precisa se prender muito a essa distinção rígida de frequência relativa e proporção. No mundo real, as pessoas usam os termos de forma intercambiável, e não há problema nenhum nisso.\nObs2: Quando calculamos todas as proporções/frequências relativas de uma variável categórica, a soma delas é 1 (ou 100%, se expressa em porcentagem). Isso é uma propriedade das proporções.\nObs3: Quando mostramos todas as proporções/frequências relativas de uma variável categórica, estamos mostrando a distribuição da variável. A distribuição é a forma como as categorias estão distribuídas em relação ao total de observações. Falaremos mais sobre isso quando discutirmos os conceitos de estatística e probabilidade.\n\n\n\n2.2.3 Razão\nVocê percebeu que, para fazer a proporção, dividimos uma frequência pela quantidade total de observações? Isso é uma razão.\n\n2.2.3.1 Definição\nA razão é a divisão entre duas quantidades de interesse. No caso da frequência relativa, estamos comparando a quantidade de vezes que uma categoria aparece com a quantidade total de observações. Mas a razão pode ser usada para comparar qualquer coisa.\n\n\n2.2.3.2 Fórmula\nBasta dividir a quantidade de interesse pela outra quantidade de interesse. Denotamos a razão pela variável \\(r\\).\n\\[r = \\frac{a}{b}\\]\nNa fórmula, \\(a\\) e \\(b\\) podem ser quaisquer números. No caso da frequência relativa, \\(a\\) é a quantidade de vezes que uma categoria aparece e \\(b\\) é a quantidade total de observações.\n\n\n2.2.3.3 Código\nA razão é obtida dividindo-se os dois números de interesse. Por exemplo, se quisermos saber quantos recursos relacionados a tráfico de drogas temos para cada recurso relacionado a furto, basta dividir a frequência de recursos de tráfico pela frequência de recursos de furto.\n\npd.set_option('display.max_colwidth', None) # Mostrar todas as colunas\n\ncamaras.value_counts('assunto').reset_index()\n\n\n\n\n\n\n\n\nassunto\ncount\n\n\n\n\n0\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Tráfico Ilícito e Uso Indevido de Drogas - Tráfico de Drogas e Condutas Afins\n3082\n\n\n1\nDIREITO PENAL - Crimes contra o Patrimônio - Roubo Majorado\n1397\n\n\n2\nDIREITO PENAL - Crimes contra o Patrimônio - Furto Qualificado\n1122\n\n\n3\nDIREITO PENAL-Crimes contra o Patrimônio-Furto\n577\n\n\n4\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Trânsito\n458\n\n\n...\n...\n...\n\n\n115\nDIREITO PENAL - Crimes Praticados por Particular Contra a Administração em Geral - Subtração ou inutilização de livro ou documento\n1\n\n\n116\nDIREITO PENAL - Crimes Contra a Administração da Justiça - Favorecimento real\n1\n\n\n117\nDIREITO PENAL - Crimes Praticados por Particular Contra a Administração em Geral - \"Lavagem\" ou Ocultação de Bens, Direitos ou Valores Oriundos de Corrupção\n1\n\n\n118\nDIREITO PENAL - Crimes Praticados por Particular Contra a Administração em Geral - Contrabando ou descaminho\n1\n\n\n119\nCrimes Resultante de Preconceito de Raça ou de Cor\n1\n\n\n\n\n120 rows × 2 columns\n\n\n\n\nassuntos_trafico = [\n  'DIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Tráfico Ilícito e Uso Indevido de Drogas - Tráfico de Drogas e Condutas Afins'\n]\n\nassuntos_furto = [\n  'DIREITO PENAL - Crimes contra o Patrimônio - Furto Qualificado',\n  'DIREITO PENAL-Crimes contra o Patrimônio-Furto'\n]\n\nqt_trafico = camaras[camaras.assunto.isin(assuntos_trafico)].shape[0]\n\nqt_furto = camaras[camaras.assunto.isin(assuntos_furto)].shape[0]\n\nrazao = qt_trafico / qt_furto\n\nqt_trafico, qt_furto, razao\n\n(3082, 1699, 1.8140082401412596)\n\n\nTemos aproximadamente 1.81 recursos relacionados a tráfico para cada recurso relacionado a furto.\nTambém podemos representar as razões multiplicando por valores constantes (usualmente potências de 10, como 100, 1000, etc). Por exemplo, a razão acima poderia ser expressa como 181 casos de tráfico para cada 100 casos de furto, ou 181:100.\n\n\n\n2.2.4 Taxa\n\n2.2.4.1 Definição\nNo inglês, rate é a divisão entre dois números com unidades de medidas diferentes (por exemplo, km/h). Já a razão ratio é a divisão entre dois números com a mesma unidade de medida (como o exemplo mostrado acima).\nNa prática, muitas vezes chamamos uma razão de taxa, mesmo que os números tenham a mesma unidade de medida. Isso acontece por vários motivos: um é a comodidade e questões culturais mesmo; outro é que na área de epidemiologia, a definição de taxa geralmente está atrelada a um denominador que varia no tempo (como, por exemplo, taxa de mortalidade, que varia no tempo porque a população varia no tempo).\nUm exemplo, muito usado em administração judiciária, é a taxa de congestionamento. Aqui, o numerador é o número de processos remanescentes no tribunal – fazendo-se a diferença entre o total de processos (novos + pendentes) e o total de processos baixados – e o denominador é o número total de processos (novos + pendentes).\nOutro exemplo é a taxa de reforma. O numerador é o número de decisões favoráveis ao recurso e o denominador é o número total de recursos baixados.\nObs: existem variações para a definição da taxa de reforma. Um exemplo é considerar no denominador apenas as decisões que poderiam ser reformadas, ou seja, as decisões de mérito.\n\n\n2.2.4.2 Fórmula\nA fórmula é a mesma da razão, mas a interpretação é diferente. Denotamos a taxa pela variável \\(t\\).\n\\[t = \\frac{a}{b}\\]\nNa fórmula, \\(a\\) e \\(b\\) podem ser quaisquer números. No caso da taxa de reforma, \\(a\\) é o número de decisões favoráveis e desfavoráveis e \\(b\\) é o número total de decisões.\n\n\n2.2.4.3 Código\nA taxa é obtida dividindo-se os dois números de interesse. Por exemplo, se quisermos saber a taxa de reforma, basta dividir a frequência de decisões favoráveis pela frequência de decisões (total ou de mérito).\n\nqt_decisoes_total = camaras.shape[0]\n\nnm_decisoes_merito = ['Negaram', 'Parcialmente', 'Provido']\n\nqt_decisoes_merito = camaras[camaras.decisao.isin(nm_decisoes_merito)].shape[0]\nqt_decisoes_favoraveis = camaras[camaras.decisao.isin(['Parcialmente', 'Provido'])].shape[0]\n\ntx_reforma_total = qt_decisoes_favoraveis / qt_decisoes_total\ntx_reforma_merito = qt_decisoes_favoraveis / qt_decisoes_merito\n\ntx_reforma_total, tx_reforma_merito\n\n(0.4407, 0.45526859504132233)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Organização de dados, frequências, proporções e taxas</span>"
    ]
  },
  {
    "objectID": "02-organizacao-dados-taxas.html#aplicação",
    "href": "02-organizacao-dados-taxas.html#aplicação",
    "title": "2  Organização de dados, frequências, proporções e taxas",
    "section": "2.3 Aplicação",
    "text": "2.3 Aplicação\nA quantidade ou proporção de recursos negados nas câmaras criminais pode variar de acordo com o polo do Ministério Público no caso. Afinal, se um magistrado tem um viés mais ou menos favorável ao MP, a quantidade de recursos negados pode variar.\n\nlen(camaras[(camaras.polo_mp == \"Ativo\") & (camaras.decisao == \"Negaram\")])\n\n358\n\n\n\nlen(camaras[(camaras.polo_mp == \"Passivo\") & (camaras.decisao == \"Negaram\")])\n\n4915\n\n\nA frequência de recursos negados nos casos em que o MP recorreu (polo ativo) é 358. Quando o MP está no polo passivo, esse número é de 4915. O que isso revela sobre o viés do tribunal com relação ao MP? O fato de existirem menos recursos negados quando o MP recorre pode ser um indício de que o tribunal é mais favorável ao MP?\nTalvez a diferença seja apenas uma questão de quantidade de recursos interpostos pelo MP em relação ao polo passivo. Afinal, se o MP recorre em menos casos, é natural que haja menos recursos negados.\nA proporção de recursos negados para cada tipo de polo do MP é o que de fato deve ser comparado nesse caso.\n\nfreq = len(camaras[(camaras.polo_mp == \"Ativo\") & (camaras.decisao == \"Negaram\")])\nN = len(camaras[camaras.polo_mp == \"Ativo\"])\n\n\nP_ativo = freq / N\nP_ativo\n\n0.44143033292231815\n\n\n\nfreq = len(camaras[(camaras.polo_mp == \"Passivo\") & (camaras.decisao == \"Negaram\")])\nN = len(camaras[camaras.polo_mp == \"Passivo\"])\n\n\nP_passivo = freq / N\nP_passivo\n\n0.5348786592665143\n\n\nA proporção de negados quando o MP está no polo passivo é maior do que a proporção de negados no polo ativo. Isso pode ser um indício de que o tribunal é mais favorável ao MP quando este recorre. Claro que ainda existem muitos fatores a serem considerados (veremos mais para frente!).\nA partir de agora, vamos considerar somente os recursos onde o MP está no polo passivo. Também vamos considerar somente as decisões de mérito.\n\ncamaras_mp_passivo = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('decisao == [\"Negaram\", \"Parcialmente\", \"Provido\"]')\n)\n\nUma análise de interesse pode ser calcular a taxa de reforma por assunto do processo. Será que existe muita variação?\nNa análise abaixo, estimamos essas proporções, considerando apenas os assuntos com 30 ou mais decisões (tente pensar no motivo dessa decisão).\n\n(\n  camaras_mp_passivo\n  .groupby('assunto')\n  .agg(\n    decisoes = ('decisao', 'count'),\n    reforma = ('decisao', lambda x: (x.isin(['Provido', 'Parcialmente'])).mean())\n  )\n  .reset_index()\n  .query('decisoes &gt;= 30')\n  .sort_values('reforma', ascending=False)\n)\n\n\n\n\n\n\n\n\nassunto\ndecisoes\nreforma\n\n\n\n\n60\nDIREITO PENAL - Crimes contra o Patrimônio - Latrocínio\n44\n0.568182\n\n\n38\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Trânsito\n394\n0.530457\n\n\n98\nDIREITO PENAL-Crimes contra o Patrimônio-Estelionato\n178\n0.511236\n\n\n58\nDIREITO PENAL - Crimes contra o Patrimônio - Furto Qualificado\n1008\n0.497024\n\n\n52\nDIREITO PENAL - Crimes contra a Fé Pública - Uso de documento falso\n84\n0.488095\n\n\n63\nDIREITO PENAL - Crimes contra o Patrimônio - Roubo Majorado\n1296\n0.475309\n\n\n37\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Tráfico Ilícito e Uso Indevido de Drogas - Tráfico de Drogas e Condutas Afins\n2787\n0.454970\n\n\n96\nDIREITO PENAL-Crimes contra o Patrimônio-Apropriação indébita\n66\n0.439394\n\n\n101\nDIREITO PENAL-Crimes contra o Patrimônio-Furto\n486\n0.413580\n\n\n93\nDIREITO PENAL-Crimes contra a vida-Homicídio Qualificado\n77\n0.402597\n\n\n68\nDIREITO PENAL - Lesão Corporal - Grave\n35\n0.400000\n\n\n39\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes do Sistema Nacional de Armas\n322\n0.397516\n\n\n102\nDIREITO PENAL-Crimes contra o Patrimônio-Receptação\n385\n0.397403\n\n\n103\nDIREITO PENAL-Crimes contra o Patrimônio-Roubo\n391\n0.388747\n\n\n48\nDIREITO PENAL - Crimes contra a Fé Pública - Falsificação de documento público\n40\n0.375000\n\n\n89\nDIREITO PENAL-Crimes contra a liberdade pessoal-Ameaça\n229\n0.371179\n\n\n65\nDIREITO PENAL - Lesão Corporal - Decorrente de Violência Doméstica\n193\n0.347150\n\n\n66\nDIREITO PENAL - Lesão Corporal - Decorrente de Violência Doméstica-Contra a Mulher\n184\n0.336957\n\n\n1\nDIREITO PENAL - Contravenções Penais\n45\n0.333333\n\n\n86\nDIREITO PENAL-Crimes contra a Propriedade Intelectual-Violação de direito autoral\n37\n0.297297\n\n\n\n\n\n\n\nPela tabela, vemos que a taxa de reforma varia de 0.30 a 0.57. O assunto com maior taxa de reforma é o latrocínio, seguido por crimes de trânsito. Já o assunto com menor taxa de reforma é a violação de direito autoral.\nOutra análise de interesse é estudar a taxa de reforma por câmara. A tabela abaixo mostra isso. dessa vez, estamos mostrando apenas as câmaras com mais de 100 observações, para remover as câmaras extraordinárias.\n\ncamara_reforma = (\n  camaras_mp_passivo\n  .groupby('camara')\n  .agg(\n    decisoes = ('decisao', 'count'),\n    reforma = ('decisao', lambda x: (x.isin(['Provido', 'Parcialmente'])).mean())\n  )\n  .reset_index()\n  .query('decisoes &gt;= 100')\n  .sort_values('reforma', ascending=False)\n)\n\ncamara_reforma\n\nDessa vez, a taxa de reforma varia de 0.12 até 0.79! Pela tabela, a câmara com maior taxa de reforma é a 12 e a câmara com menor taxa de reforma é a 4.\nPodemos ver esses dados graficamente:\n\nsns.barplot(data=camara_reforma, x='reforma', y='camara')\n\n\n\n\n\n\n\n\nNa apostila sobre visualização de dados, veremos como customizar esse gráfico.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Organização de dados, frequências, proporções e taxas</span>"
    ]
  },
  {
    "objectID": "03-medidas-posicao-variabilidade.html",
    "href": "03-medidas-posicao-variabilidade.html",
    "title": "3  Medidas de posição e variabilidade",
    "section": "",
    "text": "3.1 Introdução\nNessa análise, nosso interesse é estudar os tempos das decisões do TJSP em apelações criminais. Para isso, vamos entender o que é distribuição, o que é uma estatística e como podemos retirar estatísticas de interesse dos nossos dados.\nPor que isso é importante? As medidas resumo servem para facilitar nosso entendimento a respeito de uma variável quantitativa de interesse. Elas aparecem em todo lugar, mesmo no ramo do direito. Por exemplo, quando um advogado quer saber quanto tempo demora um processo, ele está interessado em medidas resumo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Medidas de posição e variabilidade</span>"
    ]
  },
  {
    "objectID": "03-medidas-posicao-variabilidade.html#medidas-de-posição-conceitos",
    "href": "03-medidas-posicao-variabilidade.html#medidas-de-posição-conceitos",
    "title": "3  Medidas de posição e variabilidade",
    "section": "3.2 Medidas de posição: conceitos",
    "text": "3.2 Medidas de posição: conceitos\nPara cada conceito, vamos mostrar a definição, a fórmula matemática e o código em python para calcular.\n\n3.2.1 Média\n\n3.2.1.1 Definição\nA média é a soma de todos os valores dividido pela quantidade de valores. Ela é considerada uma medida de tendência central. Está muito presente no nosso dia a dia, como por exemplo, quando queremos saber a média de idade de um grupo de pessoas.\n\n\n3.2.1.2 Fórmula\nA fórmula da média é dada por:\n\\[\\bar{x} = \\frac{x_1 + x_2 + \\dots + x_N}{N} = \\frac{1}{N}\\sum_{i=1}^{N} x_i\\]\nNessa fórmula, o símbolo \\(\\bar{x}\\) é a notação para a média.\nO símbolo \\(x_i\\) é o valor da variável quantitativa de interesse para determinado processo \\(i\\), enquanto \\(N\\) é a quantidade de valores. Por exemplo, \\(x_3\\) é o tempo de decisão do processo 3 na nossa base de dados.\nJá o símbolo \\(\\sum\\) pode assustar. Trata-se da notação para a soma. Lemos ela assim: “soma de \\(i\\) igual \\(1\\) até \\(N\\) de \\(x_i\\)”. Como muita coisa na matemática, é só uma forma de economizar esforço e tinta para expressar algo que queremos.\n\n\n3.2.1.3 Código\nA média pode ser calculada em python usando a função mean do pacote numpy, ou então dentro do próprio pandas, com o método .mean().\n\nmean_pandas = camaras.tempo.mean()\nmean_numpy = np.mean(camaras.tempo)\n\nmean_pandas, mean_numpy\n\n(3.1788413612066204, 3.1788413612066204)\n\n\nVeja que essa conta é equivalente a calcular a soma de todos os valores e dividir pela quantidade de valores.\n\nsoma = np.sum(camaras.tempo)\nquantidade = len(camaras)\n\nsoma / quantidade\n\n3.1785234770704998\n\n\nA diferença na quarta casa decimal se dá por conta da precisão dos números em ponto flutuante.\n\n\n\n3.2.2 Mediana\n\n3.2.2.1 Definição\nA mediana é o valor que divide a amostra em duas partes iguais. Ou seja, 50 % dos valores são menores que a mediana e 50 % dos valores são maiores que a mediana.\nA mediana é calculada ordenando os valores e escolhendo o valor do meio. Se a quantidade de valores for par, a mediana é a média dos dois valores centrais.\nA principal diferença entre média e mediana é que a média é sensível a valores extremos, enquanto a mediana não. Por exemplo, a média dos valores [1, 2, 3, 4 e 100] é 22, enquanto a mediana é 3.\nNo direito, a mediana é muito importante porque estamos sempre trabalhando com variáveis que assumem valores extremos. Por exemplo, o tempo de decisão de um processo pode ser muito longo, o que afeta a média. Outro exemplo são os valores: a maioria dos processos têm valores envolvidos na ordem de milhares de reais, mas também é comum se deparar com processos com valores muito altos, na ordem de milhões ou bilhões.\n\n\n3.2.2.2 Fórmula\nA mediana é o valor que divide a amostra em duas partes iguais. Se a quantidade de valores for ímpar, a mediana é o valor do meio. Se a quantidade de valores for par, a mediana é a média dos dois valores centrais.\nPara mostrar isso matematicamente, introduzimos o conceito de postos (em inglês, rank). O posto de um valor é a sua posição na amostra, quando ela está ordenada. Por exemplo, na amostra [1, 2, 100, 3, 4], \\(x_{(3)}\\) é 3 (leia assim: a lista ordenada na 3ª posição é 3), enquanto \\(x_{(5)}\\) é 100 (a lista ordenada na 5ª posição).\nAssim, a mediana é o posto do meio. Mas o que é o meio? Bem, em uma amostra com \\(N\\) valores, o meio é o valor \\(\\frac{N+1}{2}\\), se \\(N\\) for ímpar. Se \\(N\\) for par, o meio é a média dos valores \\(\\frac{N}{2}\\) e \\(\\frac{N}{2}+1\\).\nPor exemplo, se \\(N = 31\\), o meio é o valor na posição \\(\\frac{31+1}{2} = 16\\), ou seja, \\(x_{(16)}\\). Se \\(N = 30\\), o meio é a média dos valores nas posições \\(\\frac{30}{2} = 15\\) e \\(\\frac{30}{2}+1 = 16\\), ou seja, \\((x_{(15)})+(x_{(16)}))/2\\).\nAssim, temos:\n\\[md = \\begin{cases} x_{((N+1)/2)} & \\text{se } N \\text{ é ímpar} \\\\ \\frac{x_{(N/2)}+x_{(N/2+1)}}{2} & \\text{se } N \\text{ é par} \\end{cases}\\]\nComo você pode ver, a fórmula nesse caso é muito mais difícil do que o cálculo em si. A dificuldade na notação está em expressar o que queremos de forma geral, para qualquer amostra, seja ela par ou ímpar.\n\n\n3.2.2.3 Código\nA mediana pode ser calculada em python usando a função median do pacote numpy, ou então dentro do próprio pandas, com o método .median().\n\nmedian_pandas = camaras.tempo.median()\nmedian_numpy = np.median(camaras.tempo)\n\nmedian_pandas, median_numpy\n\n(2.529774127310062, nan)\n\n\nOpa, o que significa esse nan? Bem, no numpy, nan é a sigla para “not a number”. Isso acontece quando tentamos calcular a mediana de uma variável que tem valores faltantes. O pandas, por outro lado, ignora os valores faltantes e calcula a mediana normalmente. Tome cuidado com isso!\nPara calcular a mediana ignorando os valores faltantes, podemos usar a função nanmedian(), como abaixo\n\nmedian_numpy = np.nanmedian(camaras.tempo)\n\nmedian_pandas, median_numpy\n\n(2.529774127310062, 2.529774127310062)\n\n\nObs: note que a mediana deu um valor menor do que a média. Isso é esperado, pois a mediana é menos sensível a valores extremos.\n\n\n\n3.2.3 Quantis\nVocê percebeu que a mediana é a posição central da lista de dados depois de ordenada? E se considerarmos outras posições dessa lista? Por exemplo, o que está na posição que separa os 25% menores tempos? Ou os 75% menores tempos? Esses valores são chamados de quantis.\n\n3.2.3.1 Definição\nOs quantis são os valores que dividem a amostra em partes iguais. O quantil de ordem \\(p\\), com \\(p\\) entre 0 e 1, é o valor que deixa \\(p\\) dos valores menores que ele e \\((1-p)\\) dos valores maiores que ele.\nNo caso da mediana, temos a questão dos pares e ímpares. Aqui, isso também acontece. A verdade é que existem várias formas de calcular os quantis, de acordo com como escolhemos os valores quando o \\(p\\)% de interesse cai entre uma e outra observação. Mas a verdade é que a diferença entre essas técnicas é tão pequena que não vale à pena discutirmos isso.\nNa prática, existem alguns quantis que geralmente damos mais atenção. O primeiro deles é o quantil de 0%, que nada mais é do que o menor valor da amostra (o mínimo). Outro é o quantil de 100%, também conhecido como máximo. A mediana é o quantil de 50%. Os quantis de 25% e 75% também são muito comuns, e são chamados de quartis.\n\n\n3.2.3.2 Fórmula\nA fórmula para calcular o quantil de ordem \\(p\\) é dada por:\n\\[q(p) = \\begin{cases} x_{(k)} & \\text{se } k \\text{ é inteiro} \\\\ x_{(k)} + (x_{(k+1)}-x_{(k)})\\cdot (pN - k) & \\text{se } k \\text{ não é inteiro} \\end{cases}\\]\nOu seja, se ao pegar o \\(p\\) da amostra, caímos exatamente em um valor, o quantil é esse valor. Se não, o quantil é uma interpolação (uma média com pesos) dos valores nas posições \\(k\\) e \\(k+1\\), onde \\(k\\) é a parte inteira do número de observações que queremos pegar.\n\n\n3.2.3.3 Código\nOs quantis podem ser calculados em python usando a função quantile do pacote numpy, ou então dentro do próprio pandas, com o método .quantile().\n\nq20_pandas = camaras.tempo.quantile(0.2)\n\nq20_numpy = np.quantile(camaras.tempo, 0.2)\n\nq20_pandas, q20_numpy\n\n(1.5742642026009583, nan)\n\n\nNote que, assim como no caso da mediana, o numpy retorna nan quando tentamos calcular quantis de uma variável que tem valores faltantes. O pandas, por outro lado, ignora os valores faltantes e calcula os quantis normalmente.\n\n# quantil de 20%\n# interpretação: 20% dos tempos são menores que esse valor encontrado\nq20_numpy = np.nanquantile(camaras.tempo, 0.2)\n\nq20_pandas, q20_numpy\n\n(1.5742642026009583, 1.5742642026009583)\n\n\n\n# mínimo: p=0\n\nmin_pandas = camaras.tempo.min()\nmin_numpy = np.min(camaras.tempo)\n\n# usando quantis\n\nqmin_pandas = camaras.tempo.quantile(0)\nqmin_numpy = np.nanquantile(camaras.tempo, 0)\n\nmin_pandas, min_numpy, qmin_pandas, qmin_numpy\n\n(0.2600958247775496,\n 0.2600958247775496,\n 0.2600958247775496,\n 0.2600958247775496)\n\n\n\n# máximo: p=1\n\nmax_pandas = camaras.tempo.max()\nmax_numpy = np.max(camaras.tempo)\n\n# usando quantis\n\nqmax_pandas = camaras.tempo.quantile(1)\nqmax_numpy = np.nanquantile(camaras.tempo, 1)\n\nmax_pandas, max_numpy, qmax_pandas, qmax_numpy\n\n(27.49075975359343, 27.49075975359343, 27.49075975359343, 27.49075975359343)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Medidas de posição e variabilidade</span>"
    ]
  },
  {
    "objectID": "03-medidas-posicao-variabilidade.html#medidas-de-posição-aplicação",
    "href": "03-medidas-posicao-variabilidade.html#medidas-de-posição-aplicação",
    "title": "3  Medidas de posição e variabilidade",
    "section": "3.3 Medidas de posição: aplicação",
    "text": "3.3 Medidas de posição: aplicação\nAgora vamos aplicar os conceitos aprendidos para entender melhor os tempos de decisão do TJSP.\nPrimeiro, os tempos apresentados estão em anos, e são os tempos entre o ajuizamento e a decisão do recurso de apelação.\n\ncamaras.tempo.describe()\n\ncount    9999.000000\nmean        3.178841\nstd         2.182874\nmin         0.260096\n25%         1.705681\n50%         2.529774\n75%         3.931554\nmax        27.490760\nName: tempo, dtype: float64\n\n\nO método .describe() já calcula várias coisas pra gente! Primeiro, ele nos dá a quantidade de observações não nulas, que é 9999 das 10000. A média é 3.17, que dá aproximadamente 3 anos e 2 meses (para verificar a quantidade de meses, multiplique 0.17 * 12 e aproxime para o número mais próximo). Veremos o std (standard deviation) depois.\nA mediana é de 2.53, ou seja, 2 anos e 6 meses. Olhando para os quartis, temos que 50% dos processos têm entre 1.71 ano (1 ano e 8 meses) e 3.93 anos (3 anos e 11 meses). Ou seja, dependendo do ponto de vista, até que são rápidos, considerando a conhecida morosidade do judiciário e que esse tempo leva em consideração a primeira e segunda instâncias combinadas.\nObs: Essa é uma medida de variabilidade chamada intervalo interquartil, uma forma de entender a dispersão dos dados. Veremos essa medida com mais detalhes em seguida.\nO tempo máximo encontrado foi de 27.49 anos (27 anos e 6 meses). Isso é muito tempo! Vamos dar uma olhada nesse caso:\n\ncamaras[camaras.tempo &gt; 27]\n\n\n\n\n\n\n\n\nprocesso\nassunto\ncamara\nrelator\norigem\ncomarca\npolo_mp\ndecisao\nunanimidade\ndt_publicacao\nementa\ntempo\nrel_idade\nrel_id_municipio_nasc\nrel_faculdade_direito\nrel_tempo_magistratura\nrel_tipo_magistrado\nrel_quinto\n\n\n\n\n8991\n04526037019918260011\nDIREITO PENAL-Crimes contra a vida-Homicídio Q...\n01ª Câmara de Direito Criminal\nDINIZ FERNANDO\nComarca de São Paulo / Foro Regional de Pinhei...\nSAO PAULO\nPassivo\nNegaram\nUnânime\n2018-06-29\nHOMICÍDIO QUALIFICADO. Apelo fundamentado no a...\n27.49076\nNaN\nNaN\nNaN\nNaN\njuiz substituto 2º grau\nNaN\n\n\n\n\n\n\n\nVeja, pelo número do processo, que esse caso foi distribuído em 1991. Como esperado, é um caso complexo. Trata-se de um homicídio qualificado.\nSerá, então, que os tempos típicos dos processos variam conforme o assunto? Vamos analisar isso agora.\n\npd.set_option('display.max_colwidth', None) # Mostrar todas as colunas\n\n(\n  camaras\n  .groupby('assunto')['tempo']\n  .describe()\n  .reset_index()\n  .query('count &gt;= 100')\n  .sort_values('50%', ascending=False)\n)\n\n\n\n\n\n\n\n\nassunto\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n105\nDIREITO PENAL-Crimes contra o Patrimônio-Estelionato\n220.0\n6.210591\n3.287898\n0.813142\n3.856947\n5.778234\n8.112936\n21.440110\n\n\n41\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Trânsito\n458.0\n4.367793\n2.104371\n0.626968\n2.763176\n3.908282\n5.548255\n14.579055\n\n\n69\nDIREITO PENAL - Lesão Corporal - Decorrente de Violência Doméstica\n222.0\n3.790949\n1.329072\n1.122519\n2.779603\n3.579740\n4.754962\n8.323066\n\n\n109\nDIREITO PENAL-Crimes contra o Patrimônio-Receptação\n439.0\n3.745380\n2.091780\n0.418891\n2.123203\n3.329227\n4.899384\n10.603696\n\n\n108\nDIREITO PENAL-Crimes contra o Patrimônio-Furto\n577.0\n3.487429\n2.050025\n0.260096\n2.088980\n3.123888\n4.490075\n17.210130\n\n\n94\nDIREITO PENAL-Crimes contra a liberdade pessoal-Ameaça\n248.0\n3.194067\n1.459880\n0.678987\n2.084873\n2.926762\n4.145791\n8.134155\n\n\n62\nDIREITO PENAL - Crimes contra o Patrimônio - Furto Qualificado\n1122.0\n3.455517\n2.168286\n0.375086\n1.898700\n2.926762\n4.511978\n16.459959\n\n\n42\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes do Sistema Nacional de Armas\n355.0\n3.291969\n1.735908\n0.547570\n2.110883\n2.910335\n4.179329\n12.610541\n\n\n70\nDIREITO PENAL - Lesão Corporal - Decorrente de Violência Doméstica-Contra a Mulher\n204.0\n3.277932\n1.686380\n0.862423\n2.107461\n2.818617\n4.150582\n10.398357\n\n\n67\nDIREITO PENAL - Crimes contra o Patrimônio - Roubo Majorado\n1397.0\n2.824282\n1.956017\n0.323066\n1.620808\n2.310746\n3.258042\n19.529090\n\n\n110\nDIREITO PENAL-Crimes contra o Patrimônio-Roubo\n422.0\n2.431815\n1.785516\n0.465435\n1.425051\n1.924709\n2.823409\n17.289528\n\n\n40\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Tráfico Ilícito e Uso Indevido de Drogas - Tráfico de Drogas e Condutas Afins\n3081.0\n2.221805\n1.215906\n0.366872\n1.459274\n1.919233\n2.636550\n13.757700\n\n\n\n\n\n\n\nA operação acima agrupa por assunto e calcula as estatísticas da variável tempo. Depois, ela exclui os assuntos com menos de 100 casos e ordena a base pela mediana em ordem decrescente.\nPela tabela, o assunto com menor tempo mediano é o tráfico de drogas, com mediana de 1.92 anos (1 ano e 11 meses). Já o assunto com maior tempo mediano é o estelionato, com mediana de 5.77 anos (5 anos e 9 meses). Você saberia teorizar o motivo?\nOs casos de homicídio provavelmente têm tempos altos também, mas o filtro de 100 casos excluiu esse assunto da análise. Note também que os tempos médios costumam ser maiores do que as medianas, como esperado.\nChamamos variáveis que com médias diferentes das medianas de assimétricas. A assimetria é uma medida de desvio da simetria. Se a média é maior que a mediana, dizemos que há assimetria à direita. Se a média é menor que a mediana, dizemos que há assimetria à esquerda.\nIsso tem relação com a distribuição dos dados. Por distribuição, queremos dizer a forma como os dados estão distribuídos quando colocamos ele em um histograma. Vamos ver então o histograma dos tempos:\n\nsns.histplot(camaras.tempo, kde=True)\n\n\n\n\n\n\n\n\nEstá vendo como os dados estão concentrados à esquerda? Isso é um sinal de assimetria à direita. A assimetria é justamente essa massa de dados que temos a mais de um lado da distribuição. No nosso caso, isso é esperado, pois temos muitos processos que são resolvidos rapidamente, mas alguns poucos que demoram muito tempo.\nPor último, será que os tempos variam por câmara? Em tese, não deveriam, já que os casos são distribuídos aleatoriamente entre elas.\n\n(\n  camaras\n  .groupby('camara')['tempo']\n  .describe()\n  .reset_index()\n  .query('count &gt;= 100')\n  .sort_values('50%', ascending=False)\n)\n\n\n\n\n\n\n\n\ncamara\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n23\n12ª Câmara de Direito Criminal\n567.0\n3.460561\n2.096506\n0.383299\n1.941136\n2.910335\n4.399726\n17.210130\n\n\n24\n13ª Câmara de Direito Criminal\n624.0\n3.361304\n2.321851\n0.418891\n1.837098\n2.751540\n4.141684\n21.108830\n\n\n5\n03ª Câmara de Direito Criminal\n581.0\n3.199803\n2.263286\n0.487337\n1.681040\n2.650240\n4.128679\n18.893908\n\n\n15\n08ª Câmara de Direito Criminal\n683.0\n3.255774\n2.174564\n0.470910\n1.774127\n2.639288\n4.088980\n18.266940\n\n\n27\n16ª Câmara de Direito Criminal\n660.0\n3.272711\n2.330123\n0.501027\n1.704997\n2.621492\n4.143053\n18.976044\n\n\n9\n05ª Câmara de Direito Criminal\n464.0\n3.291133\n2.282669\n0.323066\n1.766598\n2.596851\n4.118412\n17.078713\n\n\n26\n15ª Câmara de Direito Criminal\n620.0\n3.135109\n2.063688\n0.410678\n1.746749\n2.568104\n3.739220\n13.930185\n\n\n3\n02ª Câmara de Direito Criminal\n523.0\n3.167589\n2.288019\n0.525667\n1.672827\n2.472279\n3.801506\n21.508556\n\n\n11\n06ª Câmara de Direito Criminal\n723.0\n3.176956\n2.443826\n0.511978\n1.618070\n2.444901\n3.822040\n21.440110\n\n\n17\n09ª Câmara de Direito Criminal\n610.0\n3.045163\n1.907837\n0.495551\n1.788501\n2.440794\n3.670089\n13.478439\n\n\n21\n11ª Câmara de Direito Criminal\n601.0\n3.086797\n2.154204\n0.375086\n1.650924\n2.395619\n3.759069\n15.203285\n\n\n13\n07ª Câmara de Direito Criminal\n655.0\n2.992580\n1.932092\n0.476386\n1.639973\n2.387406\n3.744011\n16.479124\n\n\n1\n01ª Câmara de Direito Criminal\n566.0\n2.971269\n2.218564\n0.260096\n1.594798\n2.350445\n3.632444\n27.490760\n\n\n25\n14ª Câmara de Direito Criminal\n636.0\n3.003530\n2.177534\n0.377823\n1.562628\n2.344969\n3.782341\n16.783025\n\n\n19\n10ª Câmara de Direito Criminal\n487.0\n2.882080\n1.994354\n0.366872\n1.567420\n2.321697\n3.581109\n13.738535\n\n\n7\n04ª Câmara de Direito Criminal\n568.0\n2.804750\n1.868122\n0.536619\n1.552361\n2.277892\n3.463381\n17.407255\n\n\n\n\n\n\n\nOs tempos medianos variam entre 2.91 (2 anos e 11 meses) e 2.28 (2 anos e 3 meses). A diferença é pequena, mas existe. Veremos, mais adiante na disciplina, se podemos considerar que essa diferença é estatisticamente significante.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Medidas de posição e variabilidade</span>"
    ]
  },
  {
    "objectID": "03-medidas-posicao-variabilidade.html#medidas-de-variabilidade-conceitos",
    "href": "03-medidas-posicao-variabilidade.html#medidas-de-variabilidade-conceitos",
    "title": "3  Medidas de posição e variabilidade",
    "section": "3.4 Medidas de variabilidade: conceitos",
    "text": "3.4 Medidas de variabilidade: conceitos\nMedidas de dispersão ou variabilidade nos dão uma ideia de quão espalhados os dados estão em torno de algum ponto de referência. Essas medidas são essenciais para compreender o fenômeno estudado. Na verdade, elas são o segredo por trás do funcionamento da estatística.\nAs afirmações que fazemos na estatística têm sempre um grau de incerteza atrelado, porque não temos controle sobre tudo o que pode acontecer. No entanto, nós sabemos estimar esse grau de incerteza, justamente com essas medidas de variabilidade. Elas nos dão uma noção de quanto podemos confiar em uma média ou mediana, por exemplo, para tomar decisões estratégicas em um caso.\nUm exemplo simples sobre isso. Imagine que, na câmara 3, todos os processos durem 1 ano e 3 meses. Por conta disso, sabemos que a mediana é de 1 ano e 3 meses. Na câmara 6, no entanto, a mediana é de 1 ano e 3 meses também, mas temos muitos casos que duram 5 meses e casos que duram 10 anos. Se seu recurso caísse na 3ª câmara e seu cliente perguntasse quanto tempo o processo vai demorar, você falaria o mesmo que se caísse na 6ª câmara?\nProvavelmente não. Isso porque a incerteza é maior na 6ª câmara do que na 3ª. Para a 3ª câmara, provavelmente você falaria para o cliente “tudo pode acontecer no direito, mas metade dos casos duram até 1 ano e 3 meses”. Já para a 6ª, talvez seria algo do tipo “é difícil precisar, mas geralmente demora entre X meses e Y anos”. A variabilidade nos ajuda a entender X e Y.\nNa seção anterior, já vimos um possível intervalo desse tipo quando analisamos os quartis, relacionado à amplitude a partir de quantis empírios. Vamos começar por eles.\nComo antes, para cada conceito, vamos mostrar a definição, a fórmula matemática e o código em python para calcular.\n\n3.4.1 Amplitude\nComo veremos, a amplitude é mais útil para reportar dados básicos sobre o fenômeno estudado do que para tomar decisões sobre o comportamento dos dados.\n\n3.4.1.1 Definição\nA amplitude é a diferença entre o maior e o menor valor da amostra. Ela é uma medida de variabilidade que nos dá uma ideia de quão espalhados em geral.\n\n\n3.4.1.2 Fórmula\nA fórmula da amplitude é dada por:\n\\[A = \\text{máximo} - \\text{mínimo} = q(1) - q(0)\\]\n\n\n3.4.1.3 Código\nA amplitude pode ser calculada em python fazendo a diferença entre o máximo, com .max() e o mínimo, com .min(), como já vimos anteriormente.\n\n\n\n3.4.2 Amplitude do Intervalo Interquartil (IQR)\n\n3.4.2.1 Definição\nA amplitude do intervalo interquartil (inter quantile range, IQR) é a diferença entre o terceiro quartil (75%) e o primeiro quartil (25%). Ele é uma medida de variabilidade que nos dá uma ideia de quão espalhados os dados estão em torno da mediana.\n\n\n3.4.2.2 Fórmula\nO IQR é dado por:\n\\[IQR = q(75\\%) - q(25\\%)\\]\nComo o \\(q(25\\%)\\) é o valor que separa os 25% menores valores da amostra e como o \\(q(75\\%)\\) é o valor que separa os 25% maiores valores da amostra, o IQR é a amplitude de um intervalo que contém 50% dos dados.\nPor exemplo, para um conjunto de dados [1, 2, 2, 3, 3, 4, 4, 5, 5, 7, 9, 9, 10] /com 13 observações, temos que \\(q(25\\%) = 3\\) e \\(q(75\\%) = 7\\), então o IQR é \\(7-3=4\\).\n\n\n3.4.2.3 Código\nO IQR pode ser calculado em python usando a função quantile do pacote numpy, ou então dentro do próprio pandas, com o método .quantile(). Também vimos a o método .describe(), que acaba soltando o IQR.\n\nminimo = camaras.tempo.min()\nmaximo = camaras.tempo.max()\n\namplitude = maximo - minimo\n\nq3 = camaras.tempo.quantile(0.75)\nq1 = camaras.tempo.quantile(0.25)\niqr = q3 - q1\n\namplitude, iqr\n\n(27.23066392881588, 2.2258726899383983)\n\n\nA amplitude dos tempos é de 27.23 anos (27 anos e 3 meses). Já o IQR dos tempos é de 2.23 anos (2 anos e 3 meses).\nNote como temos, aqui, um balanço entre a incerteza a informação a ser passada. Nos nossos dados, 100% do que vimos varia em torno de 27 anos, mas isso não é muito informativo. No entanto, sabemos que 50% dos dados que vimos variam em 2 anos e 3 meses em torno da mediana. Assumimos um erro, mas é uma informação mais útil para tomada de decisão. A estatística é exatamente sobre isso: assumimos a possibilidade de errar para fazer afirmações mais úteis.\nUm exemplo caricato desse pensamento é o das pesquisas eleitorais. Em pesquisas eleitorais, geralmente os institutos de pesquisa apresentam margens de erro (de 2 ou 3 pontos percentuais, para cima e para baixo). Depois, quando os resultados ficam fora dessa margem de erro, os institutos são criticados. Não seria melhor, então, para os institutos, que falassem que a intenção de votos de um candidato está entre 0% e 100%? Dessa forma, o instituto não erraria. Mas essa não é a informação que a população quer saber. O que é acertar ou errar nesse sentido?\nPara finalizar o assunto do IQR e amplitude, nada nos impede de considerar outros intervalos que não os quartis. A medida também não precisa ficar em torno da mediana, se isso for de interesse. O IQR é apenas um valor clássico e bastante usado.\n\n\n\n3.4.3 Desvio padrão\n\n3.4.3.1 Definição\nO desvio padrão é uma medida de variabilidade que nos dá uma ideia de quão espalhados os dados estão em torno da média. Ele é a medida de variabilidade mais comum na estatística.\n\n\n3.4.3.2 Fórmula\nO desvio padrão é dado por:\n\\[s = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\bar{x})^2}\\]\nOnde \\(\\bar{x}\\) é a média e \\(x_i\\) é o valor da variável quantitativa de interesse para determinado processo \\(i\\).\nA fórmula pode assustar um pouco. Por que temos uma raiz quadrada, e qual o objetivo desse valor ao quadrado? Em linhas gerais, o que queremos é medir o quanto cada valor está distante da média. Para isso, fazemos a diferença entre o valor observado e a média (\\(x_i - \\bar{x}\\)). Como esses valores podem ser positivos ou negativos, elevamos essas diferenças ao quadrado, o que torna os valores sempre positivos. Aí calculamos a média desses desvios (que chamamos de variância). Finalmente, para retornar à escala original, aplicamos a raiz quadrada, que é a operação inversa de elevar ao quadrado.\nSugestão de leitura: este texto aqui para maior aprofundamento.\n\n\n3.4.3.3 Código\nPara calcular o desvio padrão em python, usamos a função std do pacote numpy, ou então dentro do próprio pandas, com o método .std().\n\nstd_pandas = camaras.tempo.std()\nstd_numpy = np.std(camaras.tempo)\n\nstd_pandas, std_numpy\n\n(2.182874474194876, 2.1827653168264267)\n\n\nOs números do numpy são ligeiramente diferentes do que os do pandas. Vamos testar um exemplo com menos números.\n\ndf = pd.DataFrame({\n  'numeros': [1,3,5,6,9]\n})\n\nstd_pd = df['numeros'].std()\nstd_np = np.std(df['numeros'])\n\nstd_pd, std_np\n\n(3.03315017762062, 2.7129319932501073)\n\n\nCom um exemplo pequeno a diferença ficou ainda maior!\nIsso ocorre porque as fórmulas que o numpy e o pandas usam para calcular o desvio padrão são diferentes.\nFórmula no caso do pandas:\n\\[s_{\\text{pandas}} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\bar{x})^2}\\]\nFórmula no caso do numpy:\n\\[s_{\\text{numpy}} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N} (x_i - \\bar{x})^2}\\]\nVeja que a conta do pandas é igual à nossa conta original. Já a do numpy coloca o valor \\(N-1\\) no denominador, ao invés de \\(N\\). O valor \\(N-1\\) é um ajuste que estatísticos do século XX fizeram para garantir algumas propriedades matemáticas dessa estatística, que não vamos tratar agora. Na prática, no entanto, essa diferença é muito pequena, especialmente com grandes bases de dados. Se tiver interesse em estudar sobre isso, veja Bussab & Morettin (2023).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Medidas de posição e variabilidade</span>"
    ]
  },
  {
    "objectID": "03-medidas-posicao-variabilidade.html#medidas-de-variabilidade-aplicação",
    "href": "03-medidas-posicao-variabilidade.html#medidas-de-variabilidade-aplicação",
    "title": "3  Medidas de posição e variabilidade",
    "section": "3.5 Medidas de variabilidade: aplicação",
    "text": "3.5 Medidas de variabilidade: aplicação\nVamos voltar ao nosso querido .describe().\n\ncamaras.tempo.describe()\n\ncount    9999.000000\nmean        3.178841\nstd         2.182874\nmin         0.260096\n25%         1.705681\n50%         2.529774\n75%         3.931554\nmax        27.490760\nName: tempo, dtype: float64\n\n\nAgora temos conhecimento suficiente para calcular tudo a partir desses resumos!\nOs processos costumam demorar 2 anos e 6 meses desde o ajuizamento até o acórdão. Podemos dizer que 50% deles variam entre 1 ano e 8 meses e 3 anos e 11 meses. Já a média é de 3 anos e 2 meses, com desvio padrão 2 anos e 2 meses. Ou seja, tipicamente os tempos variam mais de 2 anos para mais ou para menos da média. É bastante variação.\nUma dúvida que pode surgir, já que isso foi citado na introdução, é: como eu associo essa variabilidade à uma medida de incerteza? Por exemplo, como podemos proceder para fazer afirmações do tipo: com 95% de certeza, os processos duram entre X e Y? Veremos isso quando falarmos de estatística inferencial.\nEnquanto não chegamos nessa parte mais mágica da estatística, vamos ver mais algumas visualizações interessantes. Primeiro, não seria interessante analisar esses quantis que vêm do .describe() de forma visual?\nEsse é o papel do boxplot:\n\n## Primeiro, vamos retirar da amostra os assuntos que têm menos de 100 observações\ncamaras_assuntos_100 = camaras.groupby('assunto').filter(lambda x: len(x) &gt;= 100)\n\ncamaras_assuntos_100['assunto'] = [\n  textwrap.fill(a, 70) for a in camaras_assuntos_100['assunto']\n]\n\n# agora, vamos fazer o boxplot\nsns.boxplot(data=camaras_assuntos_100, y='assunto', x='tempo')\n\n\n\n\n\n\n\n\nNesse gráfico, a linha central é a mediana. Os valores limites da ‘caixa’ são, respectivamente, os quartis 1 e 3. Ou seja, a amplitude da caixa é exatamente o IQR. As linhas contínuas que vêm em seguida são chamadas de bigodes, e são calculadas por 1,5 vezes o IQR (o motivo disso não vem ao caso agora). Valores que estão fora dos bigodes, nas bolinhas, são considerados valores atípicos para esse tipo de processo.\nMuita informação em um gráfico apenas! Geralmente usamos esses gráficos para rapidamente entender um pouco mais sobre nossos dados. Por exemplo, é possível ver que tráfico de drogas não só tem uma mediana menor mas também varia pouco. Já estelionato varia bastante, inclusive com alguns casos durando mais de 15 anos. E muito mais para investigar.\n\n3.5.1 Cuidado com algumas análises de tempo (avançado)\nUm cuidado a ser tomado em estudos relacionados a tempo de processos judiciais é com relação ao escopo da pesquisa.\nNo caso da pesquisa das câmaras, a origem dos dados é a consulta de jurisprudência. Isso significa que temos em nossa amostra apenas casos que tiveram acórdãos proferidos. Existem muitos casos criminais tramitando na segunda instância do TJSP que não estão em nossa base simplesmente porque não foram decididos ainda. Chamamos esse tipo de pesquisa de retrospectiva.\nIsso significa que a nossa análise de tempos pode ter um problema: será que os casos que ainda não foram julgados são mais demorados do que os casos que temos em nossa amostra? Pode ser que sim, mas também pode ser que não.\nA única forma de verificar se existe um viés é coletando dados de processos a partir da sua data de ajuizamento, utilizando ferramentas como Diários de Justiça ou o DataJud. Esse tipo de pesquisa é chamada de prospectiva.\nA imagem abaixo mostra exemplos de processos que ficam dentro de estudos prospectivos ou retrospectivos.\n\n\nProspectivo e retrospectivo\nApenas prospectivo\nApenas retrospectivo\nNenhum dos dois, mas poderia ser capturado por atividade no período\nfora do escopo\nfora do escopo\nNenhum dos dois tipos e não poderia ser capturado (ficou inativo no período)\n\nEsse assunto é complexo e poucas pessoas dominam. Até mesmo o CNJ comete alguns erros nesse sentido. O importante que devemos ter em mente são três coisas:\n\nQuando analisamos tempos em estudos retrospectivos, precisamos ter em mente que estamos estimando o tempo dos processos em toda a história do TJSP e não só dos últimos anos, já que nossa amostra inclui casos muito antigos.\nQuando analisamos tempos em estudos prospectivos, precisamos tomar certos cuidados para realizar estimativas. Não veremos isso aqui, mas se tiver interesse, a técnica a ser aplicada aqui é a análise de sobrevivência.\nNUNCA faça análises de aceleração dos processos (verificar se os processos estão ficando mais rápidos) em estudos retrospectivos.\n\nSobre (3), é possível verificar visualmente o motivo disso ser um problema grave. A figura abaixo mostra, no eixo x, o ano de ajuizamento dos processos e, no eixo y, o tempo dos processos até o acórdão.\n\n\ncamaras['ano_ajuizamento'] = [int(p[9:13]) for p in camaras.processo]\n\nsns.scatterplot(camaras, x = 'ano_ajuizamento', y = 'tempo')\n\n\n\n\n\n\n\n\nO que está acontecendo aqui? Os processos estão ficando mais rápidos?\nNa verdade não.\nO que está acontecendo é que não conseguimos ver processos fora da faixa de pontos por conta da forma que os dados foram coletados!\nOs dados foram coletados a partir de acórdãos publicados entre 2016 e 2023. Ou seja, não existe possibilidade, por exemplo, de encontrar casos ajuizados em 2020 que tenham tempo maior do que 4 anos. Um caso assim seria julgado em 2024 e, portanto, estaria fora do nosso escopo de busca. O mesmo ocorre para os casos muito rápidos: não temos, em nossa amostra, casos que foram ajuizados em 2005 e que duraram menos de 10 anos, já que essas decisões foram publicadas no máximo em 2015 e, portanto, também estão fora do escopo de busca.\nOu seja, esse gráfico não informa absolutamente nada a respeito da potencial aceleração dos tempos dos processos. Ele simplesmente evidencia o escopo da pesquisa.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Medidas de posição e variabilidade</span>"
    ]
  },
  {
    "objectID": "04-visualizacao-seaborn.html",
    "href": "04-visualizacao-seaborn.html",
    "title": "4  Visualizações com seaborn",
    "section": "",
    "text": "4.1 Introdução\nNessa parte, nosso interesse é trabalhar na escolha das melhores visualizações para nossos dados, e como implementar isso usando a biblioteca seaborn.\nPor que isso é importante? Além de realizar análises de dados, precisamos nos preparar para comunicar resultados. Boas visualizações são essenciais para isso. A necessidade de comunicar dados pode acontecer dentro do dos seus estudos de direito, mas também em situações de negócios, como apresentações para clientes e visual law.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizações com seaborn</span>"
    ]
  },
  {
    "objectID": "04-visualizacao-seaborn.html#introdução",
    "href": "04-visualizacao-seaborn.html#introdução",
    "title": "4  Visualizações com seaborn",
    "section": "",
    "text": "4.1.1 O que é?\nVisualização de dados é a representação de dados em gráficos, tabelas e diagramas que podem ser interpretados por pessoas. Trata-se de uma área interdisciplinar, misturando estatística, arte e comunicação. É uma parte da área de data storytelling, que envolve organizar todos os resultados de uma análise de dados em uma ordem lógica para comunicar de forma efetiva com a audiência.\n\n\n4.1.2 Por que fazer?\nVisualizações estão presentes na grande maioria dos projetos de ciência de dados. Além disso, é a parte mais acessível da ciência de dados do ponto de vista de quem lê. Mostrar uma visualização costuma ser mais efetivo do que a saída de um modelo ou uma fórmula. Finalmente, é uma das partes mais difíceis de automatizar da ciência de dados. Uma carreira em dataviz dificilmente ficará obsoleta.\n\n\n4.1.3 Para que servem?\nUma base de dados contém toda a informação que precisamos. No entanto, não somos capazes de tirar conclusões apenas olhando essas bases. Por isso, é necessário resumir esses dados em estatísticas, como vimos na apostila sobre medidas de posição e variabilidade. Nem sempre as estatísticas (os números) são úteis para uma comunicação efetiva… Por isso, faz sentido mostrá-las usando formas, cores e outros elementos que facilitam a absorção da informação pelas pessoas.\n\n\n4.1.4 Em que momento utilizamos?\nAbaixo, temos o ciclo da ciência de dados. Esse diagrama foi adaptado do livro R para ciência de dados, que é uma referência para quem quer aprender ciência de dados com R.\nEsse ciclo resume a maioria das tarefas que precisamos executar ao longo de um projeto de ciência de dados. Começamos pela importação, que envolve a leitura de dados de diferentes fontes. Em seguida, limpamos e transformamos esses dados para que possam ser usados em análises. A etapa de análise é um ciclo em si, envolvendo transformação de dados (criação de colunas e agregações), a visualização dos dados e a aplicação de modelos estatísticos / de machine learning para entender os dados. Finalmente, precisamos comunicar os resultados ou automatizar nosso produto de dados.\n\nNote que a visualização de dados aparece em duas partes principais: Visualizar e Comunicar.\nNa parte de visualização, estamos fazendo uma análise exploratória dos dados. Isso significa que estamos tentando entender os dados, e não necessariamente comunicar resultados. É um trabalho de investigação, que precisa ser rápido de fazer. O objetivo principal é aprender.\nNa parte de comunicação, estamos fazendo um trabalho de otimização visual. Agora, nosso objetivo é comunicar os resultados com outras pessoas. Isso significa que precisamos de gráficos mais bonitos, mais explicativos e mais fáceis de entender. O trabalho deve, inclusive, ser encaixado em um fluxo de storytelling. O objetivo principal é comunicar.\nO seaborn é uma biblioteca que é muito efetiva para fazer gráficos de análise exploratória. Para otimização visual, ela é um pouco limitada. No entanto, é possível fazer gráficos muito bonitos com ela, e é uma biblioteca muito fácil de usar. Por isso, é uma ótima escolha para começar a aprender visualização de dados. Se tiver interesse em estudar uma ferramenta mais robusta, recomendamos o ggplot2 do R, que é uma das melhores ferramentas para otimização visual.\nA partir de agora, vamos retomar os nossos conceitos de tipos de variáveis e descrever as melhores visualizações para cada combinação de tipos de variáveis.\nUma referência legal nesse sentido é o site From Data to Viz, que é um guia para escolher a melhor visualização para os seus dados.\n\nVamos tratar dos seguintes exemplos:\n\nVisualizações para variáveis categóricas\n\n\nGráficos univariados\nGráficos bivariados\n\nCom outra categórica\nCom variável numérica\n\n\n\nVisualizações para variáveis numéricas\n\n\nGráficos univariados\nGráficos bivariados\n\nCom variável categórica\nCom outra numérica\n\n\nPrimeiro vamos falar de exploração, depois vamos dar um exemplo de otimização visual.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizações com seaborn</span>"
    ]
  },
  {
    "objectID": "04-visualizacao-seaborn.html#visualizações-para-variáveis-categóricas",
    "href": "04-visualizacao-seaborn.html#visualizações-para-variáveis-categóricas",
    "title": "4  Visualizações com seaborn",
    "section": "4.2 Visualizações para variáveis categóricas",
    "text": "4.2 Visualizações para variáveis categóricas\nOs gráficos univariados de variáveis categóricas são os mais comuns em ciência de dados no direito, já que o tipo de variável mais comum é a categórica.\n\n4.2.1 Univariada\nA função mais comum para visualizar variáveis categóricas é o countplot. Esse gráfico é uma versão do barplot do matplotlib, mas com a contagem de cada categoria no eixo y.\n\nsns.countplot(data=camaras, x='decisao')\n\n\n\n\n\n\n\n\nVeja que os rótulos ficaram sobrepostos. Outra forma de visualizar é rotacionando o eixo:\n\nsns.countplot(data=camaras, y='decisao')\n\n\n\n\n\n\n\n\nAgora está um pouco melhor. Em alguns lugares, existe uma distinção entre ‘gráficos de barras’ (o segundo caso) e ‘gráficos de colunas’ (o primeiro caso), mas essa distinção não é importante.\nSe nosso interesse é mostrar a proporção e não os valores absolutos, podemos usar o barplot do seaborn:\nTambém podemos ordenar as barras para facilitar a leitura. Geralmente fazemos isso apenas para nominais, não para ordinais.\n\nsns.countplot(data=camaras, y='decisao', order=camaras['decisao'].value_counts().index)\n\n\n\n\n\n\n\n\nE se quisermos mostrar a proporção em vez da contagem? Podemos calcular essas proporções com o pandas e usar o barplot. Nesse caso, eu não precisei nem ordenar, porque os dados já estão ordenados do jeito que eu quero.\n\ncontagens = camaras.value_counts('decisao').reset_index()\n\ncontagens['prop'] = contagens['count'] / contagens['count'].sum()\n\nsns.barplot(data=contagens, y='decisao', x='prop')\n\n\n\n\n\n\n\n\nNa verdade, tanto o countplot quanto o barplot são versões do catplot com kind='count' e kind='bar', respectivamente. O catplot é uma função mais flexível, que permite que façamos gráficos de barras com mais de uma variável categórica.\nVeja a imagem abaixo retirada da documentação do seaborn:\n\n\nsns.catplot(data=camaras, y='decisao', kind='count', order=camaras['decisao'].value_counts().index)\n\n\n\n\n\n\n\n\n\nsns.catplot(data=contagens, y='decisao', x = 'prop', kind='bar')\n\n\n\n\n\n\n\n\nComo o catplot é mais flexível, ele é a função que vamos usar para fazer gráficos bivariados com variáveis categóricas.\nPor último, temos os gráficos de pizza ou donut. Eles podem ser úteis para mostrar proporções, mas são menos efetivos do que os gráficos de barras. A razão é que é mais difícil comparar setores/ângulos do que comprimentos. No entanto, eles são muito populares em apresentações de negócios. No seaborn, não temos uma função específica para fazer gráficos de pizza, mas podemos usar o pie do matplotlib.\n\nplt.pie(contagens['prop'], labels=contagens['decisao'], autopct='%1.1f%%')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.2.2 Bivariada: explicativa categórica\nQuando temos duas variáveis categóricas, temos várias escolhas de visualizações possíveis para os gráficos de barras. Os mais comuns são i) separar por cores e colocar as barras lado a lado; ii) separar por cores e empilhar as barras; e iii) criar sub-gráficos para cada categoria.\nBarras lado a lado\n\nct_mag = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .value_counts(['rel_tipo_magistrado', 'decisao'])\n  .reset_index()\n)\n\nct_mag['prop'] = ct_mag['count'] / ct_mag.groupby('rel_tipo_magistrado')['count'].transform('sum')\n\nct_mag\n\n\n\n\n\n\n\n\nrel_tipo_magistrado\ndecisao\ncount\nprop\n\n\n\n\n0\ndesembargador\nNegaram\n4259\n0.533041\n\n\n1\ndesembargador\nParcialmente\n3098\n0.387735\n\n\n2\njuiz substituto 2º grau\nNegaram\n656\n0.547123\n\n\n3\njuiz substituto 2º grau\nParcialmente\n445\n0.371143\n\n\n4\ndesembargador\nProvido\n370\n0.046308\n\n\n5\ndesembargador\nPunibilidade Extinta\n205\n0.025657\n\n\n6\njuiz substituto 2º grau\nProvido\n58\n0.048374\n\n\n7\ndesembargador\nOutros\n45\n0.005632\n\n\n8\njuiz substituto 2º grau\nPunibilidade Extinta\n28\n0.023353\n\n\n9\ndesembargador\nNão conhecido\n13\n0.001627\n\n\n10\njuiz substituto 2º grau\nOutros\n10\n0.008340\n\n\n11\njuiz substituto 2º grau\nNão conhecido\n2\n0.001668\n\n\n\n\n\n\n\n\nsns.catplot(ct_mag, x = 'prop', y = 'decisao', kind = 'bar', hue = 'rel_tipo_magistrado')\n\n\n\n\n\n\n\n\nBarras empilhadas\n\ncamaras['extraord'] = camaras['camara'].str.contains('Extra')\n\nct_cam = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('extraord == False')\n  .value_counts(['camara', 'unanimidade'])\n  .reset_index()\n)\n\nct_cam['prop'] = ct_cam['count'] / ct_cam.groupby('camara')['count'].transform('sum')\n\nct_cam.head(10)\n\n\n\n\n\n\n\n\ncamara\nunanimidade\ncount\nprop\n\n\n\n\n0\n06ª Câmara de Direito Criminal\nUnânime\n661\n0.995482\n\n\n1\n08ª Câmara de Direito Criminal\nUnânime\n621\n0.977953\n\n\n2\n07ª Câmara de Direito Criminal\nUnânime\n587\n0.979967\n\n\n3\n16ª Câmara de Direito Criminal\nUnânime\n579\n0.983022\n\n\n4\n13ª Câmara de Direito Criminal\nUnânime\n575\n0.984589\n\n\n5\n14ª Câmara de Direito Criminal\nUnânime\n567\n0.986087\n\n\n6\n09ª Câmara de Direito Criminal\nUnânime\n551\n0.985689\n\n\n7\n15ª Câmara de Direito Criminal\nUnânime\n538\n0.952212\n\n\n8\n11ª Câmara de Direito Criminal\nUnânime\n531\n0.960217\n\n\n9\n04ª Câmara de Direito Criminal\nUnânime\n525\n0.983146\n\n\n\n\n\n\n\nO seaborn não possui uma forma fácil de fazer as barras empilhadas. Por isso, utilizamos o matplotlib mesmo. Para isso, no entanto, precisamos montar uma base pivotada.\n\npivot_df = ct_cam.pivot(index='camara', columns='unanimidade', values='prop')\n\npivot_df\n\n\n\n\n\n\n\nunanimidade\nMaioria\nSem informação\nUnânime\n\n\ncamara\n\n\n\n\n\n\n\n01ª Câmara de Direito Criminal\n0.007782\n0.001946\n0.990272\n\n\n02ª Câmara de Direito Criminal\n0.072464\n0.004141\n0.923395\n\n\n03ª Câmara de Direito Criminal\n0.005682\n0.007576\n0.986742\n\n\n04ª Câmara de Direito Criminal\n0.016854\nNaN\n0.983146\n\n\n05ª Câmara de Direito Criminal\n0.007143\n0.007143\n0.985714\n\n\n06ª Câmara de Direito Criminal\nNaN\n0.004518\n0.995482\n\n\n07ª Câmara de Direito Criminal\n0.006678\n0.013356\n0.979967\n\n\n08ª Câmara de Direito Criminal\n0.017323\n0.004724\n0.977953\n\n\n09ª Câmara de Direito Criminal\n0.008945\n0.005367\n0.985689\n\n\n10ª Câmara de Direito Criminal\n0.004301\n0.002151\n0.993548\n\n\n11ª Câmara de Direito Criminal\n0.030741\n0.009042\n0.960217\n\n\n12ª Câmara de Direito Criminal\n0.072937\n0.024952\n0.902111\n\n\n13ª Câmara de Direito Criminal\n0.013699\n0.001712\n0.984589\n\n\n14ª Câmara de Direito Criminal\n0.010435\n0.003478\n0.986087\n\n\n15ª Câmara de Direito Criminal\n0.046018\n0.001770\n0.952212\n\n\n16ª Câmara de Direito Criminal\n0.013582\n0.003396\n0.983022\n\n\n\n\n\n\n\n\npivot_df.plot(kind='barh', stacked=True)\n\n\n\n\n\n\n\n\nSub-gráficos (facets)\nPara facer sub-gráficos com o seaborn, utilizamos o parâmetro col=\n\ncomarcas = ['SAO PAULO', 'SAO JOSE DOS CAMPOS', 'SANTOS', 'CAMPINAS']\n\nct_comarca = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('comarca in @comarcas')\n  .value_counts(['comarca', 'decisao'])\n  .reset_index()\n)\n\nct_comarca['prop'] = ct_comarca['count'] / ct_comarca.groupby('comarca')['count'].transform('sum')\n\nct_comarca\n\n\n\n\n\n\n\n\ncomarca\ndecisao\ncount\nprop\n\n\n\n\n0\nSAO PAULO\nNegaram\n1061\n0.579465\n\n\n1\nSAO PAULO\nParcialmente\n657\n0.358820\n\n\n2\nCAMPINAS\nNegaram\n127\n0.566964\n\n\n3\nCAMPINAS\nParcialmente\n79\n0.352679\n\n\n4\nSAO JOSE DOS CAMPOS\nNegaram\n70\n0.588235\n\n\n5\nSAO PAULO\nProvido\n69\n0.037684\n\n\n6\nSANTOS\nNegaram\n63\n0.636364\n\n\n7\nSAO JOSE DOS CAMPOS\nParcialmente\n42\n0.352941\n\n\n8\nSAO PAULO\nPunibilidade Extinta\n36\n0.019661\n\n\n9\nSANTOS\nParcialmente\n30\n0.303030\n\n\n10\nCAMPINAS\nProvido\n17\n0.075893\n\n\n11\nSAO PAULO\nOutros\n7\n0.003823\n\n\n12\nSAO JOSE DOS CAMPOS\nProvido\n5\n0.042017\n\n\n13\nSANTOS\nPunibilidade Extinta\n3\n0.030303\n\n\n14\nSANTOS\nProvido\n3\n0.030303\n\n\n15\nSAO JOSE DOS CAMPOS\nPunibilidade Extinta\n2\n0.016807\n\n\n16\nCAMPINAS\nPunibilidade Extinta\n1\n0.004464\n\n\n17\nSAO PAULO\nNão conhecido\n1\n0.000546\n\n\n\n\n\n\n\n\nsns.catplot(data=ct_comarca, x='prop', y='decisao', kind='bar', col='comarca', col_wrap=2)\n\n\n\n\n\n\n\n\n\n\n4.2.3 Bivariada: explicativa numérica\nQuando temos a variável de interesse categórica e a explicativa numérica, é um pouco difícil de criar visualizações, porque no fundo o que queremos entender é como o aumento/diminuição dessa variável numérica afeta a probabilidade de um evento relacionado à variável categórica acontecer. Isso geralmente é feito através de modelos estatísticos como a regressão logística, que veremos mais para frente na disciplina.\nAlgumas alternativas são: i) categorizar a variável numérica – nesse caso, voltamos ao que já vimos anteriormente; ii) analisar a distribuição da variável numérica para cada categoria – nesse caso, é como se estivéssemos invertendo qual é a variável de interesse e qual é a variável explicativa, logo isso faz parte da seção de variáveis numéricas, que veremos ainda nessa apostila, mas mais para frente.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizações com seaborn</span>"
    ]
  },
  {
    "objectID": "04-visualizacao-seaborn.html#visualização-para-variáveis-numéricas",
    "href": "04-visualizacao-seaborn.html#visualização-para-variáveis-numéricas",
    "title": "4  Visualizações com seaborn",
    "section": "4.3 Visualização para variáveis numéricas",
    "text": "4.3 Visualização para variáveis numéricas\n\n4.3.1 Univariada\nA análise univariada de uma variável de interesse numérica geralmente busca entender a distribuição dos dados.\nExistem três principais visualizações aqui: histograma, densidade e boxplot. A densidade é simplesmente uma versão suavizada do histograma, e é possível mostrar os dois ao mesmo tempo.\n\nsns.displot(camaras, x='tempo')\n\n\n\n\n\n\n\n\nPodemos mudar o número de barras com os parâmetros bins (quantidade de barras) ou binwidth (largura da barra).\n\nsns.displot(camaras, x='tempo', bins=30)\n\n\n\n\n\n\n\n\n\nsns.displot(camaras, x='tempo', binwidth=1)\n\n\n\n\n\n\n\n\n\nsns.displot(camaras, x='tempo', kind='kde')\n\n\n\n\n\n\n\n\nVeja que, nesse caso, o eixo y representa a densidade, e não as contagens. A densidade tem relação com o conceito de distribuição de probabilidades. Na verdade, essa densidade é uma estimativa da função densidade de probabilidades da variável estudada (ou seja, é um modelo estatístico!). A área total do gráfico é 1.\nAbaixo, juntamos histograma e densidade\n\n# com o gráfico de distribuição\nsns.displot(camaras, x='tempo', kde=True)\n\n\n\n\n\n\n\n\nFinalmente, temos o boxplot. O boxplot pode ser usado no caso univariado, mas é mais comum quando temos uma variável explicativa categórica.\n\nsns.boxplot(camaras, x='tempo')\n\n\n\n\n\n\n\n\n\n\n4.3.2 Bivariada: explicativa categórica\nA ideia aqui é simplesmente repetir os gráficos acima para a variável categórica de interesse.\nComeçamos pelo histograma / densidade\n\nsns.displot(camaras, x='tempo', binwidth=1, col='unanimidade', col_wrap=2)\n\n\n\n\n\n\n\n\nNote que aqui, a escala pode afetar o gráfico e dificultar a comparação.\n\nsns.displot(camaras, x='tempo', binwidth=1, col='unanimidade', stat='density', common_norm=False, col_wrap=2)\n\n\n\n\n\n\n\n\n\ncamaras_com_info = camaras[camaras.unanimidade != 'Sem informação']\n\nsns.displot(camaras_com_info, x='tempo', binwidth=2, hue='unanimidade', stat='density', common_norm=False)\n\n\n\n\n\n\n\n\n\nsns.displot(camaras_com_info, x='tempo', kind = 'kde', hue='unanimidade', common_norm=False)\n\n\n\n\n\n\n\n\nE o boxplot:\n\ncamaras_sem_extraord = camaras[~camaras['extraord']].sort_values('camara')\nsns.boxplot(data=camaras_sem_extraord, x='tempo', y='camara')\n\n\n\n\n\n\n\n\n\n# alternativa ao boxplot: violinplot\n# trata-se basicamente de um gráfico de densidade duplicado. Fica bonitinho.\n\ncamaras_sem_extraord = camaras[~camaras['extraord']].sort_values('camara')\nsns.violinplot(data=camaras_sem_extraord, x='tempo', y='camara')\n\n\n\n\n\n\n\n\n\n\n4.3.3 Bivariada: explicativa numérica\nQuando temos duas variáveis numéricas, usualmente fazemos um gráfico de dispersão. Nesse caso, usamos o relplot() do seaborn.\n\ncamaras\n\nsns.relplot(camaras, x='rel_tempo_magistratura', y='tempo')\n\n\n\n\n\n\n\n\nPodemos até adicionar mais variáveis aqui, como, por exemplo, nas cores\n\nsns.relplot(camaras, x='rel_idade', y='tempo', hue='rel_quinto')\n\n\n\n\n\n\n\n\nUm tipo especial de análise bivariada é quando o eixo x é uma data. Nesse caso, temos uma série de tempo, e representsaos os dados com um gráfico de linhas.\n\n# vamos calcular a proporção de negados ao longo do tempo\n\n# criando uma coluna que pega a data de publicação e arredonda o trimestre\n# o pandas é bem burocrático para fazer essa tarefa simples\ncamaras['periodo'] = pd.to_datetime(camaras['dt_publicacao']).dt.to_period('Q').dt.to_timestamp()\n\nprop_negados_mes = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .groupby('periodo')\n  .agg(prop_negados = ('decisao', lambda x: (x == 'Negaram').mean()))\n  .reset_index()\n)\n\nprop_negados_mes\n\nsns.relplot(prop_negados_mes, x='periodo', y='prop_negados', kind='line')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizações com seaborn</span>"
    ]
  },
  {
    "objectID": "04-visualizacao-seaborn.html#otimização",
    "href": "04-visualizacao-seaborn.html#otimização",
    "title": "4  Visualizações com seaborn",
    "section": "4.4 Otimização",
    "text": "4.4 Otimização\nA otimização visual é um assunto extenso e muito baseado em tentativa e erro. Afinal, o que queremos aqui é adaptar nosso gráfico para um fim específico, e isso pode variar muito. Nessas situações, o chatGPT e ferramentas similares podem ser muito úteis!\nVamos colocar alguns exemplos de otimização aqui:\n\nColocar o % nos eixos que são porcentagens\nMudar o título dos eixos\nMudar as cores das barras\nMudar a cor de fundo\n\nColocar o % nos eixos que são porcentagens\nVamos voltar para o gráfico que vimos anteriormente\n\nsns.catplot(ct_mag, x = 'prop', y = 'decisao', kind = 'bar', hue = 'rel_tipo_magistrado')\n\n\n\n\n\n\n\n\n\n# função para formatar eixo x\ndef to_pct(x, pos):\n  return f'{100*x:.0f}%'\n\nsns.catplot(data=ct_mag, x='prop', y='decisao', kind='bar', hue='rel_tipo_magistrado')\n\n# gca: get current axis\nplt.gca().xaxis.set_major_formatter(to_pct)\n\n\n\n\n\n\n\n\n\n# Outro jeito, usando um pouco mais de matplotlib. Não dá para usar catplot\n\nfig, ax = plt.subplots()\nsns.barplot(data=ct_mag, x='prop', y='decisao', hue='rel_tipo_magistrado', ax=ax)\n\nax.xaxis.set_major_formatter(plt.FuncFormatter(to_pct))\n\n\n\n\n\n\n\n\nVeja que precisamos voltar a usar uma função mais ‘baixo nível’, a barplot\nMudar o título dos eixos\n\nfig, ax = plt.subplots()\nsns.barplot(data=ct_mag, x='prop', y='decisao', hue='rel_tipo_magistrado', ax = ax)\n\nax.xaxis.set_major_formatter(to_pct)\nax.set_xlabel('Proporção')\nax.set_ylabel('Decisão')\nax.legend(title = 'Tipo de magistrado')\n\nplt.show()\n\n\n\n\n\n\n\n\nCores\n\nsns.set_theme()\n\nfig, ax = plt.subplots()\nsns.barplot(data=ct_mag, x='prop', y='decisao', hue='rel_tipo_magistrado', ax = ax)\n\nax.xaxis.set_major_formatter(to_pct)\nax.set_xlabel('Proporção')\nax.set_ylabel('Decisão')\nax.legend(title = 'Tipo de magistrado')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.set_style('whitegrid')\n\nfig, ax = plt.subplots()\nsns.barplot(data=ct_mag, x='prop', y='decisao', hue='rel_tipo_magistrado', ax = ax)\n\nax.xaxis.set_major_formatter(to_pct)\nax.set_xlabel('Proporção')\nax.set_ylabel('Decisão')\nax.legend(title = 'Tipo de magistrado')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.set_context('notebook')\n\n# palette options: https://seaborn.pydata.org/tutorial/color_palettes.html\n# minhas favoritas: 'rocket', 'mako', 'viridis'\n\nfig, ax = plt.subplots()\nsns.barplot(data=ct_mag, x='prop', y='decisao', hue='rel_tipo_magistrado', ax = ax, palette='mako')\n\nax.xaxis.set_major_formatter(to_pct)\nax.set_xlabel('Proporção')\nax.set_ylabel('Decisão')\nax.legend(title = 'Tipo de magistrado')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Aumentar a resolução mudando dpi (dots per inch)\n# obs: afeta todo o documento!\n\nplt.rcParams.update({\"figure.dpi\": 150})\n\nfig, ax = plt.subplots()\nsns.barplot(data=ct_mag, x='prop', y='decisao', hue='rel_tipo_magistrado', ax = ax, palette='mako')\n\nax.xaxis.set_major_formatter(to_pct)\nax.set_xlabel('Proporção')\nax.set_ylabel('Decisão')\nax.legend(title = 'Tipo de magistrado')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# também é possível mudar a configuração do jupyter\n\n%config InlineBackend.figure_format = \"svg\"\n\n# mais informações: https://seaborn.pydata.org/faq.html#why-do-the-plots-look-fuzzy-in-a-jupyter-notebook\n\nfig, ax = plt.subplots()\nsns.barplot(data=ct_mag, x='prop', y='decisao', hue='rel_tipo_magistrado', ax = ax, palette='mako')\n\nax.xaxis.set_major_formatter(to_pct)\nax.set_xlabel('Proporção')\nax.set_ylabel('Decisão')\nax.legend(title = 'Tipo de magistrado')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Vamos voltar ao 'normal'\n\n%config InlineBackend.figure_format = \"png\"\nplt.rcParams.update({\"figure.dpi\": 100})",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizações com seaborn</span>"
    ]
  },
  {
    "objectID": "05-amostras-populacoes.html",
    "href": "05-amostras-populacoes.html",
    "title": "5  Amostras e populações",
    "section": "",
    "text": "5.1 Introdução\nAgora vamos falar daquele que talvez seja o conceito mais conhecido e que praticamente define o que fazemos em estatística: a amostra.\nImagine que você está querendo saber se possui uma doença. Para detectar a doença, é necessário realizar um exame de sangue. O que você faz? Retira todo o sangue do corpo para verificar se possui a doença? Claro que não! O que você faz é retirar uma pequena quantidade de sangue, que é a amostra, e verificar se a doença está presente.\nEsse procedimento, do exame de sangue, é muito natural e faz parte do nosso cotidiano. É até um pouco óbvio pensar que não é necessário retirar todo o sangue do corpo para verificar se possui a doença. O que é interessante é que esse procedimento, que é o procedimento de amostragem, começa a ter menos confiança das pessoas quando é aplicado em outros contextos.\nPor exemplo, vamos considerar as pesquisas eleitorais. Seguindo o mesmo princípio, quando uma pesquisa eleitoral é realizada, não é necessário entrevistar todos os eleitores para saber em quem eles vão votar, certo? O que é feito é entrevistar uma pequena quantidade de pessoas, que é a amostra, e verificar em quem elas vão votar. A partir disso, é possível fazer uma estimativa de como está a opinião pública a respeito de um conjunto de candidatos. Nessa área, há quase um consenso popular de que as amostras não são confiáveis, que as pesquisas eleitorais não são confiáveis, que as pesquisas de opinião não são confiáveis, etc.\nPor que temos confiança no exame de sangue, mas não nas pesquisas eleitorais? Existem várias possíveis explicações, como a carga política, as dificuldades de aplicar métodos amostrais em pesquisas de opinião, etc. No entanto, o que é interessante é que, em ambos os casos, o procedimento é o mesmo: retirar uma pequena quantidade de sangue ou entrevistar uma pequena quantidade de pessoas. E com isso tomar algumas conclusões.\nA digressão feita acima tem como objetivo sensibilizar o leitor sobre a diferença entre as técnicas estatísticas e as aplicações ou interpretações de resultados. As metodologias de amostragem existem e funcionam se corretamente conduzidas, pois aplicam resultados matemáticos. Não é papel da estatística garantir que as pessoas vão acreditar nos resultados, mas sim garantir que o método é adequado para determinado fim.\nO que dizer, então, do direito? Nessa área do conhecimento, que é tão banhada pelo pensamento dedutivo, a possibilidade de fazer afirmações sobre o desconhecido com base na observação de alguns casos parece estranha, contraintuitiva ou até mesmo irresponsável. Afinal, como é possível saber como funcionam os processos se o caso que trabalhei no meu escritório não fez parte da amostra? É como julgar um caso sem ter acesso a todos os fatos. Falta alguma coisa. As técnicas de amostragem não deveriam funcionar no direito.\nNo entanto, é exatamente isso que faremos aqui. Vamos falar sobre amostras e populações, e como é possível fazer afirmações sobre o desconhecido com base na observação de alguns casos. Para isso, precisaremos discutir um pouco sobre como essas técnicas funcionam, através de simulações.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostras e populações</span>"
    ]
  },
  {
    "objectID": "05-amostras-populacoes.html#dados",
    "href": "05-amostras-populacoes.html#dados",
    "title": "5  Amostras e populações",
    "section": "5.2 Dados",
    "text": "5.2 Dados\nVamos continuar com os nossos dados das câmaras criminais. Para ilustrar o problema da amostragem, vamos tratar de um assunto específico agora, que é o tráfico de drogas.\n\npd.set_option('display.max_colwidth', None) # Mostrar todas as colunas\n\ndrogas = camaras[camaras['assunto'].str.contains('Tráfico de Drogas e Condutas')]\ndrogas = drogas[drogas['polo_mp'] == 'Passivo']\n\ndrogas.value_counts('assunto').reset_index()\n\n\n\n\n\n\n\n\nassunto\ncount\n\n\n\n\n0\nDIREITO PENAL - Crimes Previstos na Legislação Extravagante - Crimes de Tráfico Ilícito e Uso Indevido de Drogas - Tráfico de Drogas e Condutas Afins\n2813\n\n\n\n\n\n\n\nNesse estudo hipotético, nosso objetivo é estimar qual é a quantidade de drogas que os acusados portam. Essa informação não está na base de dados original das câmaras, e também não está nos metadados (capa, partes e movimentações) do processo. A única forma de acessar esse dado é analisando os autos dos processos.\nVamos ver um exemplo de acórdão nessa amostra.\n\ndrogas.processo.iloc[10]\n\n'15000806120208260559'\n\n\nÉ possível acessar os autos públicos do processo aqui.\nO texto relevante do acórdão, nesse caso, é esse:\n\nO Apelante foi condenado como incurso nas sanções dos artigos 33, “caput”, da Lei nº 11.343/06, porque no dia, hora e local, mencionados na denúncia, tinha em depósito, para fins de tráfico, droga consistente em 24 porções de cocaína, pesando, aproximadamente, 15,8 gramas, 5 pedras de crack, pesando, aproximadamente, 15,8 gramas e 1 tijolo de maconha, pesando, aproximadamente, 256,5 gramas, substâncias, estas, que causam dependência física e psíquica, tudo sem autorização e em desacordo com determinação legal e regulamentar.\n\nNesse caso, gostaríamos de anotar uma base de dados assim:\n\n\n\nProcesso\nCocaína\nCrack\nMaconha\n\n\n\n\n1500080-61.2020.8.26.0559\n15.8\n15.8\n256.5\n\n\n\nDigamos que, para olhar um caso, um ser humano demore aproximadamente 3 minutos. Para analisar todos os 2813 casos, essa classificação daria um total de 141 horas de trabalho, que daria quase 1 mês de trabalho para uma pessoa contratada só para isso.\nOutra alternativa, um pouco mais barata, seria usar a API do GPT-4 da OpenAI. O preço em fevereiro de 2024 da OpenAI é de 0.01 dólar para cada 1000 tokens. Assumindo que as primeiras páginas de um acórdão têm um pouco menos de 3000 tokens, conclímos que cada caso custaria aproximadamente 0.03 dólar. Para analisar todos os casos, então, o custo seria de aproximadamente 85 dólares.\nObs: Note que, só para fazer essas estimativas, já utilizamos a ideia de amostragem! Sem a amostragem, precisaríamos ler todos os casos para ver quanto é o custo e o tempo de analiar os processos.\nMas aí você pode pensar: ah, um mês nem é tudo isso. E 85 dólares é um valor pagável. Por que não fazer a classificação de todos os casos?\nBem, o que faltou foi comentar que a nossa base de dados das câmaras é uma amostra! Ela tem 10 mil casos selecionados aleatoriamente, de uma população de quase 500 mil casos. Nesse cenário, as coisas começam a ficar inviáveis: seriam anos só para classificar os processos, ou um custo de mais de uma dezena de milhares de reais. E, ainda que seja possível fazer isso tudo (por exemplo, pagando vários pesquisadores ou buscando um investimento), a pergunta que fica é: será que precisamos mesmo de todos os casos?\nA resposta é: não. A amostra é suficiente para fazer inferências sobre a população. E é isso que vamos fazer a seguir.\nPara os fins dessa apostila, não vamos gastar dinheiro aplicando o GPT-4 nem tempo lendo processos. Vamos entender como a amostragem funciona e quais as principais técnicas de amostragem.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostras e populações</span>"
    ]
  },
  {
    "objectID": "05-amostras-populacoes.html#teoria-de-amostragem",
    "href": "05-amostras-populacoes.html#teoria-de-amostragem",
    "title": "5  Amostras e populações",
    "section": "5.3 Teoria de amostragem",
    "text": "5.3 Teoria de amostragem\nAmostragem é uma área ampla de pesquisa. Veremos apenas uma parte resumida dos conceitos aqui. O mais importante, no momento, é entender dois resultados muito importantes: a lei dos grandes números e o teorema do limite central.\n\n5.3.1 Definições\nPara começar, vamos às definições, mais rígidas.\n\nObservação: é um dos elementos da amostra. No nosso caso, é um dos recursos criminais.\nPopulação: é o conjunto de todas as observações. No nosso caso, é o conjunto de todos os recursos criminais com acórdãos publicados entre 2016 e 2023.\nAmostra: é qualquer subconjunto da população. No nosso caso, pode ser qualquer subconjunto dos recursos que podemos acessar.\n\nUma amostra pode ser aleatória ou não aleatória. Quando a amostra é aleatória, ela pode ser uma amostra aleatória simples (quando todas as observações têm a mesma probabilidade de ser selecionados) ou uma amostragem complexa (quando a probabilidade de seleção de cada observação é diferente).\nEm estudos qualitativos, é muito comum realizar amostras não aleatórias. Pesquise, por exemplo, sobre amostragem por conveniência, ou então pelo método bola de neve. Em estudos quantitativos, a amostragem aleatória é a mais comum. No entanto, existem técnicas de amostragem não aleatórias, como cotas, que são utilizadas em estudos quantitativos.\n\n\n5.3.2 Lei dos grandes números\nA lei dos grandes números é um dos teoremas fundamentais da estatística, e postula que a média de uma amostra aleatória de uma população tende a se aproximar da média da população à medida que o tamanho da amostra aumenta. Em outras palavras, se você pegar uma amostra grande o suficiente, a média da amostra será muito próxima da média da população.\nPara ilustrar essa lei, vamos fazer uma simulação com algo que sabemos o resultado de antemão. Por exemplo, uma moeda honesta. A moeda honesta é aquela que tem probabilidade de 50% de sair “cara” e 50% de sair “coroa” (e ela não cai em pé 😅). O que acontece se jogarmos essa moeda para cima 30 vezes e calcularmos a proporção de caras? Vamos ver no python.\n\n# simulador de moedas\n# para simular uma moeda honesta, usamos a função random.choice do módulo random\n\nnp.random.seed(42) # isso é para dar reprodutibilidade\n\nprint(np.random.choice(['cara', 'coroa']))\n\nresultados = []\n\nfor i in range(1000):\n  resultados.append({'jogada': i + 1, 'resultado': np.random.choice(['cara', 'coroa'])})\n\nresultados = pd.DataFrame(resultados)\n\nresultados.head(10)\n\ncara\n\n\n\n\n\n\n\n\n\njogada\nresultado\n\n\n\n\n0\n1\ncoroa\n\n\n1\n2\ncara\n\n\n2\n3\ncara\n\n\n3\n4\ncara\n\n\n4\n5\ncoroa\n\n\n5\n6\ncara\n\n\n6\n7\ncara\n\n\n7\n8\ncara\n\n\n8\n9\ncoroa\n\n\n9\n10\ncara\n\n\n\n\n\n\n\nAgora, vamos calcular a média de caras ao longo das jogadas. Podemos fazer isso com uma média cumulativa, que vai calculando as proporções de caras para cada jogada.\n\nresultados['media_acumulada'] = (resultados['resultado'] == 'cara').cumsum() / (resultados.index + 1)\nresultados.head(10)\n\n\n\n\n\n\n\n\njogada\nresultado\nmedia_acumulada\n\n\n\n\n0\n1\ncoroa\n0.000000\n\n\n1\n2\ncara\n0.500000\n\n\n2\n3\ncara\n0.666667\n\n\n3\n4\ncara\n0.750000\n\n\n4\n5\ncoroa\n0.600000\n\n\n5\n6\ncara\n0.666667\n\n\n6\n7\ncara\n0.714286\n\n\n7\n8\ncara\n0.750000\n\n\n8\n9\ncoroa\n0.666667\n\n\n9\n10\ncara\n0.700000\n\n\n\n\n\n\n\nPelos resultados, parece que a média está em torno de 70% de caras! Será que tem algo errado? Vamos fazer um gráfico da média acumulada.\n\np = sns.lineplot(resultados, x = 'jogada', y = 'media_acumulada')\np.axhline(0.5, color = 'red', linestyle = '--')\np\n\n\n\n\n\n\n\n\nVeja como a média vai se aproximando de 50% conforme vamos coletando mais dados. Essa é a lei dos grandes números em funcionamento. A média da amostra vai se aproximando da média da população à medida que coletamos mais dados.\nVejamos um exemplo agora, mas com nossa base de dados das câmaras. Vamos estimar o tempo médio de duração dos processos, coletando uma amostra aleatória de casos, variando entre 100 e 2000 como tamanho de amostra.\n\n# cria uma sequência de 100 a 2000, de 50 em 50\nsequencia = list(range(100, 5001, 50))\n\n# ordena a base de câmaras em ordem aleatória\ncamaras_aleatorio = camaras.sample(len(camaras), random_state=1)\n\n# para cada valor i da sequência, pegamos uma amostra dos dados de tamanho i e calculamos a média\n\nmedias = []\n\nfor i in sequencia:\n  medias.append({'tamanho_amostra': i, 'media': camaras_aleatorio.head(i)['tempo'].mean()})\n\ndf_medias = pd.DataFrame(medias)\n\ndf_medias.head()\n\n\n\n\n\n\n\n\ntamanho_amostra\nmedia\n\n\n\n\n0\n100\n2.879261\n\n\n1\n150\n3.016181\n\n\n2\n200\n3.227844\n\n\n3\n250\n3.191086\n\n\n4\n300\n3.211791\n\n\n\n\n\n\n\n\np = sns.lineplot(df_medias, x = 'tamanho_amostra', y = 'media')\np.axhline(camaras['tempo'].mean(), color = 'red', linestyle = '--')\np\n\n\n\n\n\n\n\n\nÉ inevitável! Sim, porque esse é um resultado matemático.\nE a conclusão dele é muito intuitiva: se coletarmos mais dados, chegamos mais próximos da estimativa que queremos. Um teorema um pouco menos intuitivo é o teorema do limite central.\n\n\n5.3.3 Teorema Central do Limite\nO teorema central do limite (TCL) postula que, para uma amostra aleatória de tamanho suficientemente grande, a distribuição da média amostral segue uma distribuição normal. Em outras palavras, se coletarmos uma amostra grande o suficiente, a distribuição das médias amostrais será aproximadamente normal.\nNa apostila sobre probabilidade e curva normal, vamos dissecar essa ‘curva normal’, também chamada de ‘gaussiana’, por conta do matemático Carl Friedrich Gauss. Por enquanto, vamos fazer uma simulação para entender como a distribuição das médias amostrais se comporta. Vamos utilizar o exemplo dos tempos dos processos.\nMas como vamos avaliar se o resultado é verdadeiro, se nem sabemos o que é curva normal nem distribuição de probabilidades?\nBem, primeiro, vamos lembrar das aulas sobre visualização de dados, e do histograma. O histograma é uma forma de visualizar a distribuição dos dados (como eles se distribuem e se aglomeram ao longo do eixo x). Outra forma é com o gráfico da densidade, que literalmente estima a distribuição de probabilidade a partir dos dados.\nEntão, vamos avaliar como se distribui uma lista de médias que vamos calcular com amostras aleatórias dos dados, e compará-la com essa distribuição normal.\nVamos ver, então, essa tal distribuição normal. Você vai notar que ela soa familiar:\n\n# plotar a distribuição normal\n\nx = np.linspace(-5, 5, 1000)\n\ndf = pd.DataFrame({'x': x, 'densidade': norm.pdf(x)})\n\nsns.lineplot(df, x = 'x', y = \"densidade\")\n\n\n\n\n\n\n\n\nEssa é a distribuição normal. Mais especificamente, a distribuição normal padrão (que está centrada no zero e tem variância 1). A distribuição normal é uma das distribuições mais importantes da estatística, e é a base de muitos resultados teóricos. Ela é simétrica, tem forma de sino, e é caracterizada por dois parâmetros: a média e a variância / desvio padrão.\nAgora vamos à nossa simulação.\n\n# simular 1000 amostras de tamanho 100 a partir da base das câmaras e coletar as médias\n# guardar em um dataframe do pandas\n\nmedias = []\n\nfor i in range(1000):\n  amostra = camaras.sample(100, random_state = i + 1)\n  medias.append({'amostra': i + 1, 'media': amostra['tempo'].mean()})\n\ndf_medias = pd.DataFrame(medias)\n\ndf_medias.head(10)\n\n\n\n\n\n\n\n\namostra\nmedia\n\n\n\n\n0\n1\n2.879261\n\n\n1\n2\n3.387187\n\n\n2\n3\n3.535743\n\n\n3\n4\n3.016345\n\n\n4\n5\n3.356687\n\n\n5\n6\n3.062560\n\n\n6\n7\n3.110308\n\n\n7\n8\n3.248706\n\n\n8\n9\n3.032799\n\n\n9\n10\n3.127146\n\n\n\n\n\n\n\nVamos fazer um histograma com as 30 primeiras médias que calculamos.\n\nsns.displot(df_medias.head(30), x = 'media')\n\n\n\n\n\n\n\n\nAgora, com as 300 primeiras médias.\n\nsns.displot(df_medias.head(300), x='media')\n\n\n\n\n\n\n\n\nEstá ficando mais interessante! Agora com todas as 1000 amostras. E adicionando um kde=True para visualizar melhor a curva.\n\nsns.displot(df_medias, x='media', kde=True)\n\n\n\n\n\n\n\n\nNão está muito parecido com a curva normal? Pois bem, o resultado do TCL é exatamente esse.\nMas uma coisa que você pode estar pensando é: qual é a utilidade disso? Por que esse teorema é tão importante, a ponto de ganhar esse nome ‘Central’?\nO ponto é que podemos usar o fato da média que estamos estimando ter uma distribuição parecida com a normal para calcular margens de erro. Com base na normal, conseguimos criar intervalos de confiança para o valor de interesse. Para isso, usamos o TCL.\n\n\n5.3.4 Conclusões até agora\nCom base no que vimos até agora, estamos seguros de que i) quanto mais dados coletarmos, melhor nos aproximamos do valor de interesse e ii) caso a gente extraia apenas uma amostra de casos, coletada de forma aleatória, podemos fazer afirmações sobre a população com base nessa amostra usando a distribuição normal.\nResta agora saber como conduzir uma pesquisa amostral.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostras e populações</span>"
    ]
  },
  {
    "objectID": "05-amostras-populacoes.html#técnicas-de-amostragem",
    "href": "05-amostras-populacoes.html#técnicas-de-amostragem",
    "title": "5  Amostras e populações",
    "section": "5.4 Técnicas de amostragem",
    "text": "5.4 Técnicas de amostragem\nComo vimos na seção anterior, o principal requisito para que possamos tomar conclusões sobre a população com base em uma amostra é que a amostra seja coletada de forma aleatória. Isso significa que cada elemento da população deve ter a mesma probabilidade de ser selecionado para a amostra.\nNa vida real, no entanto, nem sempre temos uma lista de todos os indivíduos que gostaríamos de analisar. Em pesquisas eleitorais, por exemplo, não é fácil obter uma lista de todos os brasileiros e, mesmo se tivéssemos essa lista, acessar todas essas pessoas pode ser algo bem complicado. Por isso que a maioria dessas pesquisas é realizada por cotas, coletando dados até atingir determinadas proporções de pessoas a partir de características demográficas conhecidas (como a proporção de mulheres na população, fornecida pelo Censo ou PNAD-Contínua).\nFelizmente, no direito, não é tão difícil nos depararmos com situações onde temos a lista completa de processos. Esse é o caso da pesquisa das câmaras criminais, por exemplo. Temos acesso a todos os casos e precisamos apenas realizar a amostra aleatória.\nOutro tipo comum de amostragem em projetos de ciência de dados no direito é a amostragem sistemática. Partindo de uma lista com todos os casos, essa técnica consiste em selecionar um caso e pular um número fixo de casos (por exemplo, 10). Esse tipo de amostragem é bastante útil em pesquisas do direito, pois, geralmente, os resultados de uma consulta de jurisprudência vêm em um site com paginação, sem que o tribunal forneça a lista completa de casos. É possível extrair a lista completa utilizando técnicas de raspagem de dados (foi o que fizemos para montar a base das câmaras), mas pode ser que isso dê muito trabalho. Ao invés disso, podemos selecionar um caso, pular 10, selecionar outro caso, pular 10, etc.\nO quanto pulamos depende do tamanho da população e o tamanho da amostra desejado. Por exemplo, se uma pesquisa de jurisprudência contém 2000 casos, e queremos analisar 100, podemos pular de 20 em 20. Dessa forma, conseguimos cobrir toda a lista de resultados de forma igualmente espaçada. Essa lista é muito próxima do que teríamos obtido por uma amostragem simples.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostras e populações</span>"
    ]
  },
  {
    "objectID": "05-amostras-populacoes.html#tamanho-da-amostra",
    "href": "05-amostras-populacoes.html#tamanho-da-amostra",
    "title": "5  Amostras e populações",
    "section": "5.5 Tamanho da amostra",
    "text": "5.5 Tamanho da amostra\nTem um elefante na sala. Esse elefante é: qual é o tamanho da amostra que devo considerar para confiar nos meus resultados?\nEssa é uma pergunta mais complicada do que parece. O principal erro que as pessoas cometem é acreditar que o tamanho da amostra está relacionado com o tamanho da população. Por exemplo, se a população tem 1000 casos, então pegar 30% desses casos já está bom. Na verdade, pode ser que seja bom, e pode ser que não seja!\nO tamanho da amostra deve considerar como base dois critérios: i) qual é a variabilidade dos dados e ii) qual é a precisão que se deseja alcançar na estimativa. Se os dados variam muito, precisamos de mais dados para conseguir estimar o que queremos. Se os dados variam pouco, podemos pegar menos casos e isso já é suficiente. Sobre a precisão, isso está relacionado à incerteza: se o objetivo for afirmar algo com 90% de confiança, precisamos de menos dados do que se o objetivo for afirmar a mesma coisa com 95% de confiança.\nExiste ate uma fórmula de dimensionamento de amostra para estimar a média, mas ela é relativamente rara de ser utilizada:\n\\[n = \\frac{s^2}{d^2} (z_{1-\\alpha/2})^2\\]\nNessa fórmula, \\(s\\) é uma estimativa do desvio padrão dos dados, ou seja, da variabilidade. Quanto maior esse valor, maior o tamanho da amostra. No caso dos tempos, a estimativa do desvio padrão é de 2.18. Na parte de baixo, temos \\(d=(\\bar{x} - \\mu)\\), que é a diferença máxima que aceitamos encontrar entre o que estamos estimando e a média encontrada. Por exemplo, podemos escolher o valor de 3 meses, ou 0.25. Finalmente, o valor \\(z_{1-\\alpha/2}\\) é retirado da distribuição normal (veremos mais adiante), em função de um erro \\(\\alpha\\) que consideramos (geralmente o valor de \\(\\alpha\\) 5% e o valor de \\(z_{1-\\alpha/2}\\) é 1.96).\nObs: tecnicamente o valor de \\(z\\) deveria seguir outra distribuição, chamada \\(t\\)-student, mas é melhor não complicar as coisas agora!\nEntão, no nosso caso, na análise de tempos, poderíamos dimensionar a amostra da seguinte forma:\n\nn = 2.18**2 / 0.25**2 * 1.96**2\n\nnp.ceil(n) # arredonda para cima\n\n293.0\n\n\nEntão, chegamos a um valor de 293. Mas existem alguns problemas que você pode estar pensando.\nPrimeiro, esse valor de 2.18 foi estimado com os nossos dados, certo? Mas em tese nós não teríamos os dados para realizar essa estimativa. Geralmente, isso é fruto de uma pesquisa piloto, que fazemos só para estimar a variabilidade (por exemplo, com 10 ou 20 casos).\nSegundo, esse valor de \\(d\\) é arbitrário, então eu poderia usar qualquer valor. De fato, ele é arbitrário, e geralmente é definido por especialistas na área. Pode exemplo, para \\(d\\) entre 0 e 1, até que parece razoável errar, mas não é razoável termos \\(d=3\\), por exemplo (errar 3 anos não faz sentido para essa análise).\nTerceiro, e mais importante: e se eu não tiver dinheiro/tempo para coletar os dados para completar esse tamanho de amostra? Bem, esse é um dos principais motivos para o qual essa fórmula não é tão usada no mundo real. A verdade é que os problemas práticos de execução acabam sendo mais importantes do que o dimensionamento da amostra em si. No nosso caso das drogas, por exemplo, a solução seria definir um orçamento e pegar o máximo que der dentro desse orçamento.\nExistem, claro, técnicas que levam em conta tanto as propriedades de uma boa amostra quanto o custo de obtê-la. Mas isso ficaria para um curso mais avançado.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostras e populações</span>"
    ]
  },
  {
    "objectID": "06-probabilidade-cnormal.html",
    "href": "06-probabilidade-cnormal.html",
    "title": "6  Probabilidade e curva normal",
    "section": "",
    "text": "6.1 Introdução\nAté agora, fizemos a revisão de estatísticas básicas: proporções, média, mediana, desvio padrão. Depois, vimos um pouco sobre amostragem e suas técnicas. Está chegando a hora de unir as duas coisas: como que, a partir de proporções, médias, medianas e desvios padrão calculadas em uma amostra, podemos fazer afirmações sobre a população?\nAlgumas coisas das apostilas passadas foram colocadas sem detalhe. Por exemplo, a fórmula do desvio padrão, a distribuição normal e aquele valor \\(z\\) que apareceu na fórmula da amostra.\nPara amarrar tudo isso, precisamos discutir sobre probabilidade. A probabilidade é a ferramenta matemática que utilizamos para fazer afirmações sobre o desconhecido a partir de uma amostra. É a base da inferência estatística.\nComo este não é um curso de estatística, vamos discutir probabilidade de forma intuitiva, sem mostrar a matemática por trás disso (que é linda, mas fica para outra oportunidade!). Aqui, veremos aspectos pertinentes aos nossos estudos, o mínimo necessário para trabalhar com estatística inferencial.\nContinuaremos com o case das câmaras criminais. Dessa vez, nosso objetivo é estimar a probabilidade de perder um caso criminal em duas situações: antes e depois de saber em qual câmara meu caso será julgado.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidade e curva normal</span>"
    ]
  },
  {
    "objectID": "06-probabilidade-cnormal.html#conectando-os-conceitos",
    "href": "06-probabilidade-cnormal.html#conectando-os-conceitos",
    "title": "6  Probabilidade e curva normal",
    "section": "6.2 Conectando os conceitos",
    "text": "6.2 Conectando os conceitos\nVocê sabia que estava trabalhando com probabilidades desde o começo da disciplina? Ocorre que esses conceitos foram apresentados de forma mais prática, sem a formalização matemática. Por exemplo, quando falamos sobre a média, estamos falando sobre a soma das probabilidades de uma variável assumir certo valor, multiplicadas por esses valores (chamamos isso de esperança matemática).\nMas vamos formalizar um pouco mais.\nPartimos do conceito de amostra. A amostra é um subconjunto de observações da população. A partir dela, podemos calcular a média, a mediana, o desvio padrão, proporções, etc.\nEssas medidas que resumem a amostra são chamadas de estatísticas. Isso mesmo, estatísticas. Por exemplo, a média é uma estatística. O desvio padrão é uma estatística. A proporção de casos ganhos é uma estatística. Tecnicamente, uma estatística é uma função das observações da amostra. Ou seja, a estatística não é um número, não é o valor calculado como a média, por exemplo. A estatística é a fórmula que calcula a média.\nNa maioria das situações, as estatísticas estão relacionadas a um parâmetro de interesse. O parâmetro é uma característica da população. Por exemplo, a média na população, a o desvio padrão na população, etc. Geralmente, esses valores são desconhecidos, e descrevemos eles com letras gregas, como \\(\\mu\\) (média na população) e \\(\\sigma\\) (desvio padrão na população).\nChamamos uma estatística de estimador quando ela assume valores no mesmo escopo de variação do parâmetro (por exemplo). Por exemplo, a média da amostra é um estimador da média da população, pois assume valores no mesmo intervalo (no caso, qualquer número). O desvio padrão amostral é um estimador do desvio padrão da população, pois assume valores no mesmo intervalo (no caso, qualquer número positivo). A proporção de casos ganhos na amostra é um estimador da probabilidade de ganhar um caso na população (que varia entre 0 e 1).\nComo o estimador é uma estatística, ele também é uma função da amostra, uma fórmula a ser aplicada aos dados. O valor que sai dessa fórmula é chamado de estimativa.\nVárias definições!! Respire.\nVimos até agora, então, estimador, que é um tipo especial de estatística. O estimador é só uma fórmula, uma função das observações da amostra. O valor que sai dessa fórmula é chamado de estimativa. E nós gostaríamos que essa estimativa fosse próxima do parâmetro.\nVamos a um exemplo no python, para analisar o tempo.\n\n# obs: a base das câmaras que temos aqui é uma amostra da base completa, com 10 mil casos. A base completa tem mais de 500 mil.\n\n# não sabemos a média populacional\nmedia_populacional = np.nan\n\n# exemplo de estatística: a soma dos valores\n# ela não é um bom estimador da média populacional, concorda?\ndef exemplo_estatistica(x):\n  return x.sum()\n\n# esse é nosso estimador de média. Ele também é uma estatística\ndef estimador_media(x):\n  return x.mean()\n\nestimativa_media = estimador_media(camaras['tempo'])\n\nMas o parâmetro é desconhecido, então como verificar se a estimativa é próxima do parâmetro? No lugar disso, o que fazemos é estudar quais são as proprieades do estimador. Por exemplo, se ele é viesado, se ele é consistente, se ele é eficiente, etc. Não vamos discutir essas propriedades aqui, mas é importante saber que elas existem e que existem algumas propriedades desejáveis.\nPara estudar as propriedades do estimador, a ideia é estudar quais valores ele pode assumir e qual a probabilidade dele assumir esses valores. Por exemplo, ao coletar uma amostra de 100 casos, qual seria a estimativa? Ao coletar mil amostras, como seria a distribuição das estimativas? É exatamente o que vimos no TCL da apostila sobre amostragem: os valores dessas estimativas ficam próximos de uma distribuição de probabilidades normal.\nO que é, então esse estimador? Ele é o que chamamos de variável aleatória. Uma variável aleatória é uma função que recebe um conjunto de eventos aleatórios (no caso, os eventos que originaram a amostra) e retorna um valor (no caso, a estimativa).\nO que mais se faz na estatística é estudar essas tais variáveis aleatórias. Existem variáveis aleatórias com distribuição normal, Bernoulli, binomial, Poisson, exponencial, e assim por diante. A distribuição normal certamente é a mais famosa, e é a que mais aparece na estatística. Para nós, as mais importantes são a normal e a Bernoulli.\nPelo TCL, vimos que as estimativas da média (os valores) têm um histograma parecido com a função densidade de probabilidades da normal (a curva de sino). Isso equivale a dizer que o estimador da média tem, aproximadamente, uma distribuição normal.\nÉ aí que entra a probabilidade. A probabilidade é a ferramenta matemática que utilizamos para estudar as propriedades dessas variáveis aleatórias, como os estimadores. E é a partir disso que conseguiremos fazer intervalos de confiança, testes de hipótese, etc.\n\nObs: se você achou essa parte da apostila difícil, é porque ela é mesmo! Aqui, temos condensados, em poucos parágrafos, mais de um mês de aulas em um curso padrão de estatística! A ideia aqui foi apresentar os termos técnicos e dar o panorama geral do motivo pelo qual estamos estudando probabilidade.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidade e curva normal</span>"
    ]
  },
  {
    "objectID": "06-probabilidade-cnormal.html#definição-de-probabilidade",
    "href": "06-probabilidade-cnormal.html#definição-de-probabilidade",
    "title": "6  Probabilidade e curva normal",
    "section": "6.3 Definição de probabilidade",
    "text": "6.3 Definição de probabilidade\nA probabilidade é uma função matemática \\(\\mathbb P()\\) que recebe como valor de entrada um conjunto e retorna a medida desse conjunto. Por exemplo, pense em uma régua.\n\nImagine que os eventos que eu tenho interesse (por exemplo, vai chover amanhã ou não) possam ser representados por um subconjunto dessa régua. A probabilidade de um evento é a medida desse subconjunto.\nIsso pode parecer um pouco abstrato, e é mesmo, afinal eu não consigo pegar um evento e medir com uma régua. Isso é uma abstração matemática. Quanto mais estudamos matemática, mais abstrata ela fica.\nAinda que seja abstrata, a probabilidade um objeto matemático muito bem definido. Ela é definida pelos famosos axiomas de Kolmogorov:\n\nA probabilidade de um evento é sempre um número entre 0 e 1. Ou seja, \\(0 \\leq \\mathbb P(A) \\leq 1\\).\nA probabilidade do conjunto vazio é 0. Ou seja, \\(\\mathbb P(\\emptyset) = 0\\).\nA probabilidade da união de dois eventos disjuntos é a soma das probabilidades de cada evento. Ou seja, se \\(A\\) e \\(B\\) são disjuntos, então \\(\\mathbb P(A \\cup B) = \\mathbb P(A) + \\mathbb P(B)\\).\n\nA partir desses axiomas, conseguimos derivar absolutamente tudo o que precisamos para definir variáveis aleatórias, estudar propriedades, fazer intervalos de confiança, testes de hipótese, ajustar modelos de aprendizado de máquinas, criar LLMs etc. Eles são a base de tudo.\nE bem, se olharmos bem, os pontos (1) e (2) parecem até óbvios, certo? A probabilidade ser um número entre 0 e 1 é algo já bem aceito na sociedade. E a probabilidade do conjunto vazio ser 0 também é algo que parece fazer sentido, afinal a probabilidade de “nada” deve ser “nada”, que é zero.\nO ponto (3) é um pouco mais complicado, mas também é algo que parece fazer sentido. Por exemplo, digamos que os eventos de interesse sejam tirar o número 1 de um dado e tirar o número 2 de um dado. Esses eventos são disjuntos, porque eu não consigo tirar o número 1 e o número 2 ao mesmo tempo. A probabilidade de tirar o número 1 é 1/6, e a probabilidade de tirar o número 2 é 1/6. A probabilidade de tirar o número 1 ou o número 2 é 1/6 + 1/6 = 1/3. Isso parece fazer sentido.\nObs: tecnicamente, esse ponto (3) tem uma notação um pouco diferente, porque essa regra na verdade vale para uma união de qualquer coleção enumerável de conjuntos disjuntos. Mas para nossos propósitos, é suficiente.\nAs coisas começam a ficar mais divertidas quando discutimos não a definição de probabilidade, mas a interpretação da probabilidade. Esse, sim, é um tópico controverso!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidade e curva normal</span>"
    ]
  },
  {
    "objectID": "06-probabilidade-cnormal.html#interpretação-da-probabilidade",
    "href": "06-probabilidade-cnormal.html#interpretação-da-probabilidade",
    "title": "6  Probabilidade e curva normal",
    "section": "6.4 Interpretação da probabilidade",
    "text": "6.4 Interpretação da probabilidade\nExistem duas possíveis correntes de pensamento (duas ‘escolas’, por assim dizer) da estatística. Essas correntes têm definições diferentes sobre o que a probabilidade significa. Veja bem, a definição matemática da probabilidade não é motivo de discussão. Não há discordância sobre os axiomas de Kolmogorov e os elementos supracitados. O que muda é a intepretação.\nA primeira é a interpretação frequentista. A interpretação frequentista entende que a probabilidade de um evento acontecer é precisamente o resultado da aplicação da Lei dos Grandes Números a este evento. Ou seja, a probabilidade de um evento é a frequência relativa com que ele acontece em um número muito grande de experimentos. Por exemplo, a probabilidade de tirar o número 1 em um dado é 1/6, porque se eu jogar o dado muitas vezes, o número 1 vai aparecer em 1/6 das vezes. Da mesma forma, a probabilidade de um processo ser considerado procedente é a frequência relativa com que ele é considerado procedente em um número muito grande de processos.\n\nFonte: Fundamentos de estatística bayesiana\nPara muitos, essa é a definição mais intuitiva. Afinal, ela é a definição que liga todos os conceitos que mostramos antes. A probabilidade é simplesmente um parâmetro desconhecido da população, que estimamos usando os dados. A interpretação frequentista é a que mais se aproxima do senso comum.\nO problema é que, em algumas situações, a interpretação frequentista não funciona muito bem. Por exemplo, como calcular a probabilidade de um evento que só acontece uma vez? Ou a probabilidade de um evento que nunca aconteceu? Ou a probabilidade de um evento que acontece em um número finito de vezes? A interpretação frequentista só consegue responder a essas perguntas fazendo abstrações, avaliando “o que aconteceria se fosse possível repetir o experimento muitas vezes”.\nÉ o exemplo da piada do XKCD abaixo. É uma discussão entre dois estatísticos, um frequentista e outro bayesiano (veremos em seguida). A conclusão do frequentista é que o sol explodiu, já que a probabilidade dele não ter explodido é de apenas 0.027 pelo experimento realizado. Mas obviamente o sol não explodiu, pois, se tivesse, eles não estariam mais ali. O problema da estatística frequentista é justamente essa necessidade de abstrair muito e se afastar da realidade quando o evento de interesse não é fruto de um experimento que pode se repetir infinitas vezes.\n\nFonte: XKCD\nE então temos a interpretação subjetivista. Os bayesianos, ou subjetivistas, entendem que a probabilidade não existe no mundo real. Ela é uma ferramenta que o ser humano usa para mensurar sua incerteza a respeito de um fenômeno. Por consequência, a probabilidade de um evento acontecer é subjetiva, pode variar de pessoa para pessoa.\nPare para pensar sobre qual probabilidade faz mais sentido para você.\nPara toda nossa disciplina, estamos utilizando a interpretação frequentista. Nada impede, no entanto, que seja utilizada a interpretação subjetivista. Algumas fórmulas e técnicas de inferência mudam, mas no fundo ambas tentam entender mais sobre o mundo que nos cerca a partir dos dados que observamos.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidade e curva normal</span>"
    ]
  },
  {
    "objectID": "06-probabilidade-cnormal.html#variável-aleatória",
    "href": "06-probabilidade-cnormal.html#variável-aleatória",
    "title": "6  Probabilidade e curva normal",
    "section": "6.5 Variável aleatória",
    "text": "6.5 Variável aleatória\nUma variável aleatória é uma função que recebe um conjunto de eventos aleatórios (no caso, os eventos que originaram a amostra) e retorna um número. Ou seja, a variável aleatória é o mapeamento que levam os eventos que acontecem aos números. Por isso que elas são tão importantes para a estatística: é com base em variáveis aleatórias que conseguimos fazer as contas que precisamos, para criar estimadores e estudar suas propriedades. Geralmente, denotamos variáveis aleatórias com letras maiúsculas, como \\(X\\) e \\(Y\\).\nO exemplo mais simples de variável aleatória é a \\(Bernoulli(p)\\). Esta é a variável aleatória “moeda”, cujo nome é em homenagem a um dos primeiros probabilistas mais importantes da história, Jakob Bernoulli.\nSe uma variável \\(X\\) tem distribuição de Bernoulli, denotamos por \\(X \\sim Bernoulli(p)\\). Essa variável pode assumir os valores 1 e 0, e a probabilidade dela assumir o valor 1 é dada por \\(p\\), que é um parâmetro dessa variável aleatória.\nNo caso das câmaras criminais, podemos pensar \\(X\\) como a reforma do recurso. Se em um novo processo, \\(X\\) vale 1, o recurso é reformado. Se \\(X\\) vale zero, o recurso não é reformado. A probabilidade de reforma, que é o que queremos estimar, é dada por \\(p\\). Em fórmulas, temos\n\\[\n\\mathbb P(\\text{reforma}) = \\mathbb P(X=1) = p\n\\]\n\\[\n\\mathbb P(\\text{não reforma}) = \\mathbb P(X=0) = 1 - \\mathbb P(X=1) = 1-p\n\\]\nAgora, digamos que nosso interesse é estudar a quantidade de reformas em uma amostra de 10 casos. Como as variáveis aleatórias assumem valores numéricos, podemos simplesmente fazer a soma dessas variáveis: \\(X_1 + X_2 + \\dots + X_{10}\\). É por isso que a variável aleatória é um objeto matemático tão útil. As variáveis conectam as probabilidades de eventos abstratos com números que podemos usar nas contas.\nEm particular, a soma dessas 10 variáveis aleatórias também é uma variável aleatória. O nome dessa distribuição é binomial. Mas não vamos descrevê-la aqui para não sair do foco. Na verdade, qualquer operação de um conjunto de variáveis aleatórias, também é uma variável aleatória.\nE é aí que chegamos no estimador: o estimador da média, que denotamos por \\(\\bar X\\), é a média de um conjunto de variáveis aleatórias. No caso acima, por exemplo, temos\n\\[\n\\bar X = \\frac{X_1 + X_2 + \\dots + X_{10}}{10} = \\frac{1}{10} \\sum_{i=1}^{10} X_i\n\\]\nVeja que essa fórmula é exatamente igual ao que vimos na apostila de medidas de posição e variabilidade. A única diferença é que estamos usando \\(X\\) no lugar de \\(x\\). Quando estamos trabalhando com dados, usamos \\(x\\) minúsculo para deixar claro que estamos fazendo uma estimativa, e não definindo um estimador.\nDefinimos, então, o estimador média, dado por\n\\[\n\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\]\nPelo TCL, sabemos que, pelo menos de forma aproximada, \\(\\bar X_n \\approx \\mathcal N(?,?)\\), sendo que \\(\\mathcal N\\) é o símbolo da distribuição normal, que é especificada pelos parâmetros da média e da variância, que ainda não sabemos quais são.\nA verdade é que e possível descobrir quais são esses parâmetros da normal. Mas, para mostrar isso, teríamos de mostrar muitos conceitos novos, que fogem do escopo da disciplina. Por isso, vamos apenas colocar aqui:\n\\[\n\\text{Média}(\\bar X_n) = \\mu,\n\\]\nonde \\(\\mu\\) é a média populacional (um número. Por exemplo, na análise do tempo dos processos, é o tempo médio de toda a população de casos). Ou seja, a média da média é a média (😅). Explicando melhor: o valor esperado (\\(\\text{Média}\\)) do estimador média (\\(\\bar X_n\\)) é a média populacional (\\(\\mu\\)). Essa é uma propriedade conhecida como “não enviesado”, já que, o que eu espero obter da média amostral é a média populacional.\nVamos para a variância:\n\\[\n\\text{Variância}(\\bar X_n) = \\frac{\\sigma^2}{n},\n\\]\nonde \\(\\sigma^2\\) é a variância populacional (também um número. Por exemplo, na análise do tempo dos processos, é a variância dos tempos de toda a população de casos). Ou seja, a variância da média é a variância populacional dividida pelo número de observações. A intuição dessa divisão é que, quanto mais dados nós coletamos, menos varia nosso estimador e, portanto, mais próximos ficamos do “verdadeiro” valor da média. Reflita um pouco sobre isso!\nLogo acima, conseguimos descrever as probabilidades da distribuição Bernoulli, que é muito fácil, já que ela só pode assumir dois valores. No caso da distribuição normal, isso é mais complicado, porque ela pode assumir qualquer valor real (qualquer número entre menos infinito e infinito). Para descrevê-la, utilizamos a função densidade de probabilidades. Essa função não retorna exatamente a probabilidade, mas o “peso” de certo valor para aquela distribuição. A probabilidade mesmo é calculada a partir da área dessa função densidade de probabilidades.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidade e curva normal</span>"
    ]
  },
  {
    "objectID": "06-probabilidade-cnormal.html#distribuição-normal",
    "href": "06-probabilidade-cnormal.html#distribuição-normal",
    "title": "6  Probabilidade e curva normal",
    "section": "6.6 Distribuição normal",
    "text": "6.6 Distribuição normal\nA fórmula da função densidade de probabilidades da normal é um pouco cruel. Vamos colocá-la aqui, mas certamente não esperamos que isso seja entendido nem decorado. Se \\(Y\\sim \\mathcal N(\\mu, \\sigma^2)\\), sua função densidade de probabilidades é dada por:\n\\[\nf_Y(y|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right),\n\\]\nem que \\(\\pi\\) é o número \\(\\pi\\) que conhecemos (3,1415…), \\(\\mu\\) é a média, \\(\\sigma^2\\) é a variância, e \\(\\exp\\) é a função exponencial (\\(g(x) = e^x\\)).\nSe você entendeu essa função, parabéns! Se não, não há problema nenhum. O que é importante de entender é que essa função descreve a curva normal que vimos anteriormente:\n\nx = np.linspace(-5, 5, 1000)\n\ndf = pd.DataFrame({'x': x, 'densidade': norm.pdf(x)})\n\nsns.lineplot(df, x = 'x', y = \"densidade\")\n\n\n\n\n\n\n\n\nNo caso, o que plotamos acima é a \\(\\mathcal N(0,1)\\) (Lê-se normal zero um), também chamada de normal padrão.\nUma propriedade interessante da normal é que podemos transformá-la em uma normal padrão facilmente. Por exemplo, se \\(Y \\sim \\mathcal N(\\mu, \\sigma^2)\\), então\n\\[\nZ = \\frac{Y-\\mu}{\\sigma}\n\\]\ntem distribuição \\(\\mathcal N(0,1)\\). O ato de subtrair a média e dividir pelo desvio padrão é tão comum no dia a dia de cientistas de dados que tem até um nome: padronização.\nPor exemplo, no python:\n\ntempo = camaras.tempo\n\nmedia_tempos = camaras.tempo.mean()\nsd_tempos = camaras.tempo.std()\n\n\ntempo_padronizado = (tempo - media_tempos) / sd_tempos\n\ntempo_padronizado.head(10)\n\n0   -0.450363\n1   -0.730058\n2    1.438524\n3   -0.480464\n4   -1.044873\n5    0.817675\n6   -0.461651\n7   -0.659821\n8    0.208114\n9    1.265439\nName: tempo, dtype: float64\n\n\nComo calcular probabilidades na distribuição normal padrão?\nBem, olhando para a função densidade de probabilidades, ela é simétrica em torno do zero. Disso, é possível concluir, intuitivamente, que \\(\\mathbb P(Z&gt;0) = 0.5\\), e que \\(\\mathbb P(Z&lt;=0) = 0.5\\). Mas e outros valores?\nInfelizmente, não há uma fórmula matemática para calcular as probabilidades de qualquer intervalo. A princípio isso é contra intuitivo, mas é verdade: é possível demonstrar, matematicamente, que não conseguimos escrever uma fórmula que calcula a área dessa função para qualquer intervalo de valores.\nNo entanto, é possível obter esses valores de forma computacional. Existem várias técnicas para isso, que não vamos discutir por aqui. O importante é que existe uma função no python que faz exatamente isso para nós, a norm.cdf. Vejamos exemplos:\n\n# função auxiliar para representar as contas que estamos fazendo\ndef normal_pintada(a, b):\n  x = np.linspace(-5, 5, 1000)\n  df = pd.DataFrame({'x': x, 'densidade': norm.pdf(x)})\n  sns.lineplot(df, x = 'x', y = \"densidade\")\n  if np.isinf(a):\n    a = -5\n  if np.isinf(b):\n    b = 5\n  x_fill = np.linspace(a, b, 1000)\n  y_fill = norm.pdf(x_fill)\n  plt.fill_between(x_fill, y_fill, alpha=0.3)\n\n\n# Equivale a P(Z &lt; 0)\n# esperamos que seja 0.5\n\nnormal_pintada(-np.inf, 0)\n\nnorm.cdf(0)\n\n0.5\n\n\n\n\n\n\n\n\n\nOutro exemplo:\n\n# P(Z &lt; 1)\nnormal_pintada(-np.inf, 1)\n\nnorm.cdf(1)\n\n0.8413447460685429\n\n\n\n\n\n\n\n\n\n\n# P(Z &lt; 2)\n\nnormal_pintada(-np.inf, 2)\n\nnorm.cdf(2)\n\n0.9772498680518208\n\n\n\n\n\n\n\n\n\n\n# P(-2 &lt; Z &lt; 2)\nnormal_pintada(-2, 2)\n\nnorm.cdf(2) - norm.cdf(-2)\n\n0.9544997361036416\n\n\n\n\n\n\n\n\n\nAssim, com essa função, temos controle completo sobre a normal padrão. E como é fácil de transformar a normal em qualquer normal padrão, conseguimos calcular probabilidades para a distribuição normal!\nAgora olhe atentamente para a última operação. Ela diz que a probabilidade de uma variável aleatória com distribuição normal padrão assumir valores entre -2 e 2 é de aproximadamente 0.95. Isso é muito conveniente, já que esse é o valor de erro que geralmente utilizamos para testes de hipótese e intervalos de confiança (mais sobre na apostila seguinte).\nPodemos obter o valor exato da normal que resulta nessa probabilidade de 0.95. Esse é, vejam só, o quantil teórico da distribuição normal. É o mesmo conceito, mas diferente de calcular, do quantil empírico que vimos na apostila de medidas de posição e variabilidade. O quantil da distribuição normal pode ser calculado com a função norm.ppf(). Utilizamos como referência o valor 0.975, já que queremos deixar 0.025 para a direita e 0.025 para a esquerda, para que o meio da distribuição fique com 0.95.\n\nnorm.ppf(0.975)\n\n1.959963984540054\n\n\nesse valor de 1.96 é um número muito utilizado na estatística. Inclusive, esse é exatamente o valor \\(z_{1-\\alpha/2}\\) que vimos na apostila de amostragem! aqui, justamente, \\(\\alpha=0.05\\), fazendo com que \\(z_{1-\\alpha/2} = z_{0.975}\\). Então desvendamos o elemento \\(z\\): ele é o quantil da distribuição normal padrão, ou seja, o valor de \\(z\\) que faz com que a \\(\\mathbb P(Z &lt; z) = 1-\\frac \\alpha 2\\)\n\n6.6.1 Conclusões parciais\nAté agora, vimos que\n\nEstamos buscando propriedades dos nossos estimadores. Estamos usando a média como exemplo.\nSabemos que a média, pelo TCL, tem distribuição aproximadamente normal. Ou seja, ela varia mais ou menos de acordo com essa distribuição.\nSabemos calcular probabilidades na distribuição normal padrão\nSabemos relacionar a normal padrão com uma normal qualquer.\n\nO próximo passo é fazer uma inferência estatística. Uma forma de fazer isso é criar um intervalo, de acordo com essa distribuição normal, que provavelmente vai conter o valor de \\(\\mu\\). O intervalo, chamado de intervalo de confiança, terá a seguinte fórmula:\n\\[\n\\bar X \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma^2}{n}}\n\\]\nO interessante é que já vimos tudo isso: \\(\\bar X\\) é o estimador, \\(z_{1-\\alpha/2}\\) é o quantil da normal padrão, e \\(\\sigma^2/n\\) é a variância de \\(\\bar X\\), que também já vimos. A única coisa que falta é esse \\(\\sigma\\). Ele é um parâmetro populacional. Não temos acesso a ele. Mas, como vimos, podemos estimá-lo a partir da amostra, como já vimos na apostila de medidas de posição e variabilidade. Chamamos essa estimativa de \\(s^2\\). Substituindo \\(\\sigma^2\\) por \\(S^2\\) (o \\(S\\) maiúsculo para denotar o estimador), temos:\n\\[\n\\bar X \\pm z_{1-\\alpha/2} \\sqrt{\\frac{S^2}{n}} = \\bar X \\pm z_{1-\\alpha/2} \\frac{S}{\\sqrt n}\n\\]\nAgora sim, a partir de um conjunto de dados, obtemos estimativas \\(\\bar x\\) e \\(s^2\\). E podemos construir o intervalo de confiança da seguinte forma:\n\n# calcular intervalo de confiança para a coluna tempo da base camaras\n\ntempos = camaras.tempo\nmedia = tempos.mean()\nsd = tempos.std()\nz = norm.ppf(0.975)\nn = len(tempos)\n\nvl_intervalo = z * sd / np.sqrt(n)\n\nmedia - vl_intervalo, media + vl_intervalo\n\n(3.136057807684683, 3.221624914728558)\n\n\nComo temos muitas observações na amostra (10000), esse intervalo fica bem pequeno. Calculando, por exemplo, com base em uma amostra de 300 observações (que vimos na estimativa da apostila de amostragem):\n\ntempos = camaras.tempo.sample(300, random_state=1)\nmedia = tempos.mean()\nsd = tempos.std()\nz = norm.ppf(0.975)\nn = len(tempos)\n\nvl_intervalo = z * sd / np.sqrt(n)\n\nmedia - vl_intervalo, media, media + vl_intervalo\n\n(2.9557375829520214, 3.211791010723249, 3.467844438494476)\n\n\nNessa amostra, temos que o intervalo de confiança para a média dos tempos dos processos é de 2 anos e 11 meses até 3 anos e 6 meses. A estimativa pontual (média) é de aproximadamente 3 anos e 3 meses. A diferença entre a estimativa pontual e um dos limites do intervalo é próxima de 3 meses, o que está condizente com o valor de \\(d\\) que na hora de dimensionar a amostra.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidade e curva normal</span>"
    ]
  },
  {
    "objectID": "07-testes-hipoteses.html",
    "href": "07-testes-hipoteses.html",
    "title": "7  Testes de hipóteses",
    "section": "",
    "text": "7.1 Introdução\nNeste documento, vamos formalizar os conceitos de intervalo de confiança e testes de hipóteses. Vamos começar com o intervalo de confiança.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testes de hipóteses</span>"
    ]
  },
  {
    "objectID": "07-testes-hipoteses.html#intervalos-de-confiança-como-testes-de-hipóteses",
    "href": "07-testes-hipoteses.html#intervalos-de-confiança-como-testes-de-hipóteses",
    "title": "7  Testes de hipóteses",
    "section": "7.2 Intervalos de confiança como testes de hipóteses",
    "text": "7.2 Intervalos de confiança como testes de hipóteses\n\n7.2.1 Revisão\nO intervalo de confiança é utilizado para criar estimativas intervalares. Quando vemos uma pesquisa que diz que uma média é 50 +/- 5, estamos vendo um intervalo de confiança. O intervalo de confiança é uma faixa de valores que contém o valor real com uma certa probabilidade. Por exemplo, um intervalo de confiança de 95% significa que, se repetirmos o experimento muitas vezes, é esperado que 95% dos intervalos de confiança conterão o valor real.\nNão confunda o intervalo de confiança com a probabilidade do parâmetro estar dentro do intervalo. O parâmetro é um valor fixo e não é aleatório. O que é aleatório é o intervalo de confiança, pois é baseado em estimadores amostrais, que são variáveis aleatórias.\nComo vimos antes, quando nosso interesse é estimar a média de uma variável contínua, o intervalo de confiança é dado por:\n\\[\n\\bar X \\pm z_{1-\\alpha/2} \\frac{S}{\\sqrt n}\n\\]\nAqui, \\(\\bar X\\) é a média amostral, \\(S\\) é o desvio padrão amostral, \\(n\\) é o tamanho da amostra e \\(z_{1-\\alpha/2}\\) é o quantil da distribuição normal padrão. O valor de \\(z_{1-\\alpha/2}\\) é escolhido de acordo com o nível de confiança desejado. Por exemplo, para um nível de confiança de 95%, temos \\(\\alpha = 0.05\\) e \\(z_{1-\\alpha/2} = 1.96\\).\nApós observar os dados, trocamos as letras maiúsculas para minúsculas, denotando que estamos saindo do mundo das variáveis aleatórias (as funções que retornam números) para os números em si. Assim, o intervalo de confiança é escrito como:\n\\[\n\\bar x \\pm z_{1-\\alpha/2} \\frac{s}{\\sqrt n}\n\\]\n\n7.2.1.1 Aplicação no python\nNo python, podemos calcular o intervalo de confiança fazendo as seguintes operações:\n\ntempos = camaras.tempo\nmedia = tempos.mean()\nsd = tempos.std()\nz = stats.norm.ppf(0.975)\nn = len(tempos)\n\nvl_intervalo = z * sd / np.sqrt(n)\n\nmedia - vl_intervalo, media + vl_intervalo\n\n(3.136057807684683, 3.221624914728558)\n\n\nTambém é possível usar a função norm.interval do módulo scipy.stats para calcular o intervalo de confiança. Nessa função, o primeiro argumento é o nível de confiança, o segundo é a média amostral e o terceiro é o erro padrão (desvio padrão da amostra dividido pelo número de observações). O retorno é uma tupla com os limites inferior e superior do intervalo de confiança.\n\nstats.norm.interval(confidence=0.95, loc=media, scale=stats.sem(tempos, nan_policy='omit'))\n\n(3.136055668346555, 3.221627054066686)\n\n\nTambém é possível usar a função t.interval do módulo scipy.stats para calcular o intervalo de confiança. Nesse caso, o primeiro argumento é o nível de confiança, o segundo é o tamanho da amostra menos um, o terceiro é a média amostral e o quarto é o erro padrão.\n\nstats.t.interval(confidence=0.95, df=n-1, loc=media, scale=stats.sem(tempos, nan_policy='omit'))\n\n(3.136050488583062, 3.221632233830179)\n\n\nExiste uma pequena diferença entre fazer a conta na mão e fazer com a função t.interval. A função t.interval usa a distribuição t de Student no lugar da distribuição normal. A distribuição t é uma distribuição bem parecida com a distribuição normal, mas é mais achatada. Usamos a distribuição t por razões técnicas, que foram colocadas ao final da apostila por serem mais avançadas.\n\n\n\n7.2.2 Intervalo de confiança para proporção\nNa ciência de dados aplicada ao direito, é comum querermos estimar a proporção de um evento. Por exemplo, queremos saber a proporção de decisões favoráveis em um tipo de caso. Neste caso, também utilizamos o TCL para criar intervalos de confiança, afinal, a proporção é um tipo de média, mas de uma variável binária. Vimos anteriormente que a distribuição de probabilidades aplicável no caso de uma variável binária é a distribuição de Bernoulli, sendo que a soma de variáveis Bernoulli é uma variável com distribuição binomial.\nNota histórica: a distribuição binomial é uma distribuição discreta (assume valores 0, 1, 2, 3 etc, já que ela é a soma de valores que valem 0 ou 1), enquanto a distribuição normal é uma distribuição contínua. No entanto, o TCL nos permite aproximar a distribuição binomial pela distribuição normal, desde que o tamanho da amostra seja grande. Historicamente, a aproximação da distribuição binomial pela distribuição normal foi feita por Abraham de Moivre em 1743, que descobriu que a distribuição binomial se aproxima da distribuição normal (que nem tinha uma definição clara na época) quando o número de tentativas é grande. Essa demonstração foi feita muito antes do TCL, que é um resultado mais geral.\nPara fazer o intervalo de confiança para proporção, utilizamos o TCL, e, portanto, o z-score para criar o intervalo de confiança. O intervalo de confiança é dado por:\n\\[\n\\hat p \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\hat p (1-\\hat p)}{n}}\n\\]\nAqui, \\(\\hat p\\) é a proporção amostral, \\(n\\) é o tamanho da amostra e \\(z_{1-\\alpha/2}\\) é o quantil da distribuição normal padrão. O valor de \\(z_{1-\\alpha/2}\\) é escolhido de acordo com o nível de confiança desejado. Por exemplo, para um nível de confiança de 95%, temos \\(\\alpha = 0.05\\) e \\(z_{1-\\alpha/2} = 1.96\\).\nNote que utilizamos o valor \\(\\hat p (1-\\hat p)\\) no lugar de \\(S\\). Isso acontece porque a variância da distribuição de Bernoulli é dada por \\(p (1 - p)\\), fato que não vamos demostrar aqui. Então, substituímos o valor de \\(p\\), desconhecido, pelo valor de \\(\\hat p\\), que é a estimativa amostral da proporção. Aqui também fazemos uso do Teorema de Slutsky para afirmar que a distribuição amostral da proporção seja aproximadamente normal.\nMas existe um segundo jeito de fazer intervalo de confiança para proporções. Esse jeito é baseado no valor máximo que a variância da proporção pode assumir. Vamos ver o gráfico dessa função:\n\ndef variancia_bernoulli(p):\n  return p * (1 - p)\n\np_seq = np.linspace(0, 1, 200)\n\nplt.plot(p_seq, variancia_bernoulli(p_seq))\nplt.axvline(0.5, color = 'black', linestyle = '--')\n\n\n\n\n\n\n\n\nVeja que o valor máximo dessa função é 0.25. Isso ocorre porque a variância da proporção é máxima quando a proporção é 0.5. Com isso, podemos fazer um intervalo de confiança mais conservador, que é dado por:\n\\[\n\\hat p \\pm z_{1-\\alpha/2} \\sqrt{\\frac{0.25}{n}} = \\hat p \\pm \\frac{z_{1-\\alpha/2}}{2\\sqrt n}\n\\]\n\n7.2.2.1 Aplicação no python\nAgora, vamos ver os intervalos de confiança para proporção no python:\n\ndesfavoravel = camaras.decisao == 'Negaram'\n\n\np_hat = desfavoravel.mean()\n\nintervalo_otimista = stats.norm.ppf(0.975) * np.sqrt(variancia_bernoulli(p_hat) / len(camaras))\n\nintervalo_conservador = stats.norm.ppf(0.975) / 2 / np.sqrt(len(camaras))\n\nprint(p_hat - intervalo_otimista, p_hat + intervalo_otimista)\n\nprint(p_hat - intervalo_conservador, p_hat + intervalo_conservador)\n\n0.5175147983958988 0.5370852016041012\n0.5175001800772997 0.5370998199227003\n\n\nComo a amostra é grande, os intervalos otimista e conservador são praticamente iguais. Vamos ver com uma amostra pequena:\n\ndesfavoravel = camaras.sample(30, random_state=42).decisao == 'Negaram'\n\np_hat = desfavoravel.mean()\n\nintervalo_otimista = stats.norm.ppf(0.975) * np.sqrt(variancia_bernoulli(p_hat) / len(camaras))\n\nintervalo_conservador = stats.norm.ppf(0.975) / 2 / np.sqrt(len(camaras))\n\nprint(p_hat - intervalo_otimista, p_hat + intervalo_otimista)\n\nprint(p_hat - intervalo_conservador, p_hat + intervalo_conservador)\n\n0.6574273078376688 0.6759060254956645\n0.6568668467439663 0.6764664865893669\n\n\nAinda assim, os intervalos são bem próximos. Isso ocorre porque a proporção observada é próxima a 0.5, então o intervalo conservador fica parecido com o otimista. Ao estudar uma informação mais rara, como a probabilidade de maioria nas decisões colegiadas, temos:\n\nmaioria = camaras.sample(30, random_state=3).unanimidade == 'Maioria'\n\np_hat = maioria.mean()\n\nintervalo_otimista = stats.norm.ppf(0.975) * np.sqrt(variancia_bernoulli(p_hat) / len(camaras))\n\nintervalo_conservador = stats.norm.ppf(0.975) / 2 / np.sqrt(len(camaras))\n\nprint(p_hat - intervalo_otimista, p_hat + intervalo_otimista)\n\nprint(p_hat - intervalo_conservador, p_hat + intervalo_conservador)\n\n0.029815090309067926 0.03685157635759874\n0.02353351341063306 0.043133153256033605\n\n\nAgora, vamos às funções mais diretas do python, que nos dão o intervalo de confiança para proporção. Aqui, não tem muito como fugir do cálculo da variância da Bernoulli.\n\n\nic_otimista = stats.norm.interval(confidence=0.95, loc=p_hat, scale=np.sqrt(variancia_bernoulli(p_hat) / len(camaras)))\nic_conservador = stats.norm.interval(confidence=0.95, loc=p_hat, scale=1/2/np.sqrt(len(camaras)))\n\nprint(ic_otimista)\nprint(ic_conservador)\n\n(0.029815090309067926, 0.03685157635759874)\n(0.02353351341063306, 0.043133153256033605)\n\n\nAté existe outra biblioteca, statsmodels, que faz o cálculo de intervalo de confiança para proporção, mas ela pode ficar para um curso mais avançado.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testes de hipóteses</span>"
    ]
  },
  {
    "objectID": "07-testes-hipoteses.html#testes-de-hipóteses",
    "href": "07-testes-hipoteses.html#testes-de-hipóteses",
    "title": "7  Testes de hipóteses",
    "section": "7.3 Testes de hipóteses",
    "text": "7.3 Testes de hipóteses\nO teste de hipótese busca responder à seguinte pergunta: há evidências suficientes nos dados para fazer uma afirmação sobre a população? Por exemplo, se observarmos que as taxas de recursos negados em uma câmara é 67,8% e em outra câmara é 70,4%, podemos afirmar que a primeira câmara nega mais casos? Ou essa diferença é devida ao acaso?\nVamos ver esse exemplo no python:\n\nexemplos_camaras = ['03ª Câmara de Direito Criminal', '08ª Câmara de Direito Criminal']\n\ncamaras_filtrado = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('camara == @exemplos_camaras')\n  .assign(negaram = lambda x: x.decisao == 'Negaram')\n)\n\n\nproporcoes = (\n  camaras_filtrado\n  .groupby('camara')\n  .agg(n = ('decisao', len), p = ('negaram', 'mean'))\n  .reset_index(names='camara')\n)\n\nproporcoes\n\n\n\n\n\n\n\n\ncamara\nn\np\n\n\n\n\n0\n03ª Câmara de Direito Criminal\n528\n0.678030\n\n\n1\n08ª Câmara de Direito Criminal\n635\n0.703937\n\n\n\n\n\n\n\nUma forma de resolver esse problema é calculando o intervalo de confiança para cada uma das proporções e verificando se os intervalos se sobrepõem. Se eles não se sobrepõem, podemos afirmar que as proporções são diferentes. Se eles se sobrepõem, não podemos afirmar nada. Vamos calcular usando o intervalo otimista:\n\nproporcoes_com_ic = (\n  proporcoes\n  .assign(\n    int_otimista = lambda x: stats.norm.ppf(0.975) * np.sqrt((x.p*(1-x.p)) / x.n),\n    p_min = lambda x: x.p - x.int_otimista,\n    p_max = lambda x: x.p + x.int_otimista\n  )\n)\n\nproporcoes_com_ic\n\n\n\n\n\n\n\n\ncamara\nn\np\nint_otimista\np_min\np_max\n\n\n\n\n0\n03ª Câmara de Direito Criminal\n528\n0.678030\n0.039853\n0.638177\n0.717884\n\n\n1\n08ª Câmara de Direito Criminal\n635\n0.703937\n0.035508\n0.668430\n0.739445\n\n\n\n\n\n\n\n\nsns.pointplot(data = camaras_filtrado, x = 'camara', y = 'negaram')\nplt.axhline(0.69, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nObs: O seaborn faz o cálculo do intervalo de confiança automaticamente, logo não precisamos colocar os valores calculados anteriormente. Mas é possível ver que o resultado é o mesmo.\nNote que os intervalos se sobrepõem, então, estatisticamente, não podemos afirmar que as proporções são diferentes.\nAgora vamos ver um exemplo com outras câmaras.\n\nexemplos_camaras = ['04ª Câmara de Direito Criminal', '08ª Câmara de Direito Criminal']\n\ncamaras_filtrado = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('camara == @exemplos_camaras')\n  .assign(negaram = lambda x: x.decisao == 'Negaram')\n)\n\n\nproporcoes = (\n  camaras_filtrado\n  .groupby('camara')\n  .agg(n = ('decisao', len), p = ('negaram', 'mean'))\n  .reset_index(names='camara')\n)\n\nproporcoes\n\n\n\n\n\n\n\n\ncamara\nn\np\n\n\n\n\n0\n04ª Câmara de Direito Criminal\n534\n0.872659\n\n\n1\n08ª Câmara de Direito Criminal\n635\n0.703937\n\n\n\n\n\n\n\nAgora, as proporções estão bem distantes. Vamos calcular o intervalo de confiança para essas proporções:\n\nproporcoes_com_ic = (\n  proporcoes\n  .assign(\n    int_otimista = lambda x: stats.norm.ppf(0.975) * np.sqrt((x.p*(1-x.p)) / x.n),\n    p_min = lambda x: x.p - x.int_otimista,\n    p_max = lambda x: x.p + x.int_otimista\n  )\n)\n\nproporcoes_com_ic\n\n\n\n\n\n\n\n\ncamara\nn\np\nint_otimista\np_min\np_max\n\n\n\n\n0\n04ª Câmara de Direito Criminal\n534\n0.872659\n0.028274\n0.844385\n0.900933\n\n\n1\n08ª Câmara de Direito Criminal\n635\n0.703937\n0.035508\n0.668430\n0.739445\n\n\n\n\n\n\n\nFinalmente, vamos ver isso graficamente:\n\nsns.pointplot(data = camaras_filtrado, x = 'camara', y = 'negaram')\nplt.axhline(0.80, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nNesse caso, então, concluímos que as proporções são estatisticamente diferentes.\nO que acabamos de fazer intuitivamente é o que chamamos de teste de hipótese de diferença de médias. No caso, aplicado para proporções. O teste de hipótese é uma forma de formalizar e generalizar a pergunta que fizemos anteriormente. Vamos ver como fazer isso.\n\n7.3.1 Definição\nUm fato curioso sobre o teste de hipóteses é que, geralmente, quando introduzimos o tema, usamos um exemplo jurídico. Isso ocorre porque o teste de hipóteses é uma forma de gerenciar os erros nas tomadas de decisão em ambientes de incerteza, e a tomada de decisão é o que os juízes fazem. Vamos ver esse exemplo, que é o exemplo do julgamento de um réu.\nQuando fazemos um teste de hipóteses, estamos tentando decidir entre duas hipóteses: a hipótese nula (\\(H_0\\)) e a hipótese alternativa (\\(H_1\\)). A hipótese nula é a hipótese que queremos rejeitar, enquanto a hipótese alternativa é a hipótese que queremos aceitar. No exemplo do julgamento de um réu, a hipótese nula é que o réu é inocente, enquanto a hipótese alternativa é que o réu é culpado.\nDois tipos de erros. Quando fazemos um teste de hipóteses, podemos cometer dois tipos de erros: o erro do tipo I e o erro do tipo II. O erro do tipo I ocorre quando rejeitamos a hipótese nula quando ela é verdadeira. No exemplo do julgamento de um réu, seria condenar um réu inocente. O erro do tipo II ocorre quando aceitamos a hipótese nula quando ela é falsa. No exemplo do julgamento de um réu, seria absolver um réu culpado.\n\nQuando tomamos uma decisão, existem quatro possibilidades:\n\nRejeitar a hipótese nula quando ela é verdadeira: isso é o erro do tipo I. A probabilidade de cometer esse erro é chamada de nível de significância e é denotada por \\(\\alpha\\).\nRejeitar a hipótese nula quando ela é falsa: isso é o que queremos fazer. A probabilidade de fazer isso é chamada de poder do teste e é denotada por \\(1 - \\beta\\).\nAceitar a hipótese nula quando ela é verdadeira: isso é o que queremos evitar. A probabilidade de fazer isso é chamada de nível de confiança e é denotada por \\(1 - \\alpha\\).\nAceitar a hipótese nula quando ela é falsa: isso é o erro do tipo II. A probabilidade de cometer esse erro é denotada por \\(\\beta\\).\n\nDefinição. O teste de hipóteses é um procedimento estatístico que nos permite decidir entre duas hipóteses, a hipótese nula (\\(H_0\\)) e a hipótese alternativa (\\(H_1\\)), com base em uma amostra de dados. O teste de hipóteses é feito em três etapas:\n\nFormulação das hipóteses: formulamos a hipótese nula (\\(H_0\\)) e a hipótese alternativa (\\(H_1\\)). A hipótese nula é a hipótese que queremos rejeitar, enquanto a hipótese alternativa é a hipótese que queremos aceitar.\nCálculo da estatística de teste: calculamos a estatística de teste, que é uma medida da diferença entre a amostra e a hipótese nula. A estatística de teste segue uma distribuição de probabilidades conhecida.\nTomada de decisão: comparamos a estatística de teste com um valor crítico e decidimos se rejeitamos ou não a hipótese nula.\n\nO procedimento de teste mais conhecido é o teste de Neyman-Pearson, que busca minimizar a probabilidade de erro do tipo II, sujeito a um limite fixado para o erro do tipo I. O teste de Neyman-Pearson é um teste de hipóteses com regra de decisão baseada em um valor crítico.\nO valor crítico é o valor que divide a região de rejeição da região de aceitação. A região de rejeição é a região onde rejeitamos a hipótese nula, enquanto a região de aceitação é a região onde aceitamos a hipótese nula. O valor crítico é escolhido de acordo com o nível de significância desejado. Por exemplo, para um nível de significância de 5%, o valor crítico da distribuição normal é 1,96, assim como vimos no z-score para intervalos de confiança.\n\n\n7.3.2 Na prática\nNa prática, entra em jogo o famoso valor-p (ou p-valor, dependendo do livro). O valor-p é a probabilidade de observar uma estatística de teste tão ou mais extrema quanto a observada, assumindo que a hipótese nula é verdadeira.\nIntuitivamente, o valor-p é a probabilidade de encontrar um resultado que leva à rejeição da hipótese nula por mera coincidência. Em outras palavras: quando a hipótese nula é verdadeira (o réu é inocente), existem fatos que poderiam confundir o juiz ou juíza e levar ao erro, como circunstâncias complexas, preconceito, evidências falsas, entre outros. É isso que estamos chamando de “por coincidência”.\nSe o valor-p for pequeno, seria muito arriscado afirmar que um efeito identificado (por exemplo, evidências para prender o réu), ocorreu ao acaso, por coincidência. Então rejeitamos a hipótese nula. No entanto, se o valor-p for grande, é bem possível que o resultado encontrado tenha acontecido por acaso, então dizemos que não há evidências para rejeitar a hipótese nula.\n\n\n7.3.3 Teste para duas proporções\nVamos voltar ao exemplo da diferença entre a proporção de negados. Faremos um teste de hipóteses para cada situação (o caso em que os intervalos de confiança se sobrepõem e o caso em que os intervalos de confiança não se sobrepõem). Para cada um deles, faremos um exemplo com o cálculo do valor-p e outro exemplo com simulação, para entender o acontecimento “por acaso”.\n\nexemplos_camaras = ['03ª Câmara de Direito Criminal', '08ª Câmara de Direito Criminal']\n\ncamaras_filtrado = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('camara == @exemplos_camaras')\n  .assign(negaram = lambda x: x.decisao == 'Negaram')\n)\n\n\nproporcoes = (\n  camaras_filtrado\n  .groupby('camara')\n  .agg(n = ('decisao', len), n_negaram = ('negaram', 'sum'), p = ('negaram', 'mean'))\n  .reset_index(names='camara')\n)\n\nproporcoes\n\n\n\n\n\n\n\n\ncamara\nn\nn_negaram\np\n\n\n\n\n0\n03ª Câmara de Direito Criminal\n528\n358\n0.678030\n\n\n1\n08ª Câmara de Direito Criminal\n635\n447\n0.703937\n\n\n\n\n\n\n\nNesse caso, nossas hipóteses são:\n\\(H_0\\): \\(p_3 = p_8\\), ou seja, as proporções são iguais\n\\(H_1\\): \\(p_3 \\neq p_8\\), ou seja, as proporções são diferentes\nConectando com os conceitos anteriores: \\(p_3\\) e \\(p_8\\) são parâmetros, ou seja, a probabilidade de um caso ser negado em cada câmara.\n\n7.3.3.1 Cálculo do valor-p\nPara calcular o valor-p, utilizamos a função chi2_contingency do módulo scipy.stats. Essa é, na verdade, uma função para fazer o teste qui-quadrado de independência, mas podemos utilizá-la para fazer o teste de diferença de proporções. O retorno é uma tupla com a estatística de teste, o valor-p, o grau de liberdade e a tabela de contingência esperada.\n\ncontagens = np.array(proporcoes[['n_negaram', 'n']])\n\nprop_test = stats.chi2_contingency(contagens)\n\nprop_test.pvalue\n\n0.7183537580673838\n\n\nEsse número significa que, se a hipótese nula fosse verdadeira, a probabilidade de observar uma diferença tão grande ou maior do que a observada seria de 72%. Como esse valor é bem alto, realmente parece que poderia ter acontecido por acaso. Por esse motivo, não rejeitamos a hipótese nula e podemos afirmar que as proporções são iguais. Note que essa é a mesma conclusão que chegamos ao calcular o intervalo de confiança.\nNo caso do teste comparando as câmaras 8 e 12, temos:\n\nexemplos_camaras = ['12ª Câmara de Direito Criminal', '08ª Câmara de Direito Criminal']\n\ncamaras_filtrado = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('camara == @exemplos_camaras')\n  .assign(negaram = lambda x: x.decisao == 'Negaram')\n)\n\n\nproporcoes = (\n  camaras_filtrado\n  .groupby('camara')\n  .agg(n = ('decisao', len), n_negaram = ('negaram', 'sum'), p = ('negaram', 'mean'))\n  .reset_index(names='camara')\n)\n\nproporcoes\n\n\n\n\n\n\n\n\ncamara\nn\nn_negaram\np\n\n\n\n\n0\n08ª Câmara de Direito Criminal\n635\n447\n0.703937\n\n\n1\n12ª Câmara de Direito Criminal\n521\n103\n0.197697\n\n\n\n\n\n\n\nE agora o teste de hipótese:\n\ncontagens = np.array(proporcoes[['n_negaram', 'n']])\n\nprop_test = stats.chi2_contingency(contagens)\n\nprop_test.pvalue\n\n8.238913311602198e-26\n\n\nNessa situação, o valor-p é muito baixo, então rejeitamos a hipótese nula e podemos afirmar que as proporções são diferentes. Note que, novamente, essa é a mesma conclusão que chegamos ao calcular o intervalo de confiança.\nExistem situações, no entanto, que podemos ficar em dúvida. Vejamos um exemplo, agora com as câmaras 8 e 9\n\nexemplos_camaras = ['06ª Câmara de Direito Criminal', '08ª Câmara de Direito Criminal']\n\ncamaras_filtrado = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('camara == @exemplos_camaras')\n  .assign(negaram = lambda x: x.decisao == 'Negaram')\n)\n\n\nproporcoes = (\n  camaras_filtrado\n  .groupby('camara')\n  .agg(n = ('decisao', len), n_negaram = ('negaram', 'sum'), p = ('negaram', 'mean'))\n  .reset_index(names='camara')\n)\n\nproporcoes\n\n\n\n\n\n\n\n\ncamara\nn\nn_negaram\np\n\n\n\n\n0\n06ª Câmara de Direito Criminal\n664\n550\n0.828313\n\n\n1\n08ª Câmara de Direito Criminal\n635\n447\n0.703937\n\n\n\n\n\n\n\nVejamos o valor do teste:\n\ncontagens = np.array(proporcoes[['n_negaram', 'n']])\n\nprop_test = stats.chi2_contingency(contagens)\n\nprop_test.pvalue\n\n0.05950837666339642\n\n\nAgora, o valor-p, ou seja, a probabilidade dessa diferença ocorrer ao acaso é de 6%. Esse valor é baixo ou alto?\nNessas situações, precisamos estabelecer um valor de corte para o valor-p. Esse valor de corte é o nível de significância, que discutimos anteriormente na definição do teste de hipóteses. Esse valor é denotado por \\(\\alpha\\). Por razões históricas, existe um valor de corte muito comum, que é 5%. Se o valor-p for menor que 5%, rejeitamos a hipótese nula. Se o valor-p for maior que 5%, não rejeitamos a hipótese nula. Essa é a “regra de bolso” usada em muitas áreas da ciência.\nNesse caso, então, a conclusão é que as proporções não são diferentes, porque o valor-p é maior que 5%.\n\n\n\n7.3.4 Teste para várias proporções\nE se nosso objetivo for comparar mais de duas proporções? Nesse caso, uma ideia inicial seria comparar cada par de proporções. Mas existe uma alternativa melhor, que é fazer um teste de hipóteses para todas as proporções ao mesmo tempo. Isso é feito com o teste de qui-quadrado.\nO teste de qui-quadrado é um teste de hipóteses que compara a distribuição observada com a distribuição esperada. No caso de proporções, a distribuição observada é a proporção de casos em cada categoria, enquanto a distribuição esperada é a proporção de casos esperada se a hipótese nula fosse verdadeira.\nPor exemplo, se quisermos comparar as proporções de negados, parcial procedente e procedente nas câmaras 3, 8 e 12, nossas hipóteses são:\n\\(H_0\\): a distribuição de decisões é a mesma nas três câmaras\n\\(H_1\\): a distribuição de decisões é diferente em alguma das câmaras\nNo python, podemos fazer o teste de qui-quadrado com a mesma função de antes: chi2_contingency do módulo scipy.stats. O retorno é uma tupla com a estatística de teste, o valor-p, o grau de liberdade e a tabela de contingência esperada.\n\nexemplos_camaras = ['03ª Câmara de Direito Criminal', '08ª Câmara de Direito Criminal', '12ª Câmara de Direito Criminal']\n\ncamaras_filtrado = (\n  camaras\n  .query('polo_mp == \"Passivo\"')\n  .query('camara == @exemplos_camaras')\n  .query('decisao == [\"Negaram\",\"Parcialmente\",\"Provido\"]')\n)\n\ntabela_contingencia = pd.crosstab(index=camaras_filtrado['camara'], columns=camaras_filtrado['decisao'])\n\ntabela_contingencia\n\n\n\n\n\n\n\ndecisao\nNegaram\nParcialmente\nProvido\n\n\ncamara\n\n\n\n\n\n\n\n03ª Câmara de Direito Criminal\n358\n140\n10\n\n\n08ª Câmara de Direito Criminal\n447\n156\n7\n\n\n12ª Câmara de Direito Criminal\n103\n321\n75\n\n\n\n\n\n\n\nVeja que as proporções são parecidas entre as câmaras 03 e 08, mas bem diferente na câmara 12:\n\ntabela_contingencia.div(tabela_contingencia.sum(axis=1), axis=0)\n\n\n\n\n\n\n\ndecisao\nNegaram\nParcialmente\nProvido\n\n\ncamara\n\n\n\n\n\n\n\n03ª Câmara de Direito Criminal\n0.704724\n0.275591\n0.019685\n\n\n08ª Câmara de Direito Criminal\n0.732787\n0.255738\n0.011475\n\n\n12ª Câmara de Direito Criminal\n0.206413\n0.643287\n0.150301\n\n\n\n\n\n\n\nAgora, procedemos com o teste:\n\nchi2 = stats.chi2_contingency(tabela_contingencia)\n\nchi2\n\nChi2ContingencyResult(statistic=403.3146570240564, pvalue=5.346841438716827e-86, dof=4, expected_freq=array([[285.25912183, 193.83797155,  28.90290662],\n       [342.53555968, 232.75819419,  34.70624613],\n       [280.20531849, 190.40383426,  28.39084725]]))\n\n\nNesse objeto, temos os seguintes componentes:\n\nstatistic: a estatística de teste, que é a base para o cálculo do valor-p\npvalue: o valor-p, que é a probabilidade de observar uma estatística de teste tão ou mais extrema quanto a observada, assumindo que a hipótese nula é verdadeira\ndof: o grau de liberdade, que é o número de categorias menos um\nexpected: a tabela de contingência esperada, que é a distribuição esperada se a hipótese nula fosse verdadeira\n\nNesse caso, o valor-p é muito baixo, então rejeitamos a hipótese nula e podemos afirmar que a distribuição de decisões é diferente em alguma das câmaras.\n\n\n7.3.5 Teste de médias\nAté agora, vimos o teste de proporções e o teste de uma tabela de contingência completa, que é um teste de proporções generalizado. Agora, vamos ver o teste de médias. A ideia é muito parecida com o que vimos antes, mas nesse caso utilizamos outra distribuição de probabilidades, a distribuição t de Student e, portanto, outra função do python, ttest_ind do módulo scipy.stats. Nos apêndices extras dessa apostila, temos uma explicação mais detalhada sobre a distribuição t de Student.\nPara mostrar o teste de médias, vamos fazer um exemplo com a diferença de médias de idade entre relatores que vieram ou não do quinto constitucional. Nesse caso, nossas hipóteses são:\n\\(H_0\\): a média de idade dos relatores que vieram do quinto constitucional é igual à média de idade dos relatores que não vieram do quinto constitucional\n\\(H_1\\): a média de idade dos relatores que vieram do quinto constitucional é diferente da média de idade dos relatores que não vieram do quinto constitucional\n\nrelatores = camaras.drop_duplicates('relator')\n\n(\n  relatores\n  .groupby('rel_quinto')\n  .agg(n = ('relator', len), media = ('rel_idade', 'mean'), sd = ('rel_idade', 'std'))\n  .reset_index(names='rel_quinto')\n)\n\n\n\n\n\n\n\n\nrel_quinto\nn\nmedia\nsd\n\n\n\n\n0\nnão\n88\n67.173987\n5.911481\n\n\n1\nsim\n21\n65.910368\n6.665895\n\n\n\n\n\n\n\nOlhando os dados, parece que a média de idades é bem parecida. Vamos fazer o gráfico com os intervalos de confiança:\n\nsns.pointplot(data = relatores, x = 'rel_quinto', y = 'rel_idade')\nplt.axhline(66.5, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nDe fato, os intervalos de confiança se sobrepõem. Vamos fazer o teste de hipóteses, usando a função ttest_ind:\n\nttest = stats.ttest_ind(\n  relatores.rel_idade[relatores.rel_quinto == 'sim'],\n  relatores.rel_idade[relatores.rel_quinto == 'não'],\n  nan_policy='omit'\n)\n\nttest\n\nTtestResult(statistic=-0.8574885189308531, pvalue=0.3931095647661792, df=106.0)\n\n\nDe fato, o valor-p ficou em 39%, então não rejeitamos a hipótese nula, ou seja, não há evidências para afirmar que as idades dos relatores que vieram do quinto constitucional são diferentes das idades dos relatores que não vieram do quinto constitucional.\nAgora, vamos ver um exemplo em que as médias são diferentes. Vamos comparar a média de idade dos relatores que vieram da USP com a média de idade dos relatores que vieram de outras universidades. Nesse caso, nossas hipóteses são:\n\\(H_0\\): a média de idade dos relatores que vieram da USP é igual à média de idade dos relatores que vieram de outras universidades\n\\(H_1\\): a média de idade dos relatores que vieram da USP é diferente da média de idade dos relatores que vieram de outras universidades\nVamos fazer o gráfico com os intervalos de confiança:\n\n# o nome da faculdade de direito pode aparecer de diversas formas\nusp = ['Faculdade de Direito da Universidade de São Paulo', 'Faculdade de Direto da Universidade de São Paulo', 'Universidade de São Paulo']\n\nrelatores_usp = (\n  relatores\n  .assign(usp = lambda x: np.where(x.rel_faculdade_direito.isin(usp), 'USP', 'Outras'))\n)\n\nsns.pointplot(data = relatores_usp, x = 'usp', y = 'rel_idade')\nplt.axhline(66.55, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nÉ por muito pouco, mas parece que as médias são diferentes. Vamos fazer o teste de hipóteses:\n\nttest = stats.ttest_ind(\n  relatores_usp.rel_idade[relatores_usp.usp == 'USP'],\n  relatores_usp.rel_idade[~(relatores_usp.usp == 'USP')],\n  nan_policy='omit'\n)\n\nttest\n\nTtestResult(statistic=-2.663336219551609, pvalue=0.00894402839660715, df=106.0)\n\n\nDe fato, o valor-p ficou em 0.009, então rejeitamos a hipótese nula e podemos afirmar que a média de idade dos relatores que vieram da USP é diferente da média de idade dos relatores que vieram de outras universidades.\n\n\n\n7.3.6 (extra) Simulação\nO que significa, na prática, a tal da “ocorrência ao acaso”? Vamos mostrar isso através de uma simulação. Vamos simular 1000 vezes a proporção de negados em duas câmaras. Para isso, no entanto, precisamos supor que a hipótese nula é verdadeira, ou seja, que as proporções são iguais. Vamos supor que a proporção das duas câmaras é 60%.\n\ndef amostra_diferenca(prop1, prop2, n=1000):\n  # faz uma amostra com os\n  camara1 = np.random.choice([0,1], n, p=[1-prop1,prop1])\n  camara2 = np.random.choice([0,1], n, p=[1-prop2,prop2])\n  # calcula a proporção\n  prop_camara1 = camara1.mean()\n  prop_camara2 = camara2.mean()\n  # diferença entre câmaras\n  diferenca = prop_camara1 - prop_camara2\n  contagens = np.array([[camara1.sum(), camara2.sum()], [n-camara1.sum(), n-camara2.sum()]])\n  valor_p = stats.chi2_contingency(contagens).pvalue\n  df = pd.DataFrame({\n    'prop1': prop_camara1,\n    'prop2': prop_camara2,\n    'diferenca': diferenca,\n    'valor_p': valor_p\n  }, index=[0])\n  return df\n\n\namostra_diferenca(0.6, 0.6)\n\n\n\n\n\n\n\n\nprop1\nprop2\ndiferenca\nvalor_p\n\n\n\n\n0\n0.602\n0.628\n-0.026\n0.250625\n\n\n\n\n\n\n\nvamos fazer essa conta mil vezes:\n\namostras = []\nfor _ in range(1000):\n  amostras.append(amostra_diferenca(0.6, 0.6))\n\ndf = pd.concat(amostras)\n\ndf\n\n\n\n\n\n\n\n\nprop1\nprop2\ndiferenca\nvalor_p\n\n\n\n\n0\n0.589\n0.609\n-0.020\n0.386014\n\n\n0\n0.579\n0.610\n-0.031\n0.171856\n\n\n0\n0.582\n0.603\n-0.021\n0.362750\n\n\n0\n0.590\n0.592\n-0.002\n0.963724\n\n\n0\n0.605\n0.601\n0.004\n0.890948\n\n\n...\n...\n...\n...\n...\n\n\n0\n0.613\n0.616\n-0.003\n0.926790\n\n\n0\n0.595\n0.584\n0.011\n0.649429\n\n\n0\n0.587\n0.600\n-0.013\n0.584865\n\n\n0\n0.583\n0.595\n-0.012\n0.617133\n\n\n0\n0.599\n0.606\n-0.007\n0.783969\n\n\n\n\n1000 rows × 4 columns\n\n\n\nAgora, fazemos um histograma das diferenças:\n\nsns.histplot(data=df, x='diferenca', kde=True)\nplt.axvline(0, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nVeja como, nesse caso, a diferença fica centrada no zero. Nessa simulação, se extrairmos os quantis de 2,5% e 97,5%, temos o intervalo de confiança simulado para essa diferença. Veja que ela contém o zero:\n\nnp.quantile(df.diferenca, [0.025, 0.975])\n\narray([-0.041025,  0.041   ])\n\n\nE veja que interessante, a proporção de vezes em que, ao fazer o teste de hipóteses, rejeitaríamos a hipótese nula:\n\n(df.valor_p &lt; 0.05).mean()\n\n0.053\n\n\nOu seja, em aproximadamente 5% das amostras, nós rejeitaríamos a hipótese nula, mesmo ela sendo verdadeira. Isso é o erro do tipo I, que é o nível de significância que escolhemos.\nAgora, vamos para um exemplo onde as proporções são diferentes, 60% e 70%:\n\namostras = []\nfor _ in range(1000):\n  amostras.append(amostra_diferenca(0.7, 0.6))\n\ndf = pd.concat(amostras)\n\ndf\n\n\n\n\n\n\n\n\nprop1\nprop2\ndiferenca\nvalor_p\n\n\n\n\n0\n0.702\n0.605\n0.097\n6.449431e-06\n\n\n0\n0.698\n0.586\n0.112\n2.251830e-07\n\n\n0\n0.671\n0.611\n0.060\n5.956293e-03\n\n\n0\n0.732\n0.582\n0.150\n2.243348e-12\n\n\n0\n0.695\n0.614\n0.081\n1.686924e-04\n\n\n...\n...\n...\n...\n...\n\n\n0\n0.689\n0.597\n0.092\n2.166091e-05\n\n\n0\n0.700\n0.606\n0.094\n1.250206e-05\n\n\n0\n0.696\n0.596\n0.100\n3.671498e-06\n\n\n0\n0.691\n0.592\n0.099\n4.889018e-06\n\n\n0\n0.681\n0.614\n0.067\n2.007758e-03\n\n\n\n\n1000 rows × 4 columns\n\n\n\nO histograma fica assim:\n\nsns.histplot(data=df, x='diferenca', kde=True)\nplt.axvline(0, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nE o intervalo de confiança simulado fica assim:\n\nnp.quantile(df.diferenca, [0.025, 0.975])\n\narray([0.059975, 0.141   ])\n\n\nVeja que dessa vez o intervalo não contém o zero. Ou seja, a diferença é estatisticamente diferente de zero.\nAgora, veja a proporção de vezes que o valor-p é menor de 5%:\n\n(df.valor_p &lt; 0.05).mean()\n\n1.0\n\n\nTodas as vezes! Ou seja, em 100% das amostras, rejeitaríamos a hipótese nula, o que é esperado, já que ela é falsa.\nAgora vamos para um exemplo onde as proporções são diferentes, mas não muito, 60% e 62%:\n\namostras = []\nfor _ in range(1000):\n  amostras.append(amostra_diferenca(0.62, 0.60))\n\ndf = pd.concat(amostras)\n\ndf\n\n\n\n\n\n\n\n\nprop1\nprop2\ndiferenca\nvalor_p\n\n\n\n\n0\n0.623\n0.615\n0.008\n0.747217\n\n\n0\n0.611\n0.584\n0.027\n0.235815\n\n\n0\n0.625\n0.614\n0.011\n0.645114\n\n\n0\n0.599\n0.594\n0.005\n0.855337\n\n\n0\n0.650\n0.625\n0.025\n0.264270\n\n\n...\n...\n...\n...\n...\n\n\n0\n0.644\n0.609\n0.035\n0.116030\n\n\n0\n0.625\n0.604\n0.021\n0.358177\n\n\n0\n0.618\n0.599\n0.019\n0.409579\n\n\n0\n0.625\n0.601\n0.024\n0.291009\n\n\n0\n0.592\n0.591\n0.001\n1.000000\n\n\n\n\n1000 rows × 4 columns\n\n\n\nO histograma fica assim:\n\nsns.histplot(data=df, x='diferenca', kde=True)\nplt.axvline(0, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nE o intervalo de confiança simulado fica assim:\n\nnp.quantile(df.diferenca, [0.025, 0.975])\n\narray([-0.022,  0.065])\n\n\nVeja que dessa vez o intervalo não contém o zero. Ou seja, a diferença é estatisticamente diferente de zero.\nAgora, veja a proporção de vezes que o valor-p é menor de 5%:\n\n(df.valor_p &lt; 0.05).mean()\n\n0.15\n\n\nNesse caso, não há evidências suficientes para rejeitar a hipótese nula. Isso é o que ocorre quando temos uma diferença na população que é muito pequena. Mas tem um jeito de detectar essa diferença, que é aumentando o tamanho da amostra. Vamos ver isso. Considere que, ao invés de 1000 observações, temos 5000:\n\namostras = []\nfor _ in range(1000):\n  amostras.append(amostra_diferenca(0.62, 0.60, 5000))\n\ndf = pd.concat(amostras)\n\ndf\n\n\n\n\n\n\n\n\nprop1\nprop2\ndiferenca\nvalor_p\n\n\n\n\n0\n0.6302\n0.5914\n0.0388\n0.000075\n\n\n0\n0.6274\n0.6010\n0.0264\n0.007121\n\n\n0\n0.6206\n0.5982\n0.0224\n0.022898\n\n\n0\n0.6198\n0.6014\n0.0184\n0.062009\n\n\n0\n0.6196\n0.5982\n0.0214\n0.029845\n\n\n...\n...\n...\n...\n...\n\n\n0\n0.6194\n0.6086\n0.0108\n0.276297\n\n\n0\n0.6134\n0.6060\n0.0074\n0.460526\n\n\n0\n0.6222\n0.5976\n0.0246\n0.012379\n\n\n0\n0.6134\n0.6092\n0.0042\n0.681590\n\n\n0\n0.6146\n0.6072\n0.0074\n0.460276\n\n\n\n\n1000 rows × 4 columns\n\n\n\nO histograma fica assim:\n\nsns.histplot(data=df, x='diferenca', kde=True)\nplt.axvline(0, color='gray', linestyle='--')\n\n\n\n\n\n\n\n\nE o intervalo de confiança simulado fica assim:\n\nnp.quantile(df.diferenca, [0.025, 0.975])\n\narray([0.0018, 0.0388])\n\n\nVeja que dessa vez o intervalo não contém o zero. Ou seja, a diferença é estatisticamente diferente de zero.\nAgora, veja a proporção de vezes que o valor-p é menor de 5%:\n\n(df.valor_p &lt; 0.05).mean()\n\n0.536\n\n\nVeja que em mais da metade das amostras que fizemos, o valor-p foi menor de 5%.\nEssas simulações mostram como o valor-p é uma medida de quão extremo é o resultado observado. Se a diferença entre as proporções for muito pequena, o valor-p será alto, indicando que a diferença pode ter ocorrido por acaso. Se a diferença for grande, o valor-p será baixo, indicando que a diferença é estatisticamente significante.\n\n\n\n7.3.7 (extra) por que usamos t-student?\nNa apostila anterior e no início dessa apostila, vimos intevalos de confiança baseados no valor de \\(z_{1-\\alpha/2}\\).\nNos livros didáticos, o que temos é que esse valor \\(z\\) é usado quando a variância populacional (\\(\\sigma^2\\)) é conhecida, enquanto, quando a variância populacional é desconhecida, usamos o valor \\(t\\), da distribuição t-Student. É daí que vem o termo “teste t”, que são os testes que usam a distribuição t-Student no lugar da distribuição normal.\nA origem disso está na suposição sobre a distribuição de probabilidades da variável de interesse. Quando supomos que a variável de interesse é normal, existe um resultado mostrando que a média amostral (\\(\\bar X\\)) dividida pelo desvio padrão amostral (\\(S\\)) e multiplicada por uma constante segue uma distribuição t de Student. Essa constante é chamada de graus de liberdade e no caso que estamos estudando é dada por \\(n-1\\), onde \\(n\\) é o tamanho da amostra.\nVamos desenhar a t-student para alguns valores de \\(n\\), comparando-a com a distribuição normal.\n\nvalores_n = [5, 10, 20, 30]\n\nx = np.linspace(-5, 5, 1000)\n\ndf = pd.DataFrame({'x': x, 'densidade': stats.norm.pdf(x)})\n\nfig, axes = plt.subplots(2, 2, figsize = (10, 10))\n\n# mostrando a distribuição t-student para os diferentes valores de n\n\ndf = df.assign(t_pdf = stats.t.pdf(x, df = valores_n[0] - 1))\n# mostrando o gráfico da normal\nsns.lineplot(ax = axes[0,0], data = df, x = 'x', y = \"densidade\", label=\"Normal\")\n# mostrando o gráfico da t-student\nsns.lineplot(ax = axes[0,0], data = df, x = 'x', y = 't_pdf', label=\"t-student\")\n\ndf = df.assign(t_pdf = stats.t.pdf(x, df = valores_n[1] - 1))\nsns.lineplot(ax = axes[0,1], data = df, x = 'x', y = \"densidade\", label=\"Normal\")\nsns.lineplot(ax = axes[0,1], data = df, x = 'x', y = 't_pdf', label=\"t-student\")\n\ndf = df.assign(t_pdf = stats.t.pdf(x, df = valores_n[2] - 1))\nsns.lineplot(ax = axes[1,0], data = df, x = 'x', y = \"densidade\", label=\"Normal\")\nsns.lineplot(ax = axes[1,0], data = df, x = 'x', y = 't_pdf', label=\"t-student\")\n\ndf = df.assign(t_pdf = stats.t.pdf(x, df = valores_n[3] - 1))\nsns.lineplot(ax = axes[1,1], data = df, x = 'x', y = \"densidade\", label=\"Normal\")\nsns.lineplot(ax = axes[1,1], data = df, x = 'x', y = 't_pdf', label=\"t-student\")\n\n\n\n\n\n\n\n\n\nVeja que para valores pequenos de \\(n\\), a t-student é mais larga que a distribuição normal. Isso ocorre porque a variância amostral é uma estimativa da variância populacional e, portanto, é mais incerta. Conforme o tamanho da amostra aumenta, a t-student se aproxima da distribuição normal.\nPortanto, o t-score é usado quando temos a suposição de normalidade e é mais útil quando temos poucos dados, já que, quando temos muitos dados, os números são quase equivalentes ao z-score.\nJá o z-score pode ser usado em duas situações:\n\nquando assumimos que a variável de interesse tem distribuição normal e variância conhecida (portanto, não temos a razão entre a média e o desvio padrão amostrais), ou\nquando a variavel de interesse não é normal e usamos o TCL para aproximar a distribuição amostral da média. Neste segundo caso, existe um teorema adicional, chamado teorema de Slutsky, que, a partir do resultado de que o desvio padrão amostral converge em probabilidade para o desvio padrão da população, mostra que o z-score é assintoticamente normal.\n\nPor isso que dizemos que o z-score é usado quando a variância populacional é conhecida ou quando o tamanho da amostra é grande. Se não, supomos que a variável de interesse é normal e usamos o t-score.\n\n7.3.7.1 Aplicação no python\nNo python, podemos fazer o intervalo de confiança com t-score fazendo as seguintes operações:\n\ntempos = camaras.tempo\nmedia = tempos.mean()\nsd = tempos.std()\nz = stats.norm.ppf(0.975)\ntscore = stats.t.ppf(0.975, df = len(tempos) - 1)\nn = len(tempos)\n\nvl_intervalo_t = tscore * sd / np.sqrt(n)\n\nmedia - vl_intervalo_t, media + vl_intervalo_t\n\n(3.1360526281801846, 3.2216300942330562)\n\n\nComo nossa amostra é grande, o resultado é praticamente o mesmo. Vamos ver com uma amostra pequena, comparando com o z-score:\n\ntempos = camaras.sample(20, random_state=42).tempo\n\nmedia = tempos.mean()\nsd = tempos.std()\n\nz = stats.norm.ppf(0.975)\ntscore = stats.t.ppf(0.975, df = len(tempos) - 1)\n\nn = len(tempos)\n\nvl_intervalo_z = z * sd / np.sqrt(n)\nvl_intervalo_t = tscore * sd / np.sqrt(n)\n\nprint(media - vl_intervalo_z, media + vl_intervalo_z)\nprint(media - vl_intervalo_t, media + vl_intervalo_t)\n\n2.037385709232428 4.425311073792897\n1.9563287310545743 4.5063680519707505\n\n\nVeja como o intervalo de confiança da normal é mais “otimista” do que o intervalo de confiança da t-student. Isso ocorre porque a t-student é mais larga, refletindo a incerteza adicional devido à variância amostral, enquanto a normal é mais estreita e faz uso de teoremas de convergência para aproximar a distribuição amostral da média.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testes de hipóteses</span>"
    ]
  },
  {
    "objectID": "08-correlacao-regressao.html",
    "href": "08-correlacao-regressao.html",
    "title": "8  Correlação e regressão",
    "section": "",
    "text": "8.1 Introdução\nAté agora, vimos uma introdução à teoria de testes de hipóteses. Os testes de hipóteses são a base de diversos avanços científicos feitos ao longo do século XX, como a testagem de procedimentos médicos e tratamentos, a avaliação de políticas públicas, a análise de dados de pesquisas de mercado, entre outros.\nNo entanto, o teste de hipóteses, sozinho, não é suficiente para obter esses avanços. Isso acontece porque a realidade costuma ser mais complexa do que algo que pode ser resumido em uma única variável. Por exemplo, imagine que você está interessado em saber se a proporção de decisões favoráveis de um juiz é diferente da proporção de decisões favoráveis de outro juiz. Você pode realizar um teste de hipóteses para comparar as proporções, mas isso não te dá informações sobre quais aspectos podem levar um juiz a ser mais ou menos favorável a determinados casos do que outros.\nO que está relacionado com o tempo dos processos? O que está relacionado com o valor de uma indenização? Quais são os condicionantes de uma decisão judicial? Para responder a essas perguntas, é necessário ir além do teste de hipóteses e explorar a relação entre variáveis. Neste notebook, vamos explorar duas formas de fazer isso: a correlação e a regressão (que é uma forma de generalizar a correlação).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlação e regressão</span>"
    ]
  },
  {
    "objectID": "08-correlacao-regressao.html#correlação",
    "href": "08-correlacao-regressao.html#correlação",
    "title": "8  Correlação e regressão",
    "section": "8.2 Correlação",
    "text": "8.2 Correlação\nCorrelação é uma medida estatística que descreve a relação entre duas variáveis numéricas. A correlação é uma medida de associação linear entre duas variáveis. Em outras palavras, a correlação mede o quanto uma variável varia em relação à outra.\nA correlação é uma medida que varia de -1 a 1. Quando a correlação é 1, as variáveis estão perfeitamente correlacionadas positivamente, ou seja, quando uma aumenta, a outra também aumenta. Quando a correlação é -1, as variáveis estão perfeitamente correlacionadas negativamente, ou seja, quando uma aumenta, a outra diminui. Quando a correlação é 0, as variáveis não estão correlacionadas.\nhttps://www.guessthecorrelation.com/\n\n\n\nA fórmula da correlação é dada por:\n\\[\nr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n\\]\nExplicando a fórmula: \\(r\\) é a correlação entre as variáveis \\(x\\) e \\(y\\), \\(x_i\\) e \\(y_i\\) são os valores das variáveis \\(x\\) e \\(y\\) para a observação \\(i\\), \\(\\bar{x}\\) e \\(\\bar{y}\\) são as médias das variáveis \\(x\\) e \\(y\\), respectivamente, e \\(n\\) é o número de observações.\nUma forma mais simples de escrever a fórmula da correlação é pensar nas variâncias e covariância das variáveis:\n\\[\nr = \\frac{cov(x, y)}{\\sqrt{var(x)var(y)}}\n\\]\nAs fórmulas acima são equivalentes. A covariância mede o quanto duas variáveis variam juntas, enquanto a variância mede o quanto uma variável varia sozinha. A correlação é a covariância normalizada pela variância das variáveis.\n\n8.2.1 Exemplo no python\nVamos calcular a correlação entre a idade de um desembargador e o tempo de magistratura.\n\nrelatores = (\n  camaras\n  .drop_duplicates('relator')\n  .query('rel_idade.isnull() == False and rel_tempo_magistratura.isnull() == False', engine='python')\n)\n\nrelatores.head(3)\n\n\n\n\n\n\n\n\nprocesso\nassunto\ncamara\nrelator\norigem\ncomarca\npolo_mp\ndecisao\nunanimidade\ndt_publicacao\nementa\ntempo\nrel_idade\nrel_id_municipio_nasc\nrel_faculdade_direito\nrel_tempo_magistratura\nrel_tipo_magistrado\nrel_quinto\n\n\n\n\n0\n00013452120188260535\nDIREITO PENAL - Crimes contra o Patrimônio - R...\n01ª Câmara de Direito Criminal\nMÁRIO DEVIENNE FERRAZ\nComarca de Guarulhos / Foro de Guarulhos / 4ª ...\nGUARULHOS\nPassivo\nParcialmente\nUnânime\n2020-03-13\nN/A (TJSP;  Apelação Criminal 0001345-21.2018....\n2.195756\n70.598220\n3534708.0\nFaculdade Católica de Direito de Santos\n44.900753\ndesembargador\nnão\n\n\n1\n00647287520158260050\nDIREITO PENAL - Crimes Previstos na Legislação...\n07ª Câmara de Direito Criminal\nALBERTO ANDERSON FILHO\nComarca de São Paulo / Foro Central Criminal B...\nSAO PAULO\nPassivo\nNegaram\nUnânime\n2016-08-02\nAPELAÇÃO CRIMINAL – Tráfico de entorpecentes ...\n1.585216\n69.590691\n3509502.0\nFaculdade de Direito da Pontifícia Universidad...\n35.835729\ndesembargador\nnão\n\n\n2\n00039169220148260150\nDIREITO PENAL-Crimes contra o Patrimônio-Aprop...\n14ª Câmara de Direito Criminal\nFERNANDO TORRES GARCIA\nComarca de Cosmópolis / Foro de Cosmópolis / 1...\nCOSMOPOLIS\nPassivo\nNegaram\nUnânime\n2020-04-27\nAPELAÇÃO CRIMINAL – Apropriação indébita qual...\n6.318960\n64.763860\n3550308.0\nFaculdade de Direito da Universidade de São Paulo\n40.027379\ndesembargador\nnão\n\n\n\n\n\n\n\nAgora vamos fazer um gráfico de dispersão\n\nsns.relplot(data=relatores, x='rel_idade', y='rel_tempo_magistratura')\n\n\n\n\n\n\n\n\nNote que parece existir uma relação linear entre as variáveis. Vamos calcular a correlação para confirmar essa hipótese.\n\ncorrelacao = relatores['rel_idade'].corr(relatores['rel_tempo_magistratura'])\n\ncorrelacao\n\n0.6476657995825739\n\n\nNote que a correlação é uma função comutativa, ou seja, a correlação entre x e y é a mesma que a correlação entre y e x:\n\ncorrelacao_2 = relatores['rel_tempo_magistratura'].corr(relatores['rel_idade'])\n\ncorrelacao_2\n\n0.6476657995825738\n\n\nComo testar a significância da correlação? Para isso, podemos usar um teste de hipóteses. A hipótese nula é que a correlação é igual a 0, e a hipótese alternativa é que a correlação é diferente de 0. O teste de hipóteses para a correlação é um teste t de Student, já que temos um resultado bem parecido com o teste de comparação de médias.\nPara isso, utilizamos o método pearsonr da biblioteca scipy.stats. Esse método retorna a correlação e o p-valor do teste de hipóteses.\n\ncorr, p_value = stats.pearsonr(relatores['rel_idade'], relatores['rel_tempo_magistratura'])\n\ncorr, p_value\n\n(0.6476657995825736, 5.235066377782658e-12)\n\n\nNesse caso, o valor-p é menor que 0.05, então rejeitamos a hipótese nula e concluímos que a correlação é estatisticamente significante.\n\n\n8.2.2 Outliers e correlação\nVeja que no gráfico temos alguns pontos que estão bem longe do padrão. Esses pontos são chamados de outliers. Os outliers podem influenciar a correlação, já que eles podem afetar a relação linear entre as duas quantidades, como vimos no jogo guess the correlation.\nQuando isso acontece, é importante investigar a origem desses pontos. Eles podem ser erros de medição, erros de digitação, ou podem ser realmente pontos com um comportamento diferente do padrão geral, o que poderia indicar estudos de caso interessantes. Em análises do direito, estudar outliers pode ser interessante para estudar casos em que o direito foi aplicado de forma diferente do padrão e, assim, aprimorar nosso entendimento sobre o fenômeno jurídico.\nPor exemplo, no estudo Observatório do Mercado de Capitais: Conselho de Recursos do Sistema Financeiro Nacional, a ABJ analisou os recursos interpostos no Conselho de Recursos do Sistema Financeiro Nacional (CRSFN), também chamado de Conselhinho. A análise identificou 2 casos em que o valor da multa após revisão do caso foi maior do que o valor original. No sistema judiciário, isso não deveria ser possível, já que isso violaria o princípio do non reformatio in pejus. A verdade é que essa regra não se aplica ao CRSFN, já que ele é um órgão administrativo e não um órgão judicial. Isso mostra que a análise de outliers pode ser uma ferramenta poderosa para entender o sistema jurídico.\n\nNo nosso caso das câmaras, podemos investigar os outliers aplicando alguma regra para detectar os casos:\n\nrelatores.query('rel_tempo_magistratura &lt; 32')\n\n\n\n\n\n\n\n\nprocesso\nassunto\ncamara\nrelator\norigem\ncomarca\npolo_mp\ndecisao\nunanimidade\ndt_publicacao\nementa\ntempo\nrel_idade\nrel_id_municipio_nasc\nrel_faculdade_direito\nrel_tempo_magistratura\nrel_tipo_magistrado\nrel_quinto\n\n\n\n\n31\n30376156820138260405\nDIREITO PENAL-Crimes contra o Patrimônio-Recep...\n06ª Câmara de Direito Criminal\nRICARDO TUCUNDUVA\nComarca de Osasco / Foro de Osasco / 4ª Vara C...\nOSASCO\nPassivo\nNegaram\nUnânime\n2017-02-24\nN/A (TJSP;  Apelação Criminal 3037615-68.2013....\n4.147844\n76.000000\n3550308.0\nFaculdade de Direito da Universidade de São Paulo\n30.047912\ndesembargador\nsim\n\n\n93\n00421315820128260005\nDIREITO PENAL - Lesão Corporal - Decorrente de...\n12ª Câmara de Direito Criminal\nANGÉLICA DE ALMEIDA\nComarca de São Paulo / Foro Regional de São Mi...\nSAO PAULO\nPassivo\nPunibilidade Extinta\nUnânime\n2018-02-23\nN/A (TJSP;  Apelação Criminal 0042131-58.2012....\n6.146475\n77.144422\n3552205.0\nFaculdade de Direito da Pontifícia Universidad...\n28.509240\ndesembargador\nsim\n\n\n\n\n\n\n\nA explicação, nesse caso, é que os desembargadores vieram do quinto constitucional e, portanto, podem não ter tanto tempo de magistratura.\n\n\n8.2.3 Correlação e causalidade\nUma discussão muito comum em estatística é a diferença entre correlação e causalidade. Correlação é uma medida de associação entre duas variáveis, enquanto causalidade é uma relação de causa e efeito entre duas variáveis. A correlação não implica causalidade. Em outras palavras, só porque duas variáveis estão correlacionadas, não significa que uma causa a outra.\nUm site muito legal para explorar correlações espúrias é o Spurious Correlations, que mostra correlações absurdas entre variáveis, como a relação quase perfeita entre o número de roubos no Alaska e o salário dos professores nos EUA.\n\nOutro ponto importante é que o fato de duas variáveis terem uma correlação baixa não significa que elas não tenham uma relação (causal ou não). O problema de Anscombe é um exemplo clássico disso. O problema de Anscombe é um conjunto de 4 datasets que têm a mesma média, variância, correlação e regressão linear. No entanto, os gráficos são completamente diferentes. Isso mostra que a correlação não é suficiente para descrever a relação entre duas variáveis.\nOutro exemplo contemporâneo mais divertido é o Datasaurus Dozen, que mostra 12 datasets com a mesma média, variância e correlação, mas com gráficos completamente diferentes:\n\nSe quiser ver mais detalhes do Datasaurus Dozen, você pode acessar este post do Medium.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlação e regressão</span>"
    ]
  },
  {
    "objectID": "08-correlacao-regressao.html#regressão",
    "href": "08-correlacao-regressao.html#regressão",
    "title": "8  Correlação e regressão",
    "section": "8.3 Regressão",
    "text": "8.3 Regressão\nA regressão vai um pouco além da correlação, permitindo tracejar uma linha que melhor se ajusta aos dados. A regressão é uma técnica estatística que permite modelar a relação entre uma variável dependente e uma ou mais variáveis independentes. A regressão pode ser pensada como uma generalização da correlação, já que a correlação é um caso especial de regressão, onde temos apenas uma variável independente. A maior parte dos estudos, tanto na academia quanto no mercado, utiliza regressão para entender a relação entre variáveis. Por exemplo, modelos preditivos de resultados de processos e até mesmo ferramentas avançadas como LLMs utilizam o framework de regressão como a base de tudo.\nA regressão linear simples é o modelo de regresão mais simples que existe, e modela a relação entre uma variável dependente e uma variável independente. O fato de ser simples não significa que ele é inútil. Muitos problemas do mundo real podem ser modelados com uma regressão linear simples, e ela é a base para os modelos mais complexos.\nA regressão linear é dada pela fórmula:\n\\[\ny = \\beta_0 + \\beta_1x + \\epsilon\n\\]\nOnde \\(y\\) é a variável dependente, \\(x\\) é a variável independente, \\(\\beta_0\\) é o intercepto, \\(\\beta_1\\) é o coeficiente angular, e \\(\\epsilon\\) é o erro. O objetivo da regressão é encontrar os valores de \\(\\beta_0\\) e \\(\\beta_1\\) que minimizam o erro \\(\\epsilon\\).\nPrecisamos detalhar melhor o que é variável dependente e independente. A variável dependente é aquilo que queremos prever, que queremos explicar. Por exemplo, se queremos prever o valor de uma indenização, o valor da indenização é a variável dependente. A variável independente é aquilo que usamos para prever a variável dependente. Por exemplo, se queremos prever o valor de uma indenização, podemos usar o valor da causa, o local de distribuição, juiz do caso, o tipo de ação, entre outros, como variáveis independentes. Por enquanto, vamos considerar apenas uma variável independente, mas a regressão pode ser generalizada para mais variáveis independentes.\nNo nosso caso, a variável dependente é o tempo de magistratura e a variável independente é a idade do desembargador. A pergunta de pesquisa é: o tempo de magistratura é influenciado pela idade do desembargador? Intuitivamente, parece que sim, já que desembargadores mais velhos tendem a ter mais tempo de magistratura. Também vimos que a correlação entre as variáveis é positiva, o que corrobora essa intuição. Agora, vamos quantificar essa relação.\nPara ajustar nossa regressão, vamos utilizar um novo pacote: o statsmodels. O statsmodels é uma biblioteca de Python que permite fazer análises estatísticas mais avançadas, como regressão, séries de tempo, entre outros. A função que vamos usar por enquanto é a ols, que significa Ordinary Least Squares, ou Mínimos Quadrados Ordinários. Essa função ajusta uma regressão linear simples aos dados.\n\nmodelo = smf.ols('rel_tempo_magistratura ~ rel_idade', data=relatores).fit()\n\nmodelo.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrel_tempo_magistratura\nR-squared:\n0.419\n\n\nModel:\nOLS\nAdj. R-squared:\n0.413\n\n\nMethod:\nLeast Squares\nF-statistic:\n63.59\n\n\nDate:\nSun, 07 Apr 2024\nProb (F-statistic):\n5.24e-12\n\n\nTime:\n20:01:05\nLog-Likelihood:\n-237.78\n\n\nNo. Observations:\n90\nAIC:\n479.6\n\n\nDf Residuals:\n88\nBIC:\n484.6\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n6.2034\n4.070\n1.524\n0.131\n-1.884\n14.291\n\n\nrel_idade\n0.4789\n0.060\n7.974\n0.000\n0.360\n0.598\n\n\n\n\n\n\n\n\nOmnibus:\n35.274\nDurbin-Watson:\n2.006\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n92.330\n\n\nSkew:\n-1.346\nProb(JB):\n8.93e-21\n\n\nKurtosis:\n7.168\nCond. No.\n762.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nO sumário do modelo tem mais detalhes que o necessário para interpretar o modelo, mas ainda assim é muito interessante. Vamos focar nos coeficientes, que são as estimativas para os valores de \\(\\beta_0\\) e \\(\\beta_1\\) que minimizam o erro da equação acima.\n\nO coeficiente de idade é 0.5, o que significa que, para cada ano a mais de idade, o tempo de magistratura aumenta em 0.5 anos. O coeficiente de idade é estatisticamente significante, já que o valor-p associado a ele é menor que 0.05. O teste de hipóteses para o coeficiente de idade é um teste t de Student, assim como o teste de hipóteses para a correlação, e o resultado desses testes é o mesmo.\nO intercepto é 6.2, o que significa que, para um desembargador de 0 anos, o tempo de magistratura é de 6.2 anos. O intercepto não é estatisticamente significante, já que o valor-p é maior que 0.05. Nesse caso, realmente o intercepto não faz muito sentido, já que não existem desembargadores de 0 anos de idade.\n\nA outra informação mais importante do sumário é o \\(R^2\\) (r-squared). O \\(R^2\\) é uma medida de quão bem o modelo se ajusta aos dados, mostrando qual o percentual da variabilidade dos dados é explicado pelo modelo.\nO \\(R^2\\) varia de 0 a 1, e quanto mais próximo de 1, melhor o modelo se ajusta aos dados. O \\(R^2\\) é uma medida de qualidade do modelo, e é muito importante para avaliar a qualidade de um modelo de regressão.\nVocê pode se perguntar: a interpretação \\(R^2\\) e do coeficiente de correlação são muito parecidos, não é? A única diferença é que o \\(R^2\\) é sempre positivo, enquanto a correlação pode ser negativa. E de fato essa intuição faz sentido: para uma regressão linear simples (ou seja, com uma variável independente), o \\(R^2\\) é o quadrado da correlação entre as variáveis:\n\ncorrelacao = relatores['rel_idade'].corr(relatores['rel_tempo_magistratura'])\n\nr2 = modelo.rsquared\n\ncorrelacao ** 2, r2\n\n(0.4194709879489347, 0.41947098794893445)\n\n\nNo entanto, o \\(R^2\\) é uma medida mais geral, que pode ser aplicada a modelos de regressão mais complexos, com mais variáveis independentes. O \\(R^2\\) pode ser pensado como uma medida de qualidade do modelo, enquanto a correlação é uma medida de associação entre variáveis.\nPara visualizar a regressão, podemos fazer um gráfico de dispersão com a linha de regressão. Para isso, vamos usar a função lmplot que plota o gráfico de dispersão e a linha de regressão.\n\nsns.lmplot(data=relatores, x='rel_idade', y='rel_tempo_magistratura')\n\n\n\n\n\n\n\n\nNa figura, a mancha azul em torno da linha de regressão é a banda de confiança da regressão. A banda de confiança é uma faixa em torno da linha de regressão que mostra a incerteza do modelo. Quanto mais larga a banda de confiança, menos confiante o modelo é naquele ponto. Note que a dispersão dos dados é maior nas idades mais avançadas dos desembargadores, o que é refletido na banda de confiança.\n\n8.3.1 Nota histórica e referências\nO nome “Ordinary Leasts Squares” vem do chamado método dos mínimos quadrados, que é o procedimento matemático para encontrar os valores de \\(\\beta_0\\) e \\(\\beta_1\\) que melhor se ajustam aos dados. O método dos mínimos quadrados foi desenvolvido por Legendre e Gauss no início do século XIX, e é um dos métodos mais importantes da estatística.\nO método dos mínimos quadrados é um método de otimização, que é uma técnica matemática para encontrar o melhor valor de uma função. Na prática, definimos um problema de otimização (uma função de perda + um método de minimização da perda) para cada modelo. Por exemplo, em um modelo de redes neurais, podemos ter diferentes funções de perda (como a entropia cruzada) e diferentes métodos de minimização (como a descida de gradiente).\nO método dos mínimos quadrados é um dos métodos de otimização mais simples, mas é muito poderoso e é a base de muitos modelos estatísticos.\nSe tiver mais interesse sobre o tema, recomendamos o livro Introduction to Statistical Learning, que é um livro introdutório sobre aprendizado estatístico e que é muito bom para quem quer aprender mais sobre regressão e outros modelos estatísticos.\n\n\n\n8.3.2 Regressão linear múltipla\nA regressão linear simples pode ser generalizada para mais variáveis independentes. A regressão linear múltipla é dada pela fórmula:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon\n\\]\nNosso interesse agora é estudar o tempo dos processos em função da idade do desembargador e do polo do Ministério Público. Para isso, vamos ajustar um modelo de regressão linear múltipla com uma variável dependente (tempo do processo, em anos) e duas variáveis independentes (idade do desembargador e polo do Ministério Público).\n\nmodelo_tempo = smf.ols('tempo ~ rel_idade + polo_mp', data=camaras).fit()\n\nmodelo_tempo.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ntempo\nR-squared:\n0.004\n\n\nModel:\nOLS\nAdj. R-squared:\n0.003\n\n\nMethod:\nLeast Squares\nF-statistic:\n15.20\n\n\nDate:\nSun, 07 Apr 2024\nProb (F-statistic):\n2.57e-07\n\n\nTime:\n20:01:06\nLog-Likelihood:\n-18762.\n\n\nNo. Observations:\n8608\nAIC:\n3.753e+04\n\n\nDf Residuals:\n8605\nBIC:\n3.755e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.0376\n0.277\n14.575\n0.000\n3.495\n4.581\n\n\npolo_mp[T.Passivo]\n-0.4478\n0.085\n-5.268\n0.000\n-0.614\n-0.281\n\n\nrel_idade\n-0.0066\n0.004\n-1.652\n0.099\n-0.014\n0.001\n\n\n\n\n\n\n\n\nOmnibus:\n4063.861\nDurbin-Watson:\n1.990\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n27633.729\n\n\nSkew:\n2.169\nProb(JB):\n0.00\n\n\nKurtosis:\n10.631\nCond. No.\n806.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nVeja que, agora, temos dois coeficientes: um para a idade do desembargador e outro para o polo do Ministério Público. O coeficiente de idade é -0.007, o que significa que, para cada ano a mais de idade, o tempo do processo reduz em 0.007 anos (aproximadamente 3 dias). Note que o valor não é estatisticamente significante, já que o valor-p associado a ele, de 0.099, é maior que 0.05. O coeficiente de polo é -0.448, o que significa que, quando o polo do MP é passivo, o tempo do processo reduz em 0.448 dias. O coeficiente de polo é estatisticamente significante, já que o valor-p associado a ele é menor que 0.05.\nNesse caso, o \\(R^2\\) é 0.004, o que significa que o modelo explica menos de 1% da variabilidade dos dados. O \\(R^2\\) é bem baixo, o que significa que o modelo não consegue representar completamente o tempo dos processos. O valor do \\(R^2\\) pode ser usada como um indicativo da qualidade do ajuste, mas não deve ser usado como a única medida de qualidade do modelo. Outras análises de diagnóstico e discussões com especialistas são necessárias para avaliar a qualidade do modelo.\nNote que, além do \\(R^2\\), temos o \\(R^2\\) ajustado. O \\(R^2\\) ajustado é uma versão do \\(R^2\\) que penaliza a inclusão de variáveis independentes no modelo. O \\(R^2\\) ajustado é uma medida mais conservadora da qualidade do modelo, já que penaliza a inclusão de variáveis independentes desnecessárias. No nosso caso, o \\(R^2\\) ajustado é 0.003. Quando temos um modelo com diversas variáveis independentes, recomenda-se usar o \\(R^2\\) ajustado.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlação e regressão</span>"
    ]
  },
  {
    "objectID": "08-correlacao-regressao.html#regressão-logística",
    "href": "08-correlacao-regressao.html#regressão-logística",
    "title": "8  Correlação e regressão",
    "section": "8.4 Regressão logística",
    "text": "8.4 Regressão logística\nA regressão logística é uma generalização da regressão linear para variáveis dependentes categóricas. A regressão logística é usada quando a variável dependente é binária, ou seja, tem apenas dois valores possíveis. A regressão logística é dada pela fórmula:\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)}}\n\\]\nOnde \\(p\\) é a probabilidade de a variável dependente ser 1, \\(e\\) é o número de Euler (aproximadamente 2.71828), e \\(\\beta_0, \\beta_1, ..., \\beta_p\\) são os coeficientes da regressão. A interpretação dos coeficientes é a mesma da regressão linear: o coeficiente \\(\\beta_1\\) é a mudança na probabilidade de a variável dependente ser 1 para uma unidade a mais de \\(x_1\\), mantendo todas as outras variáveis constantes.\nPara entender a fórmula, vamos olhar um caso com apenas uma variável independente. Nesse caso, a fórmula da regressão logística é:\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}} = g(\\beta_0 + \\beta_1x)\n\\]\nVeja que, ainda que a relação entre a variável de interesse (dependente) e a variável explicativa (independente) tenha uma forma estranha, a relação entre a variável de interesse e a variável explicativa ainda é linear. Por isso que a regressão logística faz parte da classe de “modelos lineares generalizados”.\n\n8.4.1 Exemplo no python\nAgora, vamos fazer uma aplicação da regressão logística. Nosso interesse é estudar a probabilidade da parte autora aceitar um acordo a partir de um valor que oferecemos a ela em um caso de direito do consumidor. Na nossa base de dados, temos uma lista de processos similares, contendo\n\no valor do acordo oferecido, como uma proporção do valor da causa (no mercado, esse valor é chamado de valor de alçada). Esse valor varia de 0 a 1, e representa a proporção do valor da causa que estamos oferecendo à parte autora;\na informação sobre o tipo de caso: se envolve dano moral ou dano material;\nse a parte autora aceitou ou não o acordo.\n\nA ideia é estabelecer uma relação entre o valor do acordo e a aceitação do acordo, controlando pelo tipo de caso. Para isso, vamos ajustar um modelo de regressão logística com duas variáveis independentes: o valor da proposta de acordo (alçada) e o tipo de caso. A variável dependente é a aceitação do acordo, ou seja, uma variável binária que indica se a parte autora aceitou ou não o acordo.\n\nacordos = pd.read_csv('https://github.com/jtrecenti/main-cdad2/releases/download/data/acordos.csv')\n\n# nomes originais: Valor de Alçada, Tipo de Caso,   Aceita Acordo\nacordos.rename(columns={\n  'Valor de Alçada': 'valor_alcada',\n  'Tipo de Caso': 'tipo_caso',\n  'Aceita Acordo': 'aceita_acordo'}, inplace=True)\n\nacordos.head(10)\n\n\n\n\n\n\n\n\nvalor_alcada\ntipo_caso\naceita_acordo\n\n\n\n\n0\n0.374540\n0\n0\n\n\n1\n0.950714\n1\n1\n\n\n2\n0.731994\n1\n0\n\n\n3\n0.598658\n1\n0\n\n\n4\n0.156019\n1\n0\n\n\n5\n0.155995\n0\n0\n\n\n6\n0.058084\n1\n0\n\n\n7\n0.866176\n0\n1\n\n\n8\n0.601115\n0\n0\n\n\n9\n0.708073\n0\n1\n\n\n\n\n\n\n\n\nmodelo_logistico = smf.logit('aceita_acordo ~ valor_alcada + tipo_caso', data=acordos).fit()\n\nmodelo_logistico.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.280667\n         Iterations 8\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\naceita_acordo\nNo. Observations:\n1000\n\n\nModel:\nLogit\nDf Residuals:\n997\n\n\nMethod:\nMLE\nDf Model:\n2\n\n\nDate:\nSun, 07 Apr 2024\nPseudo R-squ.:\n0.4647\n\n\nTime:\n20:01:07\nLog-Likelihood:\n-280.67\n\n\nconverged:\nTrue\nLL-Null:\n-524.36\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.456e-106\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-7.6311\n0.541\n-14.104\n0.000\n-8.692\n-6.571\n\n\nvalor_alcada\n9.6214\n0.690\n13.941\n0.000\n8.269\n10.974\n\n\ntipo_caso\n0.0727\n0.213\n0.342\n0.733\n-0.344\n0.490\n\n\n\n\n\nVamos interpretar os resultados do modelo.\nO coeficiente de alçada é 9.621, o que significa que, para cada unidade a mais de alçada, a chance de a parte autora aceitar o acordo aumenta (a interpretação da magnitude desse aumento é mais complicada e não veremos por enquanto). O coeficiente de alçada é estatisticamente significante, já que o valor-p associado a ele é menor que 0.05.\nO coeficiente do tipo de caso ser dano moral é 0.213, o que significa que, para casos de dano moral, a probabilidade de a parte autora aceitar o acordo aumenta. O coeficiente de dano moral não é estatisticamente significante, já que o valor-p associado a ele, de 0.73, é maior que 0.05.\nJá que a probabilidade de aceitação não depende do tipo de caso, podemos simplificar o modelo e ajustar um modelo de regressão logística apenas com a alçada:\n\nmodelo_logistico = smf.logit('aceita_acordo ~ valor_alcada', data=acordos).fit()\n\nmodelo_logistico.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.280725\n         Iterations 8\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\naceita_acordo\nNo. Observations:\n1000\n\n\nModel:\nLogit\nDf Residuals:\n998\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nSun, 07 Apr 2024\nPseudo R-squ.:\n0.4646\n\n\nTime:\n20:01:07\nLog-Likelihood:\n-280.73\n\n\nconverged:\nTrue\nLL-Null:\n-524.36\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n5.568e-108\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-7.5893\n0.526\n-14.434\n0.000\n-8.620\n-6.559\n\n\nvalor_alcada\n9.6114\n0.689\n13.950\n0.000\n8.261\n10.962\n\n\n\n\n\nAgora que temos o valor final do modelo, podemos fazer uma interpretação mais direta. Podemos fazer um gráfico representando a relação entre a alçada e a probabilidade de aceitação do acordo:\n\nx = np.linspace(0, 1, 100)\ny = modelo_logistico.predict(pd.DataFrame({'valor_alcada': x}))\n\ndf_predict = pd.DataFrame({'valor_alcada': x, 'probabilidade': y})\n\nsns.lineplot(data=df_predict, x='valor_alcada', y='probabilidade')\n\n\n\n\n\n\n\n\nPelo gráfico, vemos que a probabilidade de aceitação do acordo aumenta com o valor da alçada. Um valor de alçada abaixo de 40% do valor da causa implica em probabilidades ínfimas de aceitação do acordo, enquanto um valor de alçada próximo de 100% implica em uma aceitação maior de 80% do acordo, apesar desse valor não ser interessante para a parte ré. O valor ótimo de acordo é um trade-off entre a probabilidade de aceitação e a expectativa do resultado do processo pela parte ré.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Correlação e regressão</span>"
    ]
  },
  {
    "objectID": "09-modelos-classificacao.html",
    "href": "09-modelos-classificacao.html",
    "title": "9  Modelos de classificação",
    "section": "",
    "text": "9.1 Introdução\nImagine que você é advogado em um grande escritório, responsável por gerenciar uma carteira de centenas de processos. Cada caso traz consigo uma série de informações: o juiz responsável, o histórico das partes envolvidas, a natureza da causa, entre outros. Diante desse conjunto de dados, surge a pergunta: como prever quais casos têm maior chance de sucesso, e quais vale mais a pena fazer um acordo?\nPense em poder antecipar se um juiz concederá ou não uma liminar com base em decisões anteriores e características do caso. Ou em prever se um acordo é a melhor estratégia, considerando fatores como o histórico das partes e o contexto jurídico.\nAté agora, utilizamos modelos de regressão dentro do paradigma inferencial, onde buscamos entender a relação entre variáveis e fazer afirmações utilizando testes de hipóteses e intervalos de confiança.\nAgora, vamos explorar modelos dentro paradigma preditivo, onde buscamos aumentar o desempenho das nossas previsões. No contexto jurídico, isso é especialmente útil em aplicações corporativas, já que muitas vezes nosso interesse é conseguir avaliar se um caso terá resultado positivo ou negativo, sem necessariamente precisar entender a relação entre as variáveis.\nAlém dos modelos de regressão, também vamos trabalhar com modelos de classificação, que são os casos em que a variável dependente é categórica. É importante destacar que podemos utilizar abordagens inferenciais em modelos de classificação, assim como podemos utilizar abordagens preditivas em modelos de regressão. Então, estamos mudando esses dois aspectos ao mesmo tempo: i) o tipo de variável dependente (antes era numérica e agora é categórica) e ii) a abordagem de inferência ou predição (antes era inferencial e agora é preditiva).\nNo direito, as variáveis dependentes numéricas geralmente se resumem a valor e tempo. Já as variáveis dependentes categóricas são variadas. Por exemplo:\nNote que, em todos esses casos, a variável dependente é categórica, não numérica. A quantidade de categorias pode variar: o resultado de um processo pode ser uma variável binária (procedente ou improcedente), mas também pode ter várias categorias (por exemplo, procedente, parcialmente procedente, improcedente). Existem modelos mais apropriados para cada caso.\nVale enfatizar que, como estamos no paradigma preditivista, nosso interesse não está mais em entender a relação das variáveis, mas em construir um modelo que possa fazer boas previsões para a variável dependente com base nas variáveis independentes. Esse modelo pode ser simples ou complexo, o importante é que ele funcione bem para esse objetivo.\nUm modelo preditivo, essencialmente, tenta criar uma função\n\\[\nf(x) = y\n\\]\nOnde \\(x\\) é a variável independente e \\(y\\) é a variável dependente. O objetivo é que essa função seja capaz de prever o valor de \\(y\\) a partir de um novo valor de \\(x\\). Por exemplo, se quisermos prever o resultado de um processo, \\(x\\) é a variável independente, como assunto do processo e \\(y\\) é a variável dependente, como a procedência.\nNa prática, o que temos é\n\\[\nf(x) \\approx y\n\\]\nou seja, a função não é perfeita, mas é capaz de prever o valor de \\(y\\) com uma boa margem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos de classificação</span>"
    ]
  },
  {
    "objectID": "09-modelos-classificacao.html#introdução",
    "href": "09-modelos-classificacao.html#introdução",
    "title": "9  Modelos de classificação",
    "section": "",
    "text": "Resultado de um processo;\nUnanimidade\nRecorreu ou não recorreu\nConcedeu ou não uma liminar\nProvisionar ou não o caso\nAcordo ou não\n…\n\n\n\n\n\n\n\n\n\n\n9.1.1 Overfitting\nAqui, precisaremos tomar cuidado com overfitting. Vamos criar modelos com estrutura cada vez mais complexa, podendo levar os valores de \\(x\\) até \\(y\\) de várias formas diferentes. O problema em fazer isso é que nossa função pode ficar tão complexa que ela deixa de funcionar bem para os casos que não foram usados para treinar o modelo. Para lidar com isso, vamos separar nossa base de dados em treino e teste. O modelo será treinado com a base de treino e testado com a base de teste. Se o modelo funcionar bem para a base de treino, mas mal para a base de teste, é sinal de que ele está com problemas de overfitting.\n\n\n9.1.2 No free lunch theorem\nUma dúvida que podemos ter é: será que não existe um modelo que funcione melhor para todas as situações? Ou seja, se o objetivo é prever a variável dependente, será que não existe um modelo preditivo que funcione melhor sempre?\nA resposta é não, e isso pode ser demonstrado pelo teorema No Free Lunch. O teorema diz que, se o objetivo é prever uma variável dependente, não existe um modelo preditivo que funcione melhor para todas as situações.\n\n\n9.1.3 A ferramenta: scikit-learn\nVamos utilizar o scikit-learn, que é uma biblioteca de Python muito usada para machine learning.\n\n\n\nimage.png\n\n\n\n\n9.1.4 Base de dados\nPara nossos exemplos, vamos utilizar uma base de dados de processos na área cível, da empresa Vivo. Nosso objetivo será predizer a decisão da sentença de primeiro grau (nossa variável dependente) a partir de uma série de variáveis independentes (comarca, juiz, valor da causa, etc).\n\nvivo = pd.read_csv('https://github.com/jtrecenti/main-cdad2/releases/download/data/vivo.csv')\nvivo.head(3)\n\n\n\n\n\n\n\n\nid\nassunto\nvalor\nvirtual\npolo_ativo\ndesfecho_vivo\ndata_decisao\nteve_revelia\ndata_distribuicao\nadv_polo_ativo\n...\npags_inicial\npags_contestacao\njuiz_tempo_vara\njuiz_tempo_posse\njuiz_sexo\njuiz_rendimento\ncomarca\ncircunscricao\nregiao\nentrancia\n\n\n\n\n0\n10353896220148260576\nIndenizacao Por Dano Moral\n15000.0\nSim\nJailson Fonseca Dos Santos\nDerrota (Total ou Parcial)\n2015-11-26\nNão\n2014-12-05\nJorge Antonio Pantano Pansani\n...\n9\n6\n12.607803\n30.324435\nMasculino\n37547.51\nSao Jose Do Rio Preto\nSao Jose Do Rio Preto\nSj Rio Preto\nEntrância Final\n\n\n1\n10023887720148260482\nMedida Cautelar\n724.0\nSim\nAntonio Antenor Da Silva\nDerrota (Total ou Parcial)\n2014-08-19\nNão\n2014-03-05\nMaycon Liduenha Cardoso\n...\n10\n0\n10.480493\n21.297741\nMasculino\n34769.26\nPresidente Prudente\nPresidente Prudente\nPresidente Prudente\nEntrância Final\n\n\n2\n00012546820158260297\nObrigacao De Fazer / Nao Fazer\n20000.0\nNão\nAparecido Barbato\nDerrota (Total ou Parcial)\n2015-04-08\nNão\n2015-02-11\nFabio Cesar Tondato\n...\n0\n0\n8.563997\n12.509240\nMasculino\n34903.59\nJales\nJales\nAracatuba\nEntrância Final\n\n\n\n\n3 rows × 23 columns\n\n\n\n\nvivo.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12137 entries, 0 to 12136\nData columns (total 23 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 12137 non-null  object \n 1   assunto            12137 non-null  object \n 2   valor              12088 non-null  float64\n 3   virtual            12137 non-null  object \n 4   polo_ativo         12137 non-null  object \n 5   desfecho_vivo      12137 non-null  object \n 6   data_decisao       10835 non-null  object \n 7   teve_revelia       12137 non-null  object \n 8   data_distribuicao  12137 non-null  object \n 9   adv_polo_ativo     11788 non-null  object \n 10  tipo_vara          12137 non-null  object \n 11  vara_foro          12137 non-null  object \n 12  juiz               12125 non-null  object \n 13  pags_inicial       12137 non-null  int64  \n 14  pags_contestacao   12137 non-null  int64  \n 15  juiz_tempo_vara    12125 non-null  float64\n 16  juiz_tempo_posse   12125 non-null  float64\n 17  juiz_sexo          12125 non-null  object \n 18  juiz_rendimento    12125 non-null  float64\n 19  comarca            12137 non-null  object \n 20  circunscricao      12137 non-null  object \n 21  regiao             12137 non-null  object \n 22  entrancia          12137 non-null  object \ndtypes: float64(4), int64(2), object(17)\nmemory usage: 2.1+ MB\n\n\nNossa variável dependente, então, será a desfecho_vivo. Começamos fazendo uma conta delas:\n\nvivo['desfecho_vivo'].value_counts()\n\ndesfecho_vivo\nDerrota (Total ou Parcial)       6644\nVitória                          3495\nAtivo                            1302\nAcordo                            504\nExtinto sem análise do mérito     134\nDesistência                        58\nName: count, dtype: int64\n\n\nNessa primeira etapa, vamos trabalhar apenas com os resultados de vitória (da vivo) ou derrota, total ou parcial. Assim, temos uma variável binária, ou seja, com apenas duas categorias. Para facilitar o uso dentro do scikit-learn, vamos dar o valor 1 para o desfecho de vitória e 0 para o desfecho de derrota, colocando isso na variável y. Além disso, vamos filtrar a base de dados para ter apenas os casos que tiveram desfecho de vitória ou derrota.\n\n# criando uma variável binária para a variável dependente\nvivo['y'] = np.where(vivo['desfecho_vivo'] == 'Vitória', 1, 0)\n\nvivo_f = vivo.query('desfecho_vivo == [\"Vitória\", \"Derrota (Total ou Parcial)\"]')\n\n# criamos essa cópia para facilitar as contas que faremos em seguida\nvivo_f = vivo_f.copy()\n\n# estamos tirando os vazios dessa variável por enquanto\n# depois vamos ver o que fazer com eles\nvivo_f = vivo_f.dropna(subset=['juiz_tempo_vara'])\n\nvivo_f.head(3)\n\n\n\n\n\n\n\n\nid\nassunto\nvalor\nvirtual\npolo_ativo\ndesfecho_vivo\ndata_decisao\nteve_revelia\ndata_distribuicao\nadv_polo_ativo\n...\npags_contestacao\njuiz_tempo_vara\njuiz_tempo_posse\njuiz_sexo\njuiz_rendimento\ncomarca\ncircunscricao\nregiao\nentrancia\ny\n\n\n\n\n0\n10353896220148260576\nIndenizacao Por Dano Moral\n15000.0\nSim\nJailson Fonseca Dos Santos\nDerrota (Total ou Parcial)\n2015-11-26\nNão\n2014-12-05\nJorge Antonio Pantano Pansani\n...\n6\n12.607803\n30.324435\nMasculino\n37547.51\nSao Jose Do Rio Preto\nSao Jose Do Rio Preto\nSj Rio Preto\nEntrância Final\n0\n\n\n1\n10023887720148260482\nMedida Cautelar\n724.0\nSim\nAntonio Antenor Da Silva\nDerrota (Total ou Parcial)\n2014-08-19\nNão\n2014-03-05\nMaycon Liduenha Cardoso\n...\n0\n10.480493\n21.297741\nMasculino\n34769.26\nPresidente Prudente\nPresidente Prudente\nPresidente Prudente\nEntrância Final\n0\n\n\n2\n00012546820158260297\nObrigacao De Fazer / Nao Fazer\n20000.0\nNão\nAparecido Barbato\nDerrota (Total ou Parcial)\n2015-04-08\nNão\n2015-02-11\nFabio Cesar Tondato\n...\n0\n8.563997\n12.509240\nMasculino\n34903.59\nJales\nJales\nAracatuba\nEntrância Final\n0\n\n\n\n\n3 rows × 24 columns\n\n\n\n\n\n9.1.5 Treino e teste\nNo paradigma preditivo, a base de dados é dividida em duas partes: i) treino e ii) teste. A base de treino é utilizada para ajustar o modelo, e a base de teste é utilizada para avaliar a qualidade do modelo ao final da análise. No momento, vamos esquecer completamente a base de teste, e nos concentrar apenas na base de treino.\nNo scikit learn, geralmente separamos a variável dependente e as variáveis independentes em objetos distintos. Por convenção, vamos chamar a variável dependente de y e as variáveis independentes de X. A letra maiúscula serve para denotar que X, por corresponder a várias colunas da base, é uma matriz, enquanto y é minúsculo por corresponder a apenas uma variável.\n\n# Separando as variáveis preditoras/independentes (X) e a variável alvo/dependente (y)\n# Vamos começar com um modelo simples, que considera apenas o valor e o tempo de vara do juiz como variáveis independentes.\nX = vivo_f[['valor', 'juiz_tempo_vara']]\ny = vivo_f['y']\n\n# Dividindo os dados em conjuntos de treino e teste\n# Utilizamos esse random_state para garantir que a divisão seja sempre a mesma\n# O valor de 0.25 indica que 25% dos dados serão usados para teste\n# Esse valor de 0.25 é arbitrário, poderia ser outro valor\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nX_train.head(10)\n\n\n\n\n\n\n\n\nvalor\njuiz_tempo_vara\n\n\n\n\n1843\n13308.0\n8.563997\n\n\n946\n8000.0\n2.852841\n\n\n10292\n5000.0\n8.563997\n\n\n5858\n1000.0\n12.416153\n\n\n2692\n7288.0\n4.194387\n\n\n11964\n5040.0\n4.194387\n\n\n3208\n5000.0\n2.852841\n\n\n2974\n38160.0\n13.796030\n\n\n1878\n5204.0\n6.666667\n\n\n10661\n1000.0\n22.036961\n\n\n\n\n\n\n\n\n# Vamos ver a distribuição da variável dependente nos conjuntos de treino e teste\ny_train.value_counts('y')\n\ny\n0    0.650388\n1    0.349612\nName: proportion, dtype: float64\n\n\nAlguns comentários sobre o processo de separação:\n\ntest_size=0.25: 25% dos dados serão usados para teste, e 80% dos dados serão usados para treino. Esse valor é arbitrário, e pode ser alterado de acordo com a quantidade de dados disponíveis. Por exemplo, se tivermos uma base muito grande, podemos usar um valor menor para teste, como 0.1 (10% dos dados para teste e 90% para treino).\nrandom_state=42: é uma semente para a função train_test_split. Isso garante que, se você rodar o código novamente, a base de treino e teste será a mesma, o que é importante para comparar modelos.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos de classificação</span>"
    ]
  },
  {
    "objectID": "09-modelos-classificacao.html#regressão-logística",
    "href": "09-modelos-classificacao.html#regressão-logística",
    "title": "9  Modelos de classificação",
    "section": "9.2 Regressão logística",
    "text": "9.2 Regressão logística\nPara os exemplos abaixo, vamos utilizar um modelo de classificação binária, onde a variável dependente é binária (apenas duas categorias). Nosso objetivo é prever a decisão da sentença de primeiro grau a partir de uma série de variáveis independentes.\nComo vimos na apostila de correlação e regressão: a regressão logística é uma generalização da regressão linear para variáveis dependentes binárias. A regressão logística é dada pela fórmula:\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)}}\n\\]\nOnde \\(p\\) é a probabilidade de a variável dependente ser 1, \\(e\\) é o número de Euler (aproximadamente 2.71828), e \\(\\beta_0, \\beta_1, ..., \\beta_p\\) são os coeficientes da regressão. A interpretação dos coeficientes é a mesma da regressão linear: o coeficiente \\(\\beta_1\\) é a mudança na probabilidade de a variável dependente ser 1 para uma unidade a mais de \\(x_1\\), mantendo todas as outras variáveis constantes.\nPara entender a fórmula, vamos olhar um caso com apenas uma variável independente. Nesse caso, a fórmula da regressão logística é:\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}} = g(\\beta_0 + \\beta_1x)\n\\]\nVeja que, ainda que a relação entre a variável de interesse (dependente) e a variável explicativa (independente) tenha uma forma estranha, a equação que associa os parâmetros da variável explicativa ainda é linear. Por isso que a regressão logística faz parte da classe de “modelos lineares generalizados”.\nPor exemplo, digamos que a variável independente seja o valor pedido na ação. Os valores ajustados da regressão logística ficariam com o seguinte desenho:\n\n# Obs: o código abaixo foi usado apenas para gerar o gráfico. Vamos olhar com mais calma o que ele faz mais para frente\n\ndef grafico_modelo(model):\n  # Gerar dados de exemplo\n  np.random.seed(42)\n  X = np.random.rand(100, 1) * 10000  # Valores da ação entre 0 e 10000\n  y = (X &gt; 5000 + np.random.normal(0, 1000, size=(100, 1))).astype(int).ravel()  # Decisão favorável com incerteza\n\n  # Ajustar o modelo de regressão logística\n  model.fit(X, y)\n\n  # Criar pontos para a curva logística\n  X_plot = np.linspace(0, 10000, 1000).reshape(-1, 1)\n  y_plot = model.predict_proba(X_plot)[:, 1]\n\n  df_line = pd.DataFrame({\n    'X': X_plot.ravel(),\n    'y': y_plot,\n  })\n\n  df_scatter = pd.DataFrame({\n    'X': X.ravel(),\n    'y': y,\n  })\n\n  sns.scatterplot(data=df_scatter, x='X', y='y', color = 'royalblue')\n  sns.lineplot(data=df_line, x='X', y='y', color = 'red')\n  plt.xlabel('Valor da Ação')\n  plt.ylabel('Decisão Favorável')\n\nmodel = LogisticRegression()\ngrafico_modelo(model)\n\n\n\n\n\n\n\n\nNote que os dados (os pontos em azul) são sempre 0 ou 1, por serem categóricos. É possível observar, no exemplo, que na faixa de valores entre 4000 e 6000, é possível encontrar tanto casos procedentes quanto improcedentes. Já abaixo de 4000, praticamente tudo é improcedente e, acima de 6000, praticamente tudo é procedente. O que a curva logística faz é ajustar uma curva que mostra a incerteza do modelo nos casos em que há uma sobreposição dos valores da variável independente.\nVale notar, novamente, que apesar de termos uma curva que é fácil de visualizar e interpretar, nosso interesse não é, nesse momento, interpretar a curva. Nosso interesse é utilizar o modelo para prever a decisão da sentença de primeiro grau. Para isso, precisaremos extrair métricas de qualidade do modelo que nos permitam comparar diferentes modelos e escolher o melhor deles. Ou seja, a visualização é boa para dar a intuição, mas não é nosso objetivo final.\n\n9.2.1 Ajustando um modelo de regressão logística\nPara ajustar um modelo de regressão logística, vamos utilizar a função LogisticRegression da biblioteca scikit-learn.\nLembre-se que, para ajustar um modelo de classificação, precisamos de uma base de treino. Testamos a qualidade do modelo na base de teste.\n\n# cria um modelo vazio\nmodelo = LogisticRegression()\n\n# ajusta o modelo\nmodelo_ajustado = modelo.fit(X_train, y_train)\n\n# imprime o modelo\nmodelo_ajustado\n\nc:\\Users\\julio\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\nAgora, não temos mais um .summary(), como existia no statsmodels, para estudar os resultados do modelo. Apenas por completude, vamos ajustar o mesmo modelo no statsmodels:\n\n# não vamos mais usar statsmodels, é apenas para mostrar como seria a mesma coisa com ele\nimport statsmodels.formula.api as smf\n\nmodelo_stats = smf.logit(\n  'y ~ valor + juiz_tempo_vara',\n  data=X_train.join(y_train)\n).fit()\n\nmodelo_stats.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.588215\n         Iterations 6\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n7597\n\n\nModel:\nLogit\nDf Residuals:\n7594\n\n\nMethod:\nMLE\nDf Model:\n2\n\n\nDate:\nMon, 16 Sep 2024\nPseudo R-squ.:\n0.09115\n\n\nTime:\n22:17:24\nLog-Likelihood:\n-4468.7\n\n\nconverged:\nTrue\nLL-Null:\n-4916.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n2.333e-195\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.1606\n0.071\n16.267\n0.000\n1.021\n1.300\n\n\nvalor\n-3.039e-05\n5.38e-06\n-5.650\n0.000\n-4.09e-05\n-1.98e-05\n\n\njuiz_tempo_vara\n-0.2400\n0.009\n-26.696\n0.000\n-0.258\n-0.222\n\n\n\n\n\nVamos comparar os valores ajustados pelo modelo com os valores obtidos no scikit-learn:\n\n# extraindo pelo statsmodels\nmodelo_stats.params\n\nIntercept          1.160603\nvalor             -0.000030\njuiz_tempo_vara   -0.239957\ndtype: float64\n\n\n\n# extraindo pelo sklearn\nmodelo_ajustado.intercept_, modelo_ajustado.coef_\n\n(array([1.16048347]), array([[-3.03864461e-05, -2.39937825e-01]]))\n\n\nVeja que os valores batem, mas a forma de extrair os valores ajustados é diferente. No statsmodels, usamos o .params para obter os valores ajustados, enquanto na regressão logística do scikit-learn usamos o .intercept_ e o coef_.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos de classificação</span>"
    ]
  },
  {
    "objectID": "09-modelos-classificacao.html#métricas-de-qualidade-do-modelo",
    "href": "09-modelos-classificacao.html#métricas-de-qualidade-do-modelo",
    "title": "9  Modelos de classificação",
    "section": "9.3 Métricas de qualidade do modelo",
    "text": "9.3 Métricas de qualidade do modelo\nAgora, como podemos calcular a acurácia do modelo? A acurácia é a proporção de casos que o modelo acertou. Para calcular a acurácia, vamos usar a o método .score() da biblioteca scikit-learn. Esse método calcula a acurácia do modelo, ou seja, a proporção de casos que o modelo acertou. Vamos calcular a acurácia do modelo para a base de treino e para a base de teste.\n\n# na base de treino\nmodelo.score(X_train, y_train)\n\n0.6935632486507832\n\n\n\n# na base de teste\n\nmodelo_ajustado.score(X_test, y_test)\n\n0.7039084090011843\n\n\nVeja que as acurácias na base de treino e de teste são diferentes. No caso, a acurácia na base de teste está maior. Quando a acurácia na base de treino é bem maior que na base de teste, isso é sinal de que o modelo está com problemas de overfitting. Ou seja, o modelo está muito ajustado à base de treino, e não consegue generalizar para a base de teste, que é o que usamos para avaliar como o modelo se comportaria em novos casos.\n\n9.3.1 Como a acurácia é calculada?\nVamos relembrar nosso modelo:\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2)}} = g(\\beta_0 + \\beta_1x_1 + \\beta_2x_2)\n\\]\nA saída do modelo é um número entre zero e um, representando uma probabilidade. No entanto, nossa variável dependente ou é zero, ou é um. Para calcular a acurácia, portanto, precisamos transformar a saída do modelo em uma variável binária. Uma forma de fazer isso é considerar que, se a saída do modelo for maior que 0.5, a variável dependente é 1, e se for menor que 0.5, a variável dependente é 0. Isso é feito automaticamente pelo método .score() por trás dos panos.\nVamos ver isso na prática. Vamos calcular a acurácia manualmente, a partir da saída do modelo. Para isso, vamos usar o método .decision_function() da regressão logística, que nos dá a saída do modelo para cada observação. Vamos considerar que, se a saída do modelo for maior que 0.5, a variável dependente é 1, e se for menor que 0.5, a variável dependente é 0. Vamos comparar essa saída com a variável dependente real, e calcular a acurácia manualmente.\n\nprob_predito = modelo_ajustado.predict_proba(X_test)[:,1]\nvalor_predito = np.where(prob_predito &gt; 0.5, 1, 0)\n\nnp.mean(valor_predito == y_test)\n\n0.7039084090011843\n\n\n\n# lembrando do score calculado:\nmodelo.score(X_test, y_test)\n\n0.7039084090011843\n\n\n\n\n9.3.2 Só a acurácia interessa?\nDependendo do que estamos querendo prever, a acurácia pode não ser a melhor métrica. Por exemplo, se estamos interessados em prever casos raros, a acurácia pode ser uma métrica enganosa. Imagine que temos uma base de dados com 99% de casos de vitória e 1% de casos de derrota. Se um modelo chutar que todos os casos são de vitória, ele terá uma acurácia de 99%. No entanto, ele não está fazendo um bom trabalho em prever os casos de derrota, certo?\nPara lidar com isso, precisamos dar atenção aos erros e acertos do modelo, dependendo de qual é o valor da variável dependente. A ferramenta utilizada para visualizar todos esses erros e acertos é a matriz de confusão. A matriz de confusão é uma tabela que mostra os acertos e erros do modelo, separados por categoria da variável dependente.\nPara calcular a matriz de confusão, vamos usar a função confusion_matrix da biblioteca scikit-learn. Vamos calcular a matriz de confusão para a base de teste.\n\n# utilizando as previsoes\nConfusionMatrixDisplay.from_predictions(y_test, valor_predito)\n\n\n\n\n\n\n\n\nA mesma matriz pode ser obtida utilizando a função from_estimator(), evitando que tenhamos de fazer a previsão manualmente.\n\n# utilizando o modelo\nConfusionMatrixDisplay.from_estimator(modelo_ajustado, X_test, y_test)\n\n\n\n\n\n\n\n\nVamos explicar os quadrantes da matriz de confusão:\nNo eixo x, temos os valores preditos (0 para derrota, 1 para vitória). No eixo y, temos os valores reais (0 para derrota, 1 para vitória). Os quadrantes da matriz de confusão são:\n\nCanto superior esquerdo: verdadeiros negativos (VN). São os casos em que o modelo previu 0 e o valor real era 0.\nCanto superior direito: falsos positivos (FP). São os casos em que o modelo previu 1, mas o valor real era 0.\nCanto inferior esquerdo: falsos negativos (FN). São os casos em que o modelo previu 0, mas o valor real era 1.\nCanto inferior direito: verdadeiros positivos (VP). São os casos em que o modelo previu 1 e o valor real era 1.\n\nA tabela abaixo resume a matriz de confusão:\n\n\n\n\n\nPredito: Negativo (0)\n\n\nPredito: Positivo (1)\n\n\n\n\nVerdade: Negativo (0)\n\n\nVN (Verdadeiro Negativo)\n\n\nFP (Falso Positivo)\n\n\n\n\nVerdade: Positivo (1)\n\n\nFN (Falso Negativo)\n\n\nVP (Verdadeiro Positivo)\n\n\n\nIdealmente, gostaríamos que a matriz de confusão tivesse apenas valores na diagonal principal, ou seja, apenas verdadeiros positivos e verdadeiros negativos. No entanto, isso é raro de acontecer. O que queremos, então, é que o modelo tenha o menor número possível de falsos positivos e falsos negativos.\nNo nosso caso, o valor de VN é bem alto, então o modelo parece estar acertando bem os casos de derrota. No entanto, o valor de FN também é bem alto, então o modelo está errando bastante nos casos de vitória.\nJá os valores de FN e VP são bem baixos. Isso é um sinal de que o modelo está tendendo a prever tudo como derrota. Veja que ele decidiu marcar apenas 2 casos como vitória! Isso é um problema, e vamos corrigir em breve.\nPela terminologia, a acurácia é dada por\n\\[\n\\text{Acurácia} = \\frac{VN + VP}{VN + FP + FN + VP}\n\\]\nNo nosso caso, substituindo os valores:\n\\[\n\\text{Acurácia} = \\frac{1399 + 384}{1399 + 295 + 455 + 384} = 0.70\n\\]\nComo vimos, a acurácia sozinha não consegue capturar tudo o que desejaríamos para entender a qualidade do modelo. Então vamos ver outras métricas que podem ser úteis.\n\n\n9.3.3 Precision, recall e f1-score\nAlém da acurácia, temos outras métricas que podem ser úteis para avaliar a qualidade do modelo. Duas métricas muito utilizadas são a precision (precisão) e o recall (revocação).\nA precisão é a proporção de verdadeiros positivos em relação ao total de casos previstos como positivos. Pela terminologia da matriz de confusão:\n\\[\n\\text{Precisão} = \\frac{VP}{VP + FP}\n\\]\nNa tabela, podemos visualizar pela coluna de preditos positivos:\n\n\n\n\n\nPredito: Negativo (0)\n\n\nPredito: Positivo (1)\n\n\n\n\nVerdade: Negativo (0)\n\n\nVN (Verdadeiro Negativo)\n\n\nFP (Falso Positivo)\n\n\n\n\nVerdade: Positivo (1)\n\n\nFN (Falso Negativo)\n\n\nVP (Verdadeiro Positivo)\n\n\n\nNo nosso caso, o valor seria:\n\\[\n\\text{Precisão} = \\frac{384}{384 + 295} = 0.56\n\\]\nOu seja, nosso modelo tem precisão zero.\nA revocação é a proporção de verdadeiros positivos em relação ao total de casos reais positivos. Pela terminologia da matriz de confusão:\n\\[\n\\text{Revocação} = \\frac{VP}{VP + FN}\n\\]\nNa tabela, podemos visualizar pela linha de verdadeiros positivos:\n\n\n\n\n\nPredito: Negativo (0)\n\n\nPredito: Positivo (1)\n\n\n\n\nVerdade: Negativo (0)\n\n\nVN (Verdadeiro Negativo)\n\n\nFP (Falso Positivo)\n\n\n\n\nVerdade: Positivo (1)\n\n\nFN (Falso Negativo)\n\n\nVP (Verdadeiro Positivo)\n\n\n\nNo nosso caso, o valor seria:\n\\[\n\\text{Revocação} = \\frac{384}{384 + 455} = 0.46\n\\]\nFinalmente, temos a f1-score, que é a média harmônica entre precisão e recall. Por combinar precisão e recall, é útil quando temos classes desbalanceadas.\n\\[\n\\text{F1-score} = 2 \\times \\frac{\\text{Precisão} \\times \\text{Revocação}}{\\text{Precisão} + \\text{Revocação}}\n\\]\nPela tabela, podemos visualizar pela coluna de preditos positivos e pela linha de verdadeiros positivos:\n\n\n\n\n\nPredito: Negativo (0)\n\n\nPredito: Positivo (1)\n\n\n\n\nVerdade: Negativo (0)\n\n\nVN (Verdadeiro Negativo)\n\n\nFP (Falso Positivo)\n\n\n\n\nVerdade: Positivo (1)\n\n\nFN (Falso Negativo)\n\n\nVP (Verdadeiro Positivo)\n\n\n\nNo nosso caso, o F1-score seria:\n\\[\n\\text{F1-score} = 2 \\times \\frac{0.56 \\times 0.46}{0.56 + 0.46} = 0.51\n\\]\n\n\n9.3.4 Escolhendo o valor de corte\nOs valores que calculamos acima valem para o valor de corte de 0.5, ou seja, se a saída do modelo for maior que 0.5, a predição para variável dependente é 1, e se for menor que 0.5, a variável dependente é 0. No entanto, podemos variar o valor de corte para ver como as métricas se comportam. E isso afeta as métricas de precisão e recall.\nPor exemplo, se reduzirmos o valor de corte para 0.01, o modelo vai prever muito mais casos como vitória. Isso aumenta o número de verdadeiros positivos, mas também aumenta o número de falsos positivos. Ou seja, isso pode aumentar a revocação, mas diminui a precisão. Se, por outro lado, aumentarmos o valor de corte para 0.99, o modelo vai prever muito menos casos como vitória. Isso diminui o número de falsos positivos, mas também diminui o número de verdadeiros positivos. Ou seja, isso pode (ou não) aumentar a precisão, mas diminui a revocação.\nVamos ver como ficam as métricas para diferentes valores de corte. Para isso, vamos usar o método predict_proba() da regressão logística, que nos dá a probabilidade de cada observação ser 0 ou 1. Vamos variar o valor de corte de 0.01 a 0.99, e calcular as métricas para cada valor de corte.\n\ndef grafico_calibracao(modelo):\n  prob_predito = modelo.predict_proba(X_test)[:,1]\n\n  # Definindo intervalos de valores de corte\n  cortes = np.linspace(0, 1, 100)\n\n  # Listas para armazenar as métricas\n  precision_scores = []\n  recall_scores = []\n  accuracy_scores = []\n  f1_scores = []\n\n  # Calculando as métricas para diferentes valores de corte\n  for corte in cortes:\n    y_pred = (prob_predito &gt;= corte).astype(int)\n    precision_scores.append(precision_score(y_test, y_pred, zero_division=0))\n    recall_scores.append(recall_score(y_test, y_pred))\n    accuracy_scores.append(accuracy_score(y_test, y_pred))\n    f1_scores.append(f1_score(y_test, y_pred))\n\n  # Criando um DataFrame com os resultados\n  df = pd.DataFrame({\n    'Corte': cortes,\n    'Precision': precision_scores,\n    'Recall': recall_scores,\n    'Accuracy': accuracy_scores,\n    'F1-Score': f1_scores\n  })\n\n  df_melt = df.melt(id_vars='Corte', value_vars=['Precision', 'Recall', 'Accuracy', 'F1-Score'])\n\n  sns.lineplot(\n    data=df_melt,\n    x='Corte',\n    y='value',\n    hue='variable'\n  )\n\n  return(df)\n\ngrafico_calibracao(modelo_ajustado)\n\n\n\n\n\n\n\n\nCorte\nPrecision\nRecall\nAccuracy\nF1-Score\n\n\n\n\n0\n0.000000\n0.331228\n1.000000\n0.331228\n0.497628\n\n\n1\n0.010101\n0.331225\n0.998808\n0.331623\n0.497477\n\n\n2\n0.020202\n0.331616\n0.997616\n0.333202\n0.497770\n\n\n3\n0.030303\n0.331352\n0.996424\n0.332807\n0.497323\n\n\n4\n0.040404\n0.332803\n0.996424\n0.337150\n0.498956\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n0.959596\n0.000000\n0.000000\n0.668772\n0.000000\n\n\n96\n0.969697\n0.000000\n0.000000\n0.668772\n0.000000\n\n\n97\n0.979798\n0.000000\n0.000000\n0.668772\n0.000000\n\n\n98\n0.989899\n0.000000\n0.000000\n0.668772\n0.000000\n\n\n99\n1.000000\n0.000000\n0.000000\n0.668772\n0.000000\n\n\n\n\n100 rows × 5 columns\n\n\n\n\n\n\n\n\n\n\nNo gráfico, veja que existe uma faixa de valores entre 0.2 e 0.4 em que a precisão e a revocação são altas (e, portanto, o F1 também). A acurácia também acompanha essas curvas (apesar de isso não ser uma regra geral). Isso é um sinal de que, nessa faixa de valores, o modelo está conseguindo prever bem.\nO melhor valor de corte depende muito da aplicação. Se nosso interesse está em acertar os casos em que a Vivo perde, talvez seja melhor usar um valor de corte mais alto, para diminuir os falsos positivos. Se nosso interesse está em acertar os casos em que a Vivo ganha, talvez seja melhor usar um valor de corte mais baixo, para diminuir os falsos negativos. Se o interesse é ter um equilíbrio entre os dois, talvez seja melhor usar um valor de corte que maximiza o F1-score.\n\n\n9.3.5 Curva ROC\nO último critério que vamos ver é a curva ROC. A curva ROC é uma curva que mostra a relação entre a revocação e a taxa de falsos positivos. Vamos ver como ela é construída.\n\n\n# valores importantes da curva ROC\nfalse_positive, recall, cortes = roc_curve(y_test, prob_predito)\n\ndf_valores = pd.DataFrame({\n  'Falsos Positivos': false_positive,\n  'Recall': recall,\n  'Corte': cortes,\n  'Diferença': recall - false_positive\n})\n\nsns.lineplot(data = df_valores, x = 'Falsos Positivos', y = 'Recall')\nplt.xlabel('Taxa de Falsos Positivos')\nplt.ylabel('Taxa de Verdadeiros Positivos (Recall)')\n\nText(0, 0.5, 'Taxa de Verdadeiros Positivos (Recall)')\n\n\n\n\n\n\n\n\n\nIdealmente, gostaríamos que a curva ROC estivesse o mais próximo possível do canto superior esquerdo, ou seja, com revocação 1 e taxa de falsos positivos 0. Isso significaria que o modelo está acertando todos os casos positivos e errando nada dos casos negativos. No entanto, isso é quase impossível de acontecer. O que queremos, então, é que o valor de corte que faz com que a curva ROC esteja o mais próximo possível do canto superior esquerdo.\n\ndf_valores.sort_values('Diferença', ascending=False).head(10)\n\n\n\n\n\n\n\n\nFalsos Positivos\nRecall\nCorte\nDiferença\n\n\n\n\n226\n0.184770\n0.735399\n0.437420\n0.550630\n\n\n227\n0.187131\n0.735399\n0.436573\n0.548268\n\n\n225\n0.184770\n0.733015\n0.437463\n0.548246\n\n\n228\n0.188312\n0.735399\n0.436562\n0.547088\n\n\n229\n0.189492\n0.735399\n0.436462\n0.545907\n\n\n223\n0.183589\n0.728248\n0.437665\n0.544659\n\n\n221\n0.182409\n0.727056\n0.437701\n0.544648\n\n\n230\n0.191854\n0.735399\n0.436443\n0.543546\n\n\n224\n0.184770\n0.728248\n0.437534\n0.543478\n\n\n222\n0.183589\n0.727056\n0.437675\n0.543467\n\n\n\n\n\n\n\nO melhor corte é algo em torno de 0.34. Esse valor maximiza a revocação e minimiza a taxa de falsos positivos. Esse valor também está próximo do valor que maximiza o F1-score. Esses valores, no entanto, não precisam ser exatamente iguais. Dependendo da aplicação, podemos escolher um valor de corte que maximiza a revocação, a precisão, o F1-score, a acurácia ou qualquer outra métrica que seja importante.\nO mais interessante da curva ROC é que ela nos dá uma métrica que não depende do valor de corte. Esse valor é a área abaixo da curva, ou AUC. A AUC é uma métrica que varia de 0 a 1, e que nos dá uma ideia de quão bem o modelo está conseguindo separar as classes. Se o modelo tivesse taxa de falsos positivos zero e revocação 1 para algum corte, a AUC seria 1, porque seria a área abaixo de um quadrado unitário.\n\n# calculando AUC\n\nauc = roc_auc_score(y_test, prob_predito)\n\nauc\n\n0.7167226261656862\n\n\nNesse caso, a área abaixo da curva é 0.717. Isso significa que o modelo está conseguindo separar as classes razoavelmente bem. No entanto, ainda há espaço para melhorias. Para isso, vamos adicionar algumas variáveis categóricas no modelo.\n\n\n9.3.6 Adicionando variáveis categóricas\nVamos adicionar algumas variáveis categóricas no modelo. Para isso, precisamos transformar essas variáveis em variáveis dummy. Uma variável dummy é uma variável que assume apenas dois valores, 0 ou 1. Fazemos isso para que o modelo consiga interpretar a variável categórica, porque o modelo só sabe lidar com variáveis numéricas.\nPara transformar uma variável categórica em variáveis dummy, vamos usar a função pd.get_dummies() do pandas.\n\ndummies_comarca = pd.get_dummies(vivo_f['comarca']).astype(int)\n\ndummies_comarca.head(6)\n\n\n\n\n\n\n\n\nFernandopolis\nJales\nPalmeira D'oeste\nPresidente Prudente\nSanta Fe Do Sul\nSao Jose Do Rio Preto\n\n\n\n\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n1\n0\n0\n\n\n4\n0\n0\n0\n1\n0\n0\n\n\n5\n0\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\nNote que a variável comarca foi transformada em várias variáveis dummy, uma para cada categoria. Existe um detalhe, no entanto, que é importante levar em conta. Se temos \\(n\\) categorias, precisamos de \\(n-1\\) variáveis dummy. Isso porque a última categoria é redundante: se todas as outras variáveis dummy forem zero, a última variável dummy será 1. Isso é o que chamamos de dummy trap. Para evitar o dummy trap, vamos usar o argumento drop_first=True na função pd.get_dummies().\n\n# evitar dummy trap\n\ndummies_comarca = pd.get_dummies(vivo_f['comarca'], drop_first=True).astype(int)\n\ndummies_comarca.head(6)\n\n\n\n\n\n\n\n\nJales\nPalmeira D'oeste\nPresidente Prudente\nSanta Fe Do Sul\nSao Jose Do Rio Preto\n\n\n\n\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n\n\n3\n0\n0\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n0\n\n\n5\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\nAgora, podemos recriar nossas variáveis X e y (treino e teste), e ajustar o modelo de regressão logística com as novas variáveis.\n\nvivo_f_com_dummies = pd.concat([vivo_f, dummies_comarca], axis=1)\n\nX = vivo_f_com_dummies[['valor', 'juiz_tempo_vara'] + list(dummies_comarca.columns)]\ny = vivo_f_com_dummies['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nX_train.head(5)\n\n\n\n\n\n\n\n\nvalor\njuiz_tempo_vara\nJales\nPalmeira D'oeste\nPresidente Prudente\nSanta Fe Do Sul\nSao Jose Do Rio Preto\n\n\n\n\n1843\n13308.0\n8.563997\n1\n0\n0\n0\n0\n\n\n946\n8000.0\n2.852841\n0\n0\n0\n0\n0\n\n\n10292\n5000.0\n8.563997\n1\n0\n0\n0\n0\n\n\n5858\n1000.0\n12.416153\n0\n0\n0\n0\n1\n\n\n2692\n7288.0\n4.194387\n0\n1\n0\n0\n0\n\n\n\n\n\n\n\nAgora, ajustamos o modelo com as novas variáveis. Vamos ver como ficaram as métricas do modelo na base de teste.\n\n# ajusta o modelo logístico\nmodelo = LogisticRegression()\nmodelo_ajustado = modelo.fit(X_train, y_train)\n\nc:\\Users\\julio\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nNote que o scikit reclamou que o número de iterações atingiu o limite. Isso ocorreu por um problema numérico, que pode ser resolvido aumentando o número de iterações. Vamos fazer isso e ajustar o modelo novamente.\n\nmodelo = LogisticRegression(max_iter=1000)\nmodelo_ajustado = modelo.fit(X_train, y_train)\n\n# valores ajustados para o modelo\nmodelo_ajustado.coef_\n\narray([[-4.83166624e-05, -3.87288375e-02, -2.31050884e+00,\n         1.57771461e+00, -1.75101445e-01,  8.45865179e-01,\n        -3.27685073e-01]])\n\n\nAgora funcionou! Vamos obter agora todas as métricas de interesse.\n\nacuracia = modelo_ajustado.score(X_test, y_test)\n\nacuracia\n\n0.7868140544808527\n\n\nA acurácia na base de teste ficou em 0.787, que é 8% maior que o modelo anterior. Vale a pena, no entanto, olhar todas as outras métricas, usando a função que criamos anteriormente.\n\ngrafico_calibracao(modelo_ajustado)\n\n\n\n\n\n\n\n\nO desenho das curvas de precisão, revocação, acurácia e F1-score parece ter ficado mais suave. Agora, podemos ver que o valor de corte que maximiza a acurácia é claramente diferente o valor de corte que maximiza o F1-score.\n\n# valores importantes da curva ROC\nfalse_positive, recall, cortes = roc_curve(y_test, prob_predito)\n\nsns.lineplot(x = false_positive, y = recall)\nplt.xlabel('Taxa de Falsos Positivos')\nplt.ylabel('Taxa de Verdadeiros Positivos (Recall)')\n\nText(0, 0.5, 'Taxa de Verdadeiros Positivos (Recall)')\n\n\n\n\n\n\n\n\n\n\nauc = roc_auc_score(y_test, prob_predito)\n\nauc\n\n0.8358540906487598\n\n\nAgora chegamos em um AUC de 83.6%, o que é um valor bem melhor do que tínhamos antes.\nEm seguida, veremos ver se conseguimos melhorar ainda mais nossos resultados mudando o modelo para uma árvore de decisão.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos de classificação</span>"
    ]
  },
  {
    "objectID": "09-modelos-classificacao.html#modelos-baseados-em-árvores",
    "href": "09-modelos-classificacao.html#modelos-baseados-em-árvores",
    "title": "9  Modelos de classificação",
    "section": "9.4 Modelos baseados em árvores",
    "text": "9.4 Modelos baseados em árvores\nO modelo logístico é uma excelente ferramenta para construir modelos que ajudam a predizer o resultado de um processo. No entanto, ele tem algumas limitações. Uma delas é que ele assume que a relação entre as variáveis independentes e a variável dependente é linear. Isso pode ser um problema em casos em que a relação é não-linear. Outra limitação é que ele assume que as variáveis independentes não atuam de forma conjunta, ou seja, cada uma tem um efeito separado. Isso pode ser um problema quando as variáveis independentes interagem entre si.\nA árvore de decisão é um modelo que consegue lidar com essas situações. Trata-se de um modelo que divide a base de dados em subgrupos, de forma a maximizar a homogeneidade dentro de cada subgrupo. Ela cria regras de corte baseadas nos dados, de forma que a conclusão sobre a variável dependente seja o resultado de várias perguntas de sim/não.\nVamos ajustar um modelo de árvore de decisão para a base de dados da Vivo. Para isso, vamos usar a função DecisionTreeClassifier da biblioteca scikit-learn. Vamos considerar o modelo com as variáveis categóricas.\nObs: outra vantagem dos modelos com árvores é que não precisamos transformar as variáveis categóricas em variáveis dummy, porque a árvore pode criar regras de corte baseadas nas categorias. No entanto, infelizmente, o modelo de árvore do scikit-learn não sabe trabalhar com variáveis categóricas. Por isso, ainda precisamos transformar as variáveis categóricas em variáveis dummy.\nVeja como ficaria o desenho da regressão logística com uma variável, mas utilizando árvores de decisão:\n\nmodel = DecisionTreeClassifier(max_depth=2)\ngrafico_modelo(model)\n\n\n\n\n\n\n\n\n\n9.4.1 Exemplo da Vivo\nAqui, consideramos um parâmetro chamado max_depth, que controla a profundidade da árvore. Quanto maior o valor de max_depth, mais complexa a árvore. Com max_depth=3, a árvore ficará assim:\n\nmodelo_arvore = DecisionTreeClassifier(max_depth = 3)\n\nmodelo_arvore_ajustado = modelo_arvore.fit(X_train, y_train)\n\n\nfig = plt.figure(figsize=(20,6))\nplot_tree(modelo_arvore_ajustado, feature_names=X_train.columns, class_names=['Derrota', 'Vitória'], filled=True)\n\n[Text(0.5, 0.875, 'juiz_tempo_vara &lt;= 6.935\\ngini = 0.455\\nsamples = 7597\\nvalue = [4941, 2656]\\nclass = Derrota'),\n Text(0.25, 0.625, 'valor &lt;= 8229.7\\ngini = 0.477\\nsamples = 3546\\nvalue = [1395, 2151]\\nclass = Vitória'),\n Text(0.375, 0.75, 'True  '),\n Text(0.125, 0.375, 'juiz_tempo_vara &lt;= 3.485\\ngini = 0.432\\nsamples = 2843\\nvalue = [899, 1944]\\nclass = Vitória'),\n Text(0.0625, 0.125, 'gini = 0.5\\nsamples = 1118\\nvalue = [543, 575]\\nclass = Vitória'),\n Text(0.1875, 0.125, 'gini = 0.328\\nsamples = 1725\\nvalue = [356, 1369]\\nclass = Vitória'),\n Text(0.375, 0.375, 'Presidente Prudente &lt;= 0.5\\ngini = 0.416\\nsamples = 703\\nvalue = [496.0, 207.0]\\nclass = Derrota'),\n Text(0.3125, 0.125, 'gini = 0.367\\nsamples = 628\\nvalue = [476.0, 152.0]\\nclass = Derrota'),\n Text(0.4375, 0.125, 'gini = 0.391\\nsamples = 75\\nvalue = [20.0, 55.0]\\nclass = Vitória'),\n Text(0.75, 0.625, 'juiz_tempo_vara &lt;= 12.33\\ngini = 0.218\\nsamples = 4051\\nvalue = [3546, 505]\\nclass = Derrota'),\n Text(0.625, 0.75, '  False'),\n Text(0.625, 0.375, 'valor &lt;= 1676.17\\ngini = 0.141\\nsamples = 3524\\nvalue = [3256, 268]\\nclass = Derrota'),\n Text(0.5625, 0.125, 'gini = 0.334\\nsamples = 354\\nvalue = [279, 75]\\nclass = Derrota'),\n Text(0.6875, 0.125, 'gini = 0.114\\nsamples = 3170\\nvalue = [2977, 193]\\nclass = Derrota'),\n Text(0.875, 0.375, 'Presidente Prudente &lt;= 0.5\\ngini = 0.495\\nsamples = 527\\nvalue = [290, 237]\\nclass = Derrota'),\n Text(0.8125, 0.125, 'gini = 0.396\\nsamples = 312\\nvalue = [227.0, 85.0]\\nclass = Derrota'),\n Text(0.9375, 0.125, 'gini = 0.414\\nsamples = 215\\nvalue = [63, 152]\\nclass = Vitória')]\n\n\n\n\n\n\n\n\n\nA primeira regra é juiz_tempo_vara &lt;= 6.935. Se essa regra for verdadeira, a árvore vai para a esquerda. Se for falsa, a árvore vai para a direita. Se a árvore for para a esquerda, a próxima regra é valor &lt;= 8229.7. Se essa regra for verdadeira, a árvore vai para a esquerda. Se for falsa, a árvore vai para a direita. E assim por diante. No caso das variáveis categóricas, a regra é criada com o valor 0.5, para separar os valores 0 e 1 das dummies.\nPor exemplo:\n\nValor da causa: 10000\nTempo de vara do juiz: 5\nComarca: Presidente Prudente\n\nNessas regras, vamos andar primeiro para a esquerda, porque juiz_tempo_vara &lt;= 6.935 é verdadeiro. Em seguida, vamos para a direita, porque valor &lt;= 8229.7 é falso. E, por fim, vamos para a direita, porque Presidente Prudente &lt;= 0.5 é falso. Nesse caso, a previsão do modelo é “Vitória”.\nAgora vamos ver uma árvore com max_depth=4.\n\nmodelo_arvore = DecisionTreeClassifier(max_depth = 4)\n\nmodelo_arvore_ajustado = modelo_arvore.fit(X_train, y_train)\n\nfig = plt.figure(figsize=(20,7))\nplot_tree(modelo_arvore_ajustado, feature_names=X_train.columns, class_names=['Derrota', 'Vitória'], filled=True)\n\n[Text(0.5, 0.9, 'juiz_tempo_vara &lt;= 6.935\\ngini = 0.455\\nsamples = 7597\\nvalue = [4941, 2656]\\nclass = Derrota'),\n Text(0.25, 0.7, 'valor &lt;= 8229.7\\ngini = 0.477\\nsamples = 3546\\nvalue = [1395, 2151]\\nclass = Vitória'),\n Text(0.375, 0.8, 'True  '),\n Text(0.125, 0.5, 'juiz_tempo_vara &lt;= 3.485\\ngini = 0.432\\nsamples = 2843\\nvalue = [899, 1944]\\nclass = Vitória'),\n Text(0.0625, 0.3, 'valor &lt;= 3036.0\\ngini = 0.5\\nsamples = 1118\\nvalue = [543, 575]\\nclass = Vitória'),\n Text(0.03125, 0.1, 'gini = 0.201\\nsamples = 97\\nvalue = [86, 11]\\nclass = Derrota'),\n Text(0.09375, 0.1, 'gini = 0.495\\nsamples = 1021\\nvalue = [457, 564]\\nclass = Vitória'),\n Text(0.1875, 0.3, 'valor &lt;= 4177.22\\ngini = 0.328\\nsamples = 1725\\nvalue = [356, 1369]\\nclass = Vitória'),\n Text(0.15625, 0.1, 'gini = 0.397\\nsamples = 44\\nvalue = [32, 12]\\nclass = Derrota'),\n Text(0.21875, 0.1, 'gini = 0.311\\nsamples = 1681\\nvalue = [324, 1357]\\nclass = Vitória'),\n Text(0.375, 0.5, 'Presidente Prudente &lt;= 0.5\\ngini = 0.416\\nsamples = 703\\nvalue = [496.0, 207.0]\\nclass = Derrota'),\n Text(0.3125, 0.3, 'valor &lt;= 9264.94\\ngini = 0.367\\nsamples = 628\\nvalue = [476.0, 152.0]\\nclass = Derrota'),\n Text(0.28125, 0.1, 'gini = 0.079\\nsamples = 73\\nvalue = [70, 3]\\nclass = Derrota'),\n Text(0.34375, 0.1, 'gini = 0.393\\nsamples = 555\\nvalue = [406, 149]\\nclass = Derrota'),\n Text(0.4375, 0.3, 'valor &lt;= 17026.87\\ngini = 0.391\\nsamples = 75\\nvalue = [20.0, 55.0]\\nclass = Vitória'),\n Text(0.40625, 0.1, 'gini = 0.264\\nsamples = 64\\nvalue = [10, 54]\\nclass = Vitória'),\n Text(0.46875, 0.1, 'gini = 0.165\\nsamples = 11\\nvalue = [10, 1]\\nclass = Derrota'),\n Text(0.75, 0.7, 'juiz_tempo_vara &lt;= 12.33\\ngini = 0.218\\nsamples = 4051\\nvalue = [3546, 505]\\nclass = Derrota'),\n Text(0.625, 0.8, '  False'),\n Text(0.625, 0.5, 'valor &lt;= 1676.17\\ngini = 0.141\\nsamples = 3524\\nvalue = [3256, 268]\\nclass = Derrota'),\n Text(0.5625, 0.3, 'valor &lt;= 1170.69\\ngini = 0.334\\nsamples = 354\\nvalue = [279, 75]\\nclass = Derrota'),\n Text(0.53125, 0.1, 'gini = 0.325\\nsamples = 348\\nvalue = [277, 71]\\nclass = Derrota'),\n Text(0.59375, 0.1, 'gini = 0.444\\nsamples = 6\\nvalue = [2, 4]\\nclass = Vitória'),\n Text(0.6875, 0.3, 'Jales &lt;= 0.5\\ngini = 0.114\\nsamples = 3170\\nvalue = [2977, 193]\\nclass = Derrota'),\n Text(0.65625, 0.1, 'gini = 0.21\\nsamples = 663\\nvalue = [584, 79]\\nclass = Derrota'),\n Text(0.71875, 0.1, 'gini = 0.087\\nsamples = 2507\\nvalue = [2393, 114]\\nclass = Derrota'),\n Text(0.875, 0.5, 'Presidente Prudente &lt;= 0.5\\ngini = 0.495\\nsamples = 527\\nvalue = [290, 237]\\nclass = Derrota'),\n Text(0.8125, 0.3, 'valor &lt;= 9989.0\\ngini = 0.396\\nsamples = 312\\nvalue = [227.0, 85.0]\\nclass = Derrota'),\n Text(0.78125, 0.1, 'gini = 0.492\\nsamples = 94\\nvalue = [53, 41]\\nclass = Derrota'),\n Text(0.84375, 0.1, 'gini = 0.322\\nsamples = 218\\nvalue = [174, 44]\\nclass = Derrota'),\n Text(0.9375, 0.3, 'valor &lt;= 15755.6\\ngini = 0.414\\nsamples = 215\\nvalue = [63, 152]\\nclass = Vitória'),\n Text(0.90625, 0.1, 'gini = 0.354\\nsamples = 196\\nvalue = [45, 151]\\nclass = Vitória'),\n Text(0.96875, 0.1, 'gini = 0.1\\nsamples = 19\\nvalue = [18, 1]\\nclass = Derrota')]\n\n\n\n\n\n\n\n\n\nVeja que agora os ramos da árvore estão mais complexos, e as regras de classificação podem ser mais complicadas.\nO que você acha que acontece quando a árvore fica muito complexa?\n\n\n9.4.2 Overvitting em árvores\nO maior problema das árvores de classificação é que elas são muito suscetíveis ao overfitting. Isso acontece porque a árvore pode criar regras muito específicas para a base de treino, que não generalizam bem para a base de teste. Isso é um problema, porque o objetivo do modelo é prever bem a base de teste, e não a base de treino.\nVamos ver isso acontecendo na prática. Vamos ajustar modelos com diferentes valores de max_depth, e ver como eles se comportam na base de treino e na base de teste.\n\nacuracia_treino = []\nacuracia_teste = []\nprofundidades = range(1, 40)\nfor profundidade in profundidades:\n  modelo_arvore = DecisionTreeClassifier(max_depth = profundidade)\n  modelo_arvore_ajustado = modelo_arvore.fit(X_train, y_train)\n  acuracia_treino.append(modelo_arvore_ajustado.score(X_train, y_train))\n  acuracia_teste.append(modelo_arvore_ajustado.score(X_test, y_test))\n\ndf_arvores = pd.DataFrame({\n  'Profundidade': profundidades,\n  'Acurácia Treino': acuracia_treino,\n  'Acurácia Teste': acuracia_teste\n})\n\ndf_melt = df_arvores.melt(id_vars='Profundidade', value_vars=['Acurácia Treino', 'Acurácia Teste'])\n\nsns.lineplot(\n  data=df_melt,\n  x='Profundidade',\n  y='value',\n  hue='variable'\n)\n\n\n\n\n\n\n\n\n\ndf_arvores.sort_values('Acurácia Teste', ascending=False).head(5)\n\n\n\n\n\n\n\n\nProfundidade\nAcurácia Treino\nAcurácia Teste\n\n\n\n\n9\n10\n0.858760\n0.840111\n\n\n10\n11\n0.863104\n0.838926\n\n\n11\n12\n0.869159\n0.838926\n\n\n7\n8\n0.850467\n0.837742\n\n\n12\n13\n0.874161\n0.837347\n\n\n\n\n\n\n\nNote que a acurácia no treino ou fica igual ou aumenta à medida que aumentamos a complexidade da árvore. Mas a acurácia no teste começa a cair. Esse é o momento em que o modelo apresenta problemas de overfitting. Ou seja, ele está muito ajustado à base de treino, e não consegue generalizar para a base de teste. Nesse caso, parece que o melhor valor de max_depth é 10, que é o valor que maximiza a acurácia na base de teste.\nObs: Veja que a acurácia está bem melhor que o modelo linear logístico. O modelo de árvore nem sempre tem desempenho melhor do que o modelo logístico. Isso depende muito da base de dados. No entanto, o modelo de árvore é mais flexível, e pode ser uma boa alternativa quando a relação entre as variáveis é não-linear.\nAgora, vale notar que estamos, novamente, olhando apenas para a acurácia, e temos de olhar outras métricas para entender melhor o desempenho do modelo. Vamos ver a matriz de confusão para o modelo com max_depth=10.\nA árvore de decisão, no seu último nível, concluirá que a decisão é favorável ou desfavorável dependendo da frequência das categorias dentro do filtro aplicado. Se a maioria dos casos for favorável, a decisão será favorável, e vice-versa. A proporção de casos favoráveis, então, é a probabilidade de a decisão ser favorável estimada pelo modelo.\n\nmodelo_arvore = DecisionTreeClassifier(max_depth = 10)\nmodelo_arvore_ajustado = modelo_arvore.fit(X_train, y_train)\ndf_calibracao = grafico_calibracao(modelo_arvore_ajustado)\n\n\n\n\n\n\n\n\n\ndf_calibracao.sort_values('Accuracy', ascending=False).head(5)\n\n\n\n\n\n\n\n\nCorte\nPrecision\nRecall\nAccuracy\nF1-Score\n\n\n\n\n60\n0.606061\n0.804742\n0.687723\n0.841295\n0.741645\n\n\n62\n0.626263\n0.804196\n0.685340\n0.840505\n0.740026\n\n\n61\n0.616162\n0.804196\n0.685340\n0.840505\n0.740026\n\n\n47\n0.474747\n0.793758\n0.697259\n0.839716\n0.742386\n\n\n46\n0.464646\n0.793758\n0.697259\n0.839716\n0.742386\n\n\n\n\n\n\n\nVeja que, nesse caso, o melhor valor de corte está entre 0.4 e 0.6, então o corte de 0.5 já está bem próximo do ideal.\n\n\n9.4.3 Validação cruzada\nNo gráfico que compara a acurácia no treino e no teste para diferentes valores de max_depth, verificamos a acurácia do modelo várias vezes na base de teste. No entanto, o uso excessivo da base de teste pode levar a problemas de overfitting. Isso acontece porque o modelo pode acabar, de forma não intencional, se ajustando à base de teste. É como se esses dados estivessem sendo considerados no nosso treinamento, certo?\nIdealmente, no entanto, devemos utilizar apenas a base de treino para tudo, sendo a base de testes escondida e apenas acessada para avaliar a qualidade do modelo final. Para isso, podemos usar a validação cruzada. A validação cruzada é uma técnica que divide a base de treino em várias partes, e ajusta o modelo em cada uma dessas partes, criando mini-bases de teste que depois são descartadas. Isso permite que o modelo seja ajustado usando somente a base de treino, sem sujá-lo com a base de teste, mas ainda permitindo avaliar a qualidade do modelo e lidar com overvitting.\nA validação cruzada é uma técnica essencial para modelagem preditiva. É ela que nos permite utilizar modelos extremamente complexos, mas ainda controlando o overfitting.\n\n\n\nPara fazer a validação cruzada, portanto, partimos da base de treino.\nA técnica mais tradicional de validação cruzada, que é a que vamos utilizar aqui, é a validação cruzada \\(k\\)-fold. Nessa técnica, dividimos a base de treino em \\(k\\) partes (geralmente de forma aleatória), ajustamos o modelo em \\(k-1\\) partes, e avaliamos na parte restante. Isso é feito \\(k\\) vezes, de forma que cada parte é usada uma vez como base de teste. No final, tiramos a média das métricas de qualidade do modelo.\n\n\n\n\n\n9.4.4 Validação cruzada para árvores\nPara fazer a validação cruzada, precisamos de algumas ferramentas novas do scikit-learn. A primeira é a função cross_validate, que faz a validação cruzada para um modelo e uma base de dados. A segunda é a função GridSearchCV, que faz a validação cruzada para vários modelos e várias bases de dados, e nos ajuda a escolher o melhor modelo.\n\nmodelo_arvore = DecisionTreeClassifier(max_depth = 4)\n\nscores = cross_validate(modelo_arvore, X_train, y_train, cv=10)\n\nscores\n\n{'fit_time': array([0.00647974, 0.00885916, 0.00438976, 0.00717497, 0.00400519,\n        0.00399852, 0.00591993, 0.        , 0.03208375, 0.01122522]),\n 'score_time': array([0.00114012, 0.00099874, 0.        , 0.00199366, 0.00100279,\n        0.0010488 , 0.00202084, 0.        , 0.        , 0.00401521]),\n 'test_score': array([0.81184211, 0.84210526, 0.82105263, 0.81052632, 0.81710526,\n        0.81447368, 0.82236842, 0.83135705, 0.80105402, 0.81291173])}\n\n\nÉ possível alterar o critério de avaliação utilizando o parâmetro scoring. Exemplos:\n\n# F1-Score\nscores_f1 = cross_validate(modelo_arvore, X_train, y_train, cv=10, scoring='f1')\nprint(scores_f1)\n\n# AUC\nscores_auc = cross_validate(modelo_arvore, X_train, y_train, cv=10, scoring='roc_auc')\nprint(scores_auc)\n\n{'fit_time': array([0.        , 0.00599813, 0.00789595, 0.01025581, 0.00299811,\n       0.00308943, 0.00399899, 0.00655508, 0.00657821, 0.00133991]), 'score_time': array([0.01123714, 0.00499964, 0.00109887, 0.0030067 , 0.00299239,\n       0.00292087, 0.        , 0.        , 0.00299811, 0.00539613]), 'test_score': array([0.71905697, 0.75409836, 0.73745174, 0.71764706, 0.75134168,\n       0.71856287, 0.72505092, 0.76811594, 0.71669794, 0.72692308])}\n{'fit_time': array([0.0066762 , 0.00446558, 0.00524306, 0.0038445 , 0.00647974,\n       0.00466228, 0.00517797, 0.00356984, 0.00442767, 0.00377846]), 'score_time': array([0.        , 0.0083046 , 0.00404763, 0.00251532, 0.00345397,\n       0.00200653, 0.00185847, 0.00240993, 0.00274611, 0.00328207]), 'test_score': array([0.84815323, 0.89143025, 0.86869502, 0.8663511 , 0.87572677,\n       0.87541475, 0.86933427, 0.87905049, 0.83757543, 0.87652204])}\n\n\nGeralmente, o que fazemos é escolher vários métodos de scoring numa lista\n\ncriterios = ['f1', 'roc_auc', 'accuracy', 'precision', 'recall']\n\nresultados = cross_validate(modelo_arvore, X_train, y_train, cv=10, scoring=criterios)\n\ndf_resultados = pd.DataFrame(resultados)\n\ndf_resultados\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_f1\ntest_roc_auc\ntest_accuracy\ntest_precision\ntest_recall\n\n\n\n\n0\n0.005008\n0.011754\n0.719057\n0.848153\n0.811842\n0.750000\n0.690566\n\n\n1\n0.001734\n0.000000\n0.754098\n0.891430\n0.842105\n0.828829\n0.691729\n\n\n2\n0.013973\n0.001139\n0.737452\n0.868695\n0.821053\n0.757937\n0.718045\n\n\n3\n0.000000\n0.020305\n0.717647\n0.866351\n0.810526\n0.750000\n0.687970\n\n\n4\n0.002509\n0.010182\n0.751342\n0.875727\n0.817105\n0.716724\n0.789474\n\n\n5\n0.006395\n0.018292\n0.718563\n0.875415\n0.814474\n0.765957\n0.676692\n\n\n6\n0.004304\n0.010160\n0.725051\n0.869334\n0.822368\n0.791111\n0.669173\n\n\n7\n0.003873\n0.013084\n0.768116\n0.879050\n0.831357\n0.738676\n0.800000\n\n\n8\n0.002861\n0.015674\n0.716698\n0.837575\n0.801054\n0.712687\n0.720755\n\n\n9\n0.005521\n0.010748\n0.726923\n0.876522\n0.812912\n0.741176\n0.713208\n\n\n\n\n\n\n\nPara obter as estimativas de F1, AUC, acurácia, etc, podemos tomar a média dos resultados obtidos em cada iteração da validação cruzada.\n\ndf_resultados.mean()\n\nfit_time          0.004618\nscore_time        0.011134\ntest_f1           0.733495\ntest_roc_auc      0.868825\ntest_accuracy     0.818480\ntest_precision    0.755310\ntest_recall       0.715761\ndtype: float64\n\n\nIsso tudo foi feito para um valor específico de max_depth. O que queremos na prática, no entanto, é escolher o melhor valor de max_depth para o modelo. Para isso, vamos usar a função GridSearchCV, que faz a validação cruzada para vários valores de max_depth, e nos ajuda a escolher o melhor valor. Isso é chamado de grid search, ou busca em grade, porque criamos uma grade dos chamados hiperparâmetros do modelo (neste caso, o max_depth), e testamos todos os valores possíveis. A função GridSearchCV nos dá o melhor valor de max_depth e o melhor modelo.\n\nparametros = {\n  'max_depth': range(1, 20)\n}\n\nmodelo_arvore = DecisionTreeClassifier()\n\ngrid = GridSearchCV(modelo_arvore, parametros, scoring='f1', cv=10)\n\ngrid.fit(X_train, y_train)\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 20)}, scoring='f1')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 20)}, scoring='f1') best_estimator_: DecisionTreeClassifierDecisionTreeClassifier(max_depth=7)  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=7) \n\n\nVeja que, pelo grid search realizado e pela métrica escolhida (F1-Score), o melhor valor de max_depth é 7. Isso é um sinal de que o modelo de árvore de decisão com essa profundidade é o melhor modelo para a base de dados da Vivo.\nUma coisa que eu não contei é que existem vários outros parâmetros em uma árvore de decisão para controlar. Por exemplo, podemos controlar a quantidade mínima de observações em cada folha (o parâmetro min_samples_leaf), a quantidade mínima de observações em cada nó (o parâmetro min_samples_split), a quantidade máxima de variáveis a serem consideradas em cada nó (o parâmetro max_features), e muitos outros. O GridSearchCV nos permite testar todos esses parâmetros de uma vez, e escolher o melhor modelo. O único problema em fazer isso é que o número de modelos a serem testados cresce exponencialmente com o número de parâmetros. Por isso, é importante ter uma ideia do que cada parâmetro faz, e testar apenas os parâmetros mais importantes, ou então utilizar uma técnica de otimização mais sofisticada, como a otimização bayesiana (fora do escopo da nossa disciplina).\n\nparametros = {\n  'max_depth': range(5, 10),\n  'min_samples_split': range(2, 10),\n  'min_samples_leaf': range(5, 20)\n}\n\nmodelo_arvore = DecisionTreeClassifier()\n\n# obs: esse código pode demorar um pouco para rodar\n\ngrid = GridSearchCV(\n  modelo_arvore,      # modelo que queremos usar\n  parametros,         # parâmetros que queremos testar\n  scoring=criterios,  # métricas que queremos avaliar\n  refit='f1',    # métrica que queremos otimizar. Nesse caso, a AUC\n  cv=10,              # número de folds na validação cruzada\n  n_jobs=-1,          # número de processadores a serem usados, -1 significa todos\n  verbose=1           # exibir mensagens, quanto maior o número, mais mensagens\n)\n\ngrid.fit(X_train, y_train)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 7\n      1 parametros = {\n      2   'max_depth': range(5, 10),\n      3   'min_samples_split': range(2, 10),\n      4   'min_samples_leaf': range(5, 20)\n      5 }\n----&gt; 7 modelo_arvore = DecisionTreeClassifier()\n      9 # obs: esse código pode demorar um pouco para rodar\n     11 grid = GridSearchCV(\n     12   modelo_arvore,      # modelo que queremos usar\n     13   parametros,         # parâmetros que queremos testar\n   (...)\n     18   verbose=1           # exibir mensagens, quanto maior o número, mais mensagens\n     19 )\n\nNameError: name 'DecisionTreeClassifier' is not defined\n\n\n\nCom o grid search realizado, podemos ver que os valores que maximizam a AUC são max_depth=9, min_samples_split=2 e min_samples_leaf=11. Isso significa que o modelo de árvore de decisão com esses parâmetros é o melhor modelo para a base de dados da Vivo a partir do critério da área abaixo da curva ROC.\n\ngrid.best_params_, grid.best_score_\n\n({'criterion': 'gini',\n  'max_depth': 9,\n  'min_samples_leaf': 11,\n  'min_samples_split': 2},\n 0.8835200232556633)\n\n\nDepois de realizar essa validação cruzada, podemos ajustar o modelo com os melhores parâmetros e ver como ele se comporta na base de teste.\n\n# ajustando modelo com os melhores parâmetros\n\nmodelo_arvore_final = grid.best_estimator_\n\nmodelo_arvore_final.fit(X_train, y_train)\nmodelo_arvore_final.score(X_test, y_test)\n\n0.837741808132649\n\n\nVeja que a melhor acurácia ficou em torno de 84%, que é bem melhor que o modelo logístico, e também é melhor que o modelo de árvore com max_depth=4, que foi o melhor na verificação sem validação cruzada. Isso é um sinal de que o grid search foi bem-sucedido em encontrar o melhor modelo para a base de dados da Vivo.\nMas ainda pode ficar melhor! Agora, podemos otimizar a regra de corte para maximizar a acurácia.\n\ndf_cortes = grafico_calibracao(modelo_arvore_final)\n\n\n\n\n\n\n\n\n\ndf_cortes.sort_values('Accuracy', ascending=False).head(5)\n\n\n\n\n\n\n\n\nCorte\nPrecision\nRecall\nAccuracy\nF1-Score\n\n\n\n\n63\n0.636364\n0.830104\n0.663886\n0.843664\n0.737748\n\n\n62\n0.626263\n0.819103\n0.674613\n0.842874\n0.739869\n\n\n61\n0.616162\n0.819103\n0.674613\n0.842874\n0.739869\n\n\n60\n0.606061\n0.819103\n0.674613\n0.842874\n0.739869\n\n\n59\n0.595960\n0.810271\n0.676996\n0.840505\n0.737662\n\n\n\n\n\n\n\nFinalizamos o conteúdo sobre árvores de decisão. Agora, vamos ver um outro tipo de modelo mais poderoso que a árvore de decisão, que é o modelo de floresta aleatória.\n\n\n9.4.5 Florestas aleatórias\nA floresta aleatória é um dos algoritmos de machine learning mais famosos, sendo utilizado como “modelo base” para diversos problemas e competições como as do Kaggle.\nComo funciona: A floresta aleatória é uma coleção de árvores de decisão, que são ajustadas em subconjuntos aleatórios da base de dados. A previsão da floresta aleatória é a média das previsões de cada árvore.\nPor que funciona: A floresta aleatória é um modelo muito poderoso porque consegue capturar a complexidade dos dados sem sofrer tanto de overfitting. Isso acontece porque, ao fazer uma média dos resultados de cada árvore, a floresta aleatória consegue capturar a tendência dos dados sem se ajustar demais à base de treino.\nVejamos no exemplo visual como seria essa floresta.\n\nmodel = RandomForestClassifier()\ngrafico_modelo(model)\n\n\n\n\n\n\n\n\nVeja que a floresta aleatória é capaz de criar regras complexas, mas também não são regras tão agressivas quanto as da árvore de decisão. Ou seja, ela conseguiria se adaptar tanto à estrutura de um modelo logístico quanto à estrutura de uma árvore de decisão.\nVamos ver agora uma grid search para a floresta aleatória. Vamos testar os seguintes valores:\n\nn_estimators: o número de árvores na floresta\nmax_depth: a profundidade de cada árvore\n\nTecnicamente, seria possível testar muitos outros parâmetros, como min_samples_split, min_samples_leaf, max_features, etc. No entanto, isso pode ser muito custoso computacionalmente (considere que cada floresta aleatória pode estar rodando 100 árvores por baixo dos panos!)… Por isso, vamos testar apenas esses dois parâmetros.\n\nparametros = {\n  'n_estimators': [100, 200, 300],\n  'max_depth': range(6, 15),\n  'min_samples_split': range(2, 10),\n}\n\nmodelo_floresta = RandomForestClassifier()\n\n# obs: esse código pode demorar um pouco para rodar\n\ngrid = GridSearchCV(\n  modelo_floresta,      # modelo que queremos usar\n  parametros,         # parâmetros que queremos testar\n  scoring=criterios,  # métricas que queremos avaliar\n  refit='roc_auc',    # métrica que queremos otimizar. Nesse caso, a AUC\n  cv=10,              # número de folds na validação cruzada\n  n_jobs=-1,          # número de processadores a serem usados, -1 significa todos\n  verbose=1           # exibir mensagens, quanto maior o número, mais mensagens\n)\n\ngrid.fit(X_train, y_train)\n\nFitting 10 folds for each of 216 candidates, totalling 2160 fits\n\n\nGridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=-1,\n             param_grid={'max_depth': range(6, 15),\n                         'min_samples_split': range(2, 10),\n                         'n_estimators': [100, 200, 300]},\n             refit='roc_auc',\n             scoring=['f1', 'roc_auc', 'accuracy', 'precision', 'recall'],\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=-1,\n             param_grid={'max_depth': range(6, 15),\n                         'min_samples_split': range(2, 10),\n                         'n_estimators': [100, 200, 300]},\n             refit='roc_auc',\n             scoring=['f1', 'roc_auc', 'accuracy', 'precision', 'recall'],\n             verbose=1) best_estimator_: RandomForestClassifierRandomForestClassifier(max_depth=9, min_samples_split=9)  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(max_depth=9, min_samples_split=9) \n\n\nAgora, temos o melhor modelo de floresta aleatória para a base de dados da Vivo. Vamos ver os hiperparâmetros:\n\ngrid.best_params_, grid.best_score_\n\n({'max_depth': 9, 'min_samples_split': 9, 'n_estimators': 100},\n 0.8886485989750245)\n\n\nAgora, vamos ver como o modelo final (ajustado com todos os dados de treino) se comporta na base de teste.\n\n# ajustando modelo com os melhores parâmetros\n\nmodelo_floresta_final = grid.best_estimator_\n\nmodelo_floresta_final.fit(X_train, y_train)\nmodelo_floresta_final.score(X_test, y_test)\n\n0.8409001184366364\n\n\nAgora, finalmente, podemos calibrar o valor de corte para maximizar a acurácia.\n\ndf_cortes = grafico_calibracao(modelo_floresta_final)\n\n\n\n\n\n\n\n\n\ndf_cortes.sort_values('Accuracy', ascending=False).head(5)\n\n\n\n\n\n\n\n\nCorte\nPrecision\nRecall\nAccuracy\nF1-Score\n\n\n\n\n57\n0.575758\n0.815233\n0.688915\n0.845243\n0.746770\n\n\n56\n0.565657\n0.808926\n0.691299\n0.843664\n0.745501\n\n\n58\n0.585859\n0.815714\n0.680572\n0.843269\n0.742040\n\n\n59\n0.595960\n0.820700\n0.671037\n0.842479\n0.738361\n\n\n55\n0.555556\n0.804709\n0.692491\n0.842479\n0.744395",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos de classificação</span>"
    ]
  },
  {
    "objectID": "10-corporativo.html",
    "href": "10-corporativo.html",
    "title": "10  Aplicações corporativas",
    "section": "",
    "text": "10.1 Provisionamento\nVocê trabalha no departamento jurídico de uma grande empresa que vende eletrônicos. A empresa possui uma base de dados de 1000 processos encerrados e 100 casos ativos. A empresa quer saber quanto deve provisionar em cada um dos casos ativos. Para isso, ela te forneceu a base de dados com os processos encerrados e ativos.\nO provisionamento é uma prática contábil que visa alocar recursos financeiros para cobrir possíveis perdas futuras. Ela é comum em departamentos jurídicos de empresas, já que os litígios têm, por natureza, prazo e valor incertos.\nAs provisões, passivos contingentes e ativos contingentes são regulados pelo Pronunciamento Contábil número 25 do Comitê de Pronunciamentos Contábeis (CPC 25). O CPC25 é baseado no International Accounting Standard (IAS) número 37, produzido pelo International Accounting Standards Board (IASB).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Aplicações corporativas</span>"
    ]
  },
  {
    "objectID": "10-corporativo.html#provisionamento",
    "href": "10-corporativo.html#provisionamento",
    "title": "10  Aplicações corporativas",
    "section": "",
    "text": "10.1.1 Definições\nSegundo o CPC25, temos as definições abaixo:\n\nProvisão: passivo de prazo ou valor incerto.\nPassivo: obrigação presente da entidade, derivada de eventos passados, cuja liquidação se espera que resulte em saída de recursos da entidade capazes de gerar benefícios econômicos.\nPassivo contingente: duas possíveis definições:\n\nobrigação presente (possível) que resulta de eventos passados e cuja existência será confirmada apenas pela ocorrência ou não de um ou mais eventos futuros incertos que não estão totalmente sob controle da entidade.\numa obrigação presente que resulta de eventos passados, mas que não é reconhecida porque: i) não é provável que uma saída de recursos que incorporam benefícios econômicos seja exigida para liquidar a obrigação; ou ii) o valor da obrigação não pode ser mensurado com confiabilidade.\n\n\nEm sentido geral, todas as provisões são contingentes porque são incertas quanto ao seu prazo ou valor. Porém, nem todos os passivos contingentes são provisões. Para ser uma provisão, a obrigação deve ser presente e derivada de eventos passados.\nUma provisão deve ser reconhecida quando:\n\na entidade tem uma obrigação presente como resultado de um evento passado;\né provável que uma saída de recursos que incorporam benefícios econômicos seja exigida para liquidar a obrigação; e\no valor da obrigação pode ser mensurado com confiabilidade.\n\n\n\n10.1.2 Explicando as definições\nAs definições de provisão e passivo contingente são bastante técnicas. Vamos explicar com um exemplo.\nUm caso acaba de chegar no departamento jurídico. Trata-se de uma reclamação de um consumidor que comprou uma máquina de lavar da empresa e alega que o aparelho pegou fogo, sem causar danos às pessoas, mas causando danos a um móvel. O valor da reclamação é de R$ 15.000,00.\n\nfrom IPython.display import IFrame\nIFrame('assets/decisao.pdf', width=600, height=450)\n\n\n        \n        \n\n\nNo momento que o caso chega, a empresa tem uma obrigação possível (o processo judicial), que resulta de um evento passado (a venda da máquina de lavar). Porém, a empresa ainda não sabe se a reclamação é procedente. Portanto, estamos na situação de passivo contingente. Nas bases de dados jurídicas, geralmente esses casos são classificados como perda possível.\nSuponha que, após a realização de uma perícia e da análise cuidadosa da sua equipe de advocacia, a empresa conclui que o fogo do aparelho não foi causado por defeito de fabricação, mas sim por um erro do consumidor. Nesse caso, ainda temos um passivo contingente, mas a empresa pode optar por classificar esse caso como perda remota.\nSuponha, por outro lado, que a perícia e a análise da equipe de advocacia concluem que o fogo foi causado por defeito de fabricação. Nesse caso, é provável que a empresa tenha que pagar uma indenização e o valor da indenização pode ser mensurado com confiabilidade. Nesse caso, o caso pode ser classificado como perda provável e a empresa pode provisionar o valor da indenização.\nProvisionar significa alocar recursos financeiros para cobrir possíveis perdas futuras. Geralmente isso é feito guardando-se uma quantia em dinheiro em uma conta bancária específica para esse fim. O valor provisionado é contabilizado como um passivo no balanço da empresa. Por um lado, a provisão garante que a empresa tenha recursos para cobrir as perdas quando elas se manifestarem na realidade. Por outro lado, a provisão pode limitar a atuação da empresa, já que ela está reservando uma quantia para cobrir possíveis perdas.\nAlém de alocar os recursos financeiros, a empresa deve relatar a provisão em suas demonstrações financeiras. Ou seja, os relatórios financeiros da empresa devem informar que a empresa provisionou um determinado valor para cobrir possíveis perdas futuras. Isso pode afetar a imagem da empresa perante investidores, credores e outros interessados.\nPor isso, é importante que o provisionamento seja feito de forma precisa. Se a empresa provisionar muito, ela pode ter menos recursos disponíveis para investir em outras áreas e pode ter uma imagem de empresa com muitos problemas jurídicos. Se a empresa provisionar pouco, ela pode ter problemas de caixa no futuro.\nVale notar que isso é apenas um resumo de um tópico relativamente longo. Nosso objetivo é ver como a ciência de dados pode ajudar nesse processo. Se você quiser saber mais sobre o CPC25, recomendamos a leitura do pronunciamento completo e a consulta a profissionais de contabilidade.\nVale notar também que estamos falando apenas dos passivos contingentes. Existem também os ativos contingentes, que são direitos presentes que surgem de eventos passados e cuja existência será confirmada pela ocorrência ou não de um ou mais eventos futuros incertos que não estão totalmente sob controle da entidade. Por exemplo, uma empresa pode ter um ativo contingente se ela ganhar um processo judicial contra um concorrente. Nesse caso, a empresa terá um direito presente (o direito de receber uma indenização) que surgiu de um evento passado (a violação de uma patente) e cuja existência será confirmada pela decisão judicial. Porém, a empresa não pode reconhecer esse ativo na contabilidade até que a decisão judicial seja proferida. Como a dinâmica contábil dos ativos contingentes é diferente da dos passivos contingentes, não vamos abordá-los nesta apostila.\n\nfrom IPython.display import IFrame\nIFrame('assets/resultados_gpa.pdf', width=600, height=450)\n\n\n        \n        \n\n\n\n\n10.1.3 Como a ciência de dados pode ajudar?\nCada empresa possui uma política de provisionamento. Essa política está relacionada com os critérios utilizados para classificar casos como perda possível, perda remota e perda provável. A política de provisionamento é uma decisão estratégica da empresa e pode variar de acordo com o setor, o porte e a cultura da empresa.\nGeralmente, os processos judiciais começam como perda possível e, à medida que a empresa obtém mais informações, eles são reclassificados como perda remota ou perda provável. Por exemplo, se ocorre uma sentença de primeira instância desfavorável à empresa, o caso pode ser reclassificado como perda provável. Se a sentença é favorável à empresa, o caso pode ser reclassificado como perda remota. Essa classificação, na maioria dos casos, é feita manualmente por advogados, que analisam os documentos do processo, as decisões judiciais e as informações obtidas em audiências e perícias.\nÉ nessa parte que a ciência de dados pode ajudar. A ciência de dados pode ser utilizada para analisar as bases de dados jurídicas e criar modelos que produzem a probabilidade de perda de cada caso, permitindo sua classificação como perda possível, remota ou provável. Por ser baseada no histórico de processos judiciais, essa análise pode ser mais precisa e rápida do que a análise manual feita pelos advogados, economizando recursos e aumentando a transparência do processo de provisionamento.\nComo vimos anteriormente, no entanto, não é só o resultado do processo que importa para fins de provisionamento. A empresa também precisa saber o valor da indenização que ela terá que pagar se o caso for desfavorável. A ciência de dados também pode ser utilizada para predizer o valor da indenização para cada caso.\nPor último, também é importante saber predizer o tempo que o caso vai levar para ser encerrado. Isso é importante porque a empresa precisa provisionar recursos financeiros para cobrir as despesas com o caso enquanto ele estiver em andamento. No entanto, a empresa não precisa provisionar imediatamente todos os processos, já que alguns deles podem demorar muitos anos para serem encerrados. Os modelos de predição de tempo de encerramento de processos judiciais podem ajudar a empresa a decidir quais processos devem ser provisionados imediatamente e quais podem esperar.\nObs: O provisionamento considerando o tempo de encerramento do processo é uma prática comum em departamentos jurídicos de empresas. Porém, não é uma prática contábil. O provisionamento contábil é feito apenas com base na probabilidade de perda e no valor da indenização. O tempo de encerramento do processo é uma variável importante para a gestão do departamento jurídico, mas não é uma teoria consolidada na contabilidade.\n\n\n10.1.4 Exemplo prático\nAgora que vimos os conceitos, vamos ver um exemplo prático. Suponha que a empresa de eletrônicos quer saber quanto deve provisionar em cada um dos 100 casos ativos. Para isso, ela te forneceu a base de dados com os processos encerrados e ativos. A base de dados contém as seguintes variáveis:\n\nid: identificador do processo\nencerrado: indica se o processo está encerrado (1) ou ativo (0)\nperda: indica se o processo resultou em perda (1) ou vitória (0)\nvalor_final: valor da indenização paga no processo\ntempo: tempo de duração do processo até seu encerramento\nassunto: assunto do processo, que pode ser “consumidor”, “trabalhista” ou “tributário”, por exemplo\nvalor_causa: valor da causa do processo\ndata_distribuicao: data de início do processo\ncomarca: comarca onde o processo foi distribuído, que pode ser “São Paulo”, “São Bernardo do Campo” ou “Santo André”, por exemplo\ndecisao_desfavoravel: indica se houve uma decisão desfavorável à empresa em primeira instância (1) ou não (0)\n\nVamos simular uma base de dados com 1000 processos encerrados e 100 processos ativos usando o python. A base de dados simulada é a seguinte:\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\ndef random_date(start, end):\n  return start + timedelta(days=random.randint(0, int((end - start).days)))\n\nnp.random.seed(42)\nrandom.seed(42)\n\nn_encerrados = 1000\nn_ativos = 100\n\ndef gerar_valor_final(perda, valor_causa, assunto, comarca, encerrado):\n  if encerrado == 0:\n    return np.nan\n  if perda == 0:\n    return 0\n  base = valor_causa * np.random.uniform(0.8, 1.2)\n  if assunto == \"trabalhista\":\n    return base * np.random.uniform(0.8, 1.1)\n  elif assunto == \"tributário\":\n    return base * np.random.uniform(1.1, 1.5)\n  else:\n    return base * np.random.uniform(0.9, 1.3)\n\ndef gerar_tempo(assunto, comarca):\n  base_tempo = np.random.randint(300, 1500)  # Base aleatória de 300 a 1500 dias\n  if assunto == \"tributário\":\n    base_tempo *= 2\n  if comarca == \"São Paulo\":\n    base_tempo *= 0.7\n  return base_tempo\n\ndef gerar_prob_perda(valor_causa, assunto, comarca, decisao_desfavoravel, encerrado):\n  base_prob = 0.3 if assunto == \"tributário\" else 0.2\n  if decisao_desfavoravel == 1:\n    base_prob += 0.6\n  if valor_causa &gt; 50000:\n    base_prob += 0.4\n  prob_final = 1 if np.random.uniform(0, 1) &lt; base_prob else 0\n  if encerrado == 0:\n    prob_final = np.nan\n  return prob_final\n\nn_processos = 1100\nprocessos_simulados = pd.DataFrame({\n  'id': range(1, n_processos + 1),\n  'encerrado': [1] * 1000 + [0] * 100,  # 1000 encerrados, 100 ativos\n  'valor_causa': np.round(np.random.uniform(1000, 100000, size=n_processos), 2),\n  'assunto': np.random.choice(['consumidor', 'trabalhista', 'tributário'], size=n_processos),\n  'comarca': np.random.choice(['São Paulo', 'São Bernardo do Campo', 'Santo André'], size=n_processos),\n  'decisao_desfavoravel': np.random.choice([0, 1], size=n_processos, p=[0.6, 0.4]),  # 60% não desfavorável\n  'data_distribuicao': [random_date(datetime(2010, 1, 1), datetime(2023, 12, 31)) for _ in range(n_processos)]\n})\n\nprocessos_simulados['perda'] = processos_simulados.apply(\n  lambda x: gerar_prob_perda(x['valor_causa'], x['assunto'], x['comarca'], x['decisao_desfavoravel'], x['encerrado']), axis=1\n)\n\nprocessos_simulados['valor_final'] = processos_simulados.apply(\n  lambda x: gerar_valor_final(x['perda'], x['valor_causa'], x['assunto'], x['comarca'], x['encerrado']), axis=1\n)\n\nprocessos_simulados['tempo'] = processos_simulados.apply(\n  lambda x: gerar_tempo(x['assunto'], x['comarca']) if x['encerrado'] == 1 else np.nan, axis=1\n)\n\nprocessos_simulados['valor_final'] = processos_simulados.apply(\n  lambda x: min(x['valor_final'], x['valor_causa']), axis=1\n)\n\n# Criar a variável de tempo desde a distribuição para os processos ativos\nprocessos_simulados['tempo_desde_distribuicao'] = (datetime.now() - pd.to_datetime(processos_simulados['data_distribuicao'])).dt.days\n\n\nprocessos_simulados\n\n\n\n\n\n\n\n\nid\nencerrado\nvalor_causa\nassunto\ncomarca\ndecisao_desfavoravel\ndata_distribuicao\nperda\nvalor_final\ntempo\ntempo_desde_distribuicao\n\n\n\n\n0\n1\n1\n38079.47\nconsumidor\nSão Paulo\n1\n2012-07-01\n1.0\n31547.018995\n512.4\n4491\n\n\n1\n2\n1\n95120.72\ntrabalhista\nSanto André\n0\n2010-07-24\n0.0\n0.000000\n1476.0\n5199\n\n\n2\n3\n1\n73467.40\ntributário\nSão Bernardo do Campo\n1\n2016-03-03\n1.0\n73467.400000\n1400.0\n3150\n\n\n3\n4\n1\n60267.19\ntributário\nSão Bernardo do Campo\n0\n2015-06-30\n0.0\n0.000000\n712.0\n3397\n\n\n4\n5\n1\n16445.85\nconsumidor\nSão Paulo\n0\n2015-01-03\n0.0\n0.000000\n911.4\n3575\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1095\n1096\n0\n18476.28\ntributário\nSão Paulo\n1\n2015-09-12\nNaN\nNaN\nNaN\n3323\n\n\n1096\n1097\n0\n22828.13\nconsumidor\nSão Paulo\n0\n2020-04-22\nNaN\nNaN\nNaN\n1639\n\n\n1097\n1098\n0\n19457.39\ntrabalhista\nSanto André\n0\n2015-09-23\nNaN\nNaN\nNaN\n3312\n\n\n1098\n1099\n0\n78178.86\ntributário\nSão Paulo\n0\n2010-03-18\nNaN\nNaN\nNaN\n5327\n\n\n1099\n1100\n0\n35662.40\nconsumidor\nSanto André\n1\n2020-06-08\nNaN\nNaN\nNaN\n1592\n\n\n\n\n1100 rows × 11 columns\n\n\n\nAgora vamos preparar a base de dados para cada modelo, como fizemos anteriormente. A única diferença é que teremos, além do modelo de classificação de resultado, um modelo de regressão para prever o valor da indenização e um modelo de regressão para prever o tempo de encerramento do processo.\nModelo de classificação\n\n# criando dummies para variáveis assunto e comarca\n\ndf = pd.get_dummies(\n  processos_simulados,\n  columns=['assunto', 'comarca'],\n  drop_first=True,\n  dtype=int\n)\n\n# vamos criar os modelos de resultado e valor somente para os processos encerrados\ndf_encerrados = df[df['encerrado'] == 1].copy()\n# não precisaremos dessas colunas\ndf_encerrados = df_encerrados.drop(columns=['encerrado', 'data_distribuicao', 'tempo_desde_distribuicao', 'tempo'])\n\ndf_ativos = df[df['encerrado'] == 0].copy()\n\ndf_encerrados.head()\n\n\n\n\n\n\n\n\nid\nvalor_causa\ndecisao_desfavoravel\nperda\nvalor_final\nassunto_trabalhista\nassunto_tributário\ncomarca_São Bernardo do Campo\ncomarca_São Paulo\n\n\n\n\n0\n1\n38079.47\n1\n1.0\n31547.018995\n0\n0\n0\n1\n\n\n1\n2\n95120.72\n0\n0.0\n0.000000\n1\n0\n0\n0\n\n\n2\n3\n73467.40\n1\n1.0\n73467.400000\n0\n1\n1\n0\n\n\n3\n4\n60267.19\n0\n0.0\n0.000000\n0\n1\n1\n0\n\n\n4\n5\n16445.85\n0\n0.0\n0.000000\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n# para separar os dados em treino e teste\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n# regressão logística\nfrom sklearn.linear_model import LogisticRegression\n# árvore de decisão\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n# random forest\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n# matriz de confusão e métricas\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_curve, roc_auc_score, ConfusionMatrixDisplay\n\n\nX = df_encerrados.drop(columns=['perda', 'valor_final'])\nX_ativos = df_ativos.drop(columns=['encerrado', 'data_distribuicao', 'tempo_desde_distribuicao', 'perda', 'valor_final', 'tempo'])\n\ny_perda = df_encerrados['perda']\ny_valor = df_encerrados['valor_final']\n\n# é possível fazer a separação em treino e teste com todas nossas variáveis dependentes de interesse\nX_train, X_test, y_perda_train, y_perda_test, y_valor_train, y_valor_test = train_test_split(\n  X, y_perda, y_valor, test_size=0.3, random_state=42\n)\n\nX_train.head()\n\n\n\n\n\n\n\n\nid\nvalor_causa\ndecisao_desfavoravel\nassunto_trabalhista\nassunto_tributário\ncomarca_São Bernardo do Campo\ncomarca_São Paulo\n\n\n\n\n541\n542\n61011.29\n0\n1\n0\n1\n0\n\n\n440\n441\n9398.93\n1\n0\n0\n0\n0\n\n\n482\n483\n45999.57\n0\n0\n1\n1\n0\n\n\n422\n423\n10029.40\n1\n0\n0\n0\n0\n\n\n778\n779\n72283.62\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\ny_perda_train.head()\n\n541    1.0\n440    1.0\n482    1.0\n422    1.0\n778    1.0\nName: perda, dtype: float64\n\n\n\ny_valor_train.head()\n\n541    52845.697821\n440     9398.930000\n482    45999.570000\n422    10029.400000\n778    71648.641835\nName: valor_final, dtype: float64\n\n\n\nX_ativos.head()\n\n\n\n\n\n\n\n\nid\nvalor_causa\ndecisao_desfavoravel\nassunto_trabalhista\nassunto_tributário\ncomarca_São Bernardo do Campo\ncomarca_São Paulo\n\n\n\n\n1000\n1001\n19328.16\n0\n0\n0\n1\n0\n\n\n1001\n1002\n54648.19\n0\n0\n0\n0\n1\n\n\n1002\n1003\n87421.64\n1\n0\n0\n0\n0\n\n\n1003\n1004\n73490.26\n0\n1\n0\n0\n0\n\n\n1004\n1005\n80849.55\n1\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n# modelo de florestas aleatórias para perda e valor\n\n# modelo de perda\nrf_perda = RandomForestClassifier(random_state=42)\nrf_perda.fit(X_train, y_perda_train)\n\n# modelo de valor. Note que usamos RandomForestRegressor no lugar de RandomForestClassifier\nrf_valor = RandomForestRegressor(random_state=42)\nrf_valor.fit(X_train, y_valor_train)\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(random_state=42) \n\n\nNo mundo real, teríamos de trabalhar com a otimização dos hiperparâmetros do modelo. No entanto, como nosso objetivo é apenas ilustrar o processo, vamos usar os modelos de florestas aleatórias com os hiperparâmetros padrão.\n\nX_predito = X_ativos.copy()\n\nX_predito['perda'] = rf_perda.predict_proba(X_ativos)[:, 1]\nX_predito['valor_final'] = rf_valor.predict(X_ativos)\n\nX_predito\n\n\n\n\n\n\n\n\nid\nvalor_causa\ndecisao_desfavoravel\nassunto_trabalhista\nassunto_tributário\ncomarca_São Bernardo do Campo\ncomarca_São Paulo\nperda\nvalor_final\n\n\n\n\n1000\n1001\n19328.16\n0\n0\n0\n1\n0\n0.17\n6608.459252\n\n\n1001\n1002\n54648.19\n0\n0\n0\n0\n1\n0.64\n38553.643555\n\n\n1002\n1003\n87421.64\n1\n0\n0\n0\n0\n0.93\n84485.949135\n\n\n1003\n1004\n73490.26\n0\n1\n0\n0\n0\n0.22\n15237.886716\n\n\n1004\n1005\n80849.55\n1\n1\n0\n1\n0\n0.93\n72403.530666\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1095\n1096\n18476.28\n1\n0\n1\n0\n1\n0.91\n17256.654609\n\n\n1096\n1097\n22828.13\n0\n0\n0\n0\n1\n0.28\n7061.341277\n\n\n1097\n1098\n19457.39\n0\n1\n0\n0\n0\n0.32\n8331.907196\n\n\n1098\n1099\n78178.86\n0\n0\n1\n0\n1\n0.31\n33225.020546\n\n\n1099\n1100\n35662.40\n1\n0\n0\n0\n0\n0.82\n31761.527306\n\n\n\n\n100 rows × 9 columns\n\n\n\nCom isso, temos a probabilidade de perda esperada para cada processo, bem como o valor esperado da indenização para cada um deles. Podemos aplicar a política da empresa com base nesses insumos e tomar decisões. Por exemplo, a empresa pode decidir provisionar somente os casos que tiverem alguma decisão desfavorável e cuja probabilidade de perda é maior que 50%. O valor provisionado é a soma do valor final de todos os casos provisionados.\n\n# Um exemplo de como podemos usar o modelo para tomar decisões\n# Lembre-se, isso é apenas um exemplo! Tudo depende da política da empresa\n\n# Vamos considerar que a empresa provisiona somente nos casos com decisão desfavorável==1 e perda&gt;0.5\n\nX_provisionar = X_predito[(X_predito['perda'] &gt; 0.5) & (X_predito['decisao_desfavoravel'] == 1)]\n\nX_provisionar = X_provisionar.copy()[['id', 'perda', 'valor_final']]\n\nX_provisionar.head(15)\n\n\n\n\n\n\n\n\nid\nperda\nvalor_final\n\n\n\n\n1002\n1003\n0.93\n84485.949135\n\n\n1004\n1005\n0.93\n72403.530666\n\n\n1008\n1009\n0.73\n18765.443725\n\n\n1011\n1012\n0.96\n86351.066654\n\n\n1016\n1017\n0.60\n14336.943351\n\n\n1020\n1021\n0.93\n65636.248002\n\n\n1027\n1028\n0.95\n82979.440932\n\n\n1029\n1030\n0.95\n81366.649377\n\n\n1030\n1031\n0.90\n2672.454445\n\n\n1034\n1035\n0.87\n8325.816117\n\n\n1038\n1039\n0.87\n6308.679590\n\n\n1039\n1040\n0.92\n23570.829377\n\n\n1041\n1042\n0.87\n78797.329211\n\n\n1044\n1045\n0.93\n85439.301809\n\n\n1048\n1049\n0.92\n83953.100667\n\n\n\n\n\n\n\n\nvalor_provisionado = X_provisionar['valor_final'].sum()\n\nprint(valor_provisionado)\n\n1446024.484373209\n\n\n\n\n10.1.5 (Avançado) Modelo considerando o tempo de encerramento\nAgora vamos ver como o tempo pode nos ajudar a fazer as provisões. Vamos criar um modelo de regressão para prever o tempo de encerramento do processo. Vamos usar um modelo que não vimos antes chamado ‘modelo de sobrevivência’. Esse modelo é capaz de lidar com o tempo que já passou do processo e com o fato de que o processo ainda não foi encerrado. O modelo de sobrevivência é muito utilizado em estudos de medicina para prever o tempo de sobrevivência de pacientes com câncer, por exemplo.\nComo o tempo que já passou do processo é uma variável importante de ser considerada, a análise considera todos os processos na base, ativos e encerrados. Vamos ver como isso funciona na prática.\nNesse caso, utilizamos o modelo de regressão de Cox, que é um modelo de sobrevivência muito popular. Essa parte está fora do escopo da nossa disciplina, mas coloco aqui apenas para ilustrar como a ciência de dados pode ser aplicada a esse problema.\n\nfrom sksurv.linear_model import CoxPHSurvivalAnalysis\n\ndf_survival = df.copy()\ndf_survival['event'] = df_survival['encerrado'] == 1\ndf_survival['duration'] = df_survival.apply(\n  lambda row: row['tempo'] if row['encerrado'] == 1 else row['tempo_desde_distribuicao'], axis=1\n)\ndf_survival_ativos = df_survival[df_survival['encerrado'] == 0].copy()\n\nX_survival = df_survival.drop(columns=['encerrado', 'data_distribuicao', 'event', 'duration','valor_final', 'tempo', 'perda'])\nX_ativos_survival = df_survival_ativos.drop(columns=['encerrado', 'data_distribuicao', 'duration', 'event','valor_final', 'tempo', 'perda'])\ny_survival = df_survival[['event', 'duration']].to_records(index=False)\n\n\nX_survival\n\n\n\n\n\n\n\n\nid\nvalor_causa\ndecisao_desfavoravel\ntempo_desde_distribuicao\nassunto_trabalhista\nassunto_tributário\ncomarca_São Bernardo do Campo\ncomarca_São Paulo\n\n\n\n\n0\n1\n38079.47\n1\n4491\n0\n0\n0\n1\n\n\n1\n2\n95120.72\n0\n5199\n1\n0\n0\n0\n\n\n2\n3\n73467.40\n1\n3150\n0\n1\n1\n0\n\n\n3\n4\n60267.19\n0\n3397\n0\n1\n1\n0\n\n\n4\n5\n16445.85\n0\n3575\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1095\n1096\n18476.28\n1\n3323\n0\n1\n0\n1\n\n\n1096\n1097\n22828.13\n0\n1639\n0\n0\n0\n1\n\n\n1097\n1098\n19457.39\n0\n3312\n1\n0\n0\n0\n\n\n1098\n1099\n78178.86\n0\n5327\n0\n1\n0\n1\n\n\n1099\n1100\n35662.40\n1\n1592\n0\n0\n0\n0\n\n\n\n\n1100 rows × 8 columns\n\n\n\n\n# modelo de sobrevivência\n\ncoxph = CoxPHSurvivalAnalysis()\ncoxph.fit(X_survival, y_survival)\n\npred_survival = coxph.predict_survival_function(X_ativos_survival)\n\nDepois de obter as predições, precisamos estimar o tempo de encerramento do processo. Isso precisa levar em conta o tempo que já passou do processo. Para isso, vamos usar a função de sobrevivência do modelo de Cox. Com base nela, aplicamos algumas condições para considerar o tempo desde a distribuição na estimativa do tempo de encerramento.\n\n# Para cada caso ativo, calcular o tempo restante condicional ao tempo já decorrido\nestimated_times = []\nfor i, func in enumerate(pred_survival):\n  tempo_decorrido = df_survival_ativos.iloc[i]['tempo_desde_distribuicao']\n  # Encontrar o ponto da curva de sobrevivência onde estamos atualmente (tempo &gt;= tempo_decorrido)\n  mask = func.x &gt;= tempo_decorrido\n  if np.any(mask):  # Verifica se há dados para esse intervalo de tempo\n    cond_survival_prob = func.y[mask]  # Sobrevivência condicional a partir de tempo_decorrido\n    cond_time_points = func.x[mask]  # Tempos correspondentes\n    # Verifica se existe uma mediana de sobrevivência &lt;= 0.5\n    time_median_idx = np.where(cond_survival_prob &lt;= 0.5)[0]\n    if len(time_median_idx) &gt; 0:\n      # O tempo restante será o ponto mediano a partir de agora\n      time_remaining = cond_time_points[time_median_idx[0]] - tempo_decorrido\n    else:\n      # Caso não tenha um ponto onde a sobrevivência seja &lt;= 0.5, consideramos o último tempo disponível\n      time_remaining = cond_time_points[-1] - tempo_decorrido\n  else:\n      time_remaining = 0  # Caso o tempo decorrido seja maior que o maior tempo predito pela curva de sobrevivência\n  estimated_times.append(time_remaining)\n\nX_ativos_survival['tempo_restante'] = estimated_times\n\nX_ativos_survival[['id', 'tempo_desde_distribuicao', 'tempo_restante']]\n\n\n\n\n\n\n\n\nid\ntempo_desde_distribuicao\ntempo_restante\n\n\n\n\n1000\n1001\n3086\n0.0\n\n\n1001\n1002\n2958\n0.0\n\n\n1002\n1003\n595\n651.0\n\n\n1003\n1004\n653\n585.0\n\n\n1004\n1005\n1392\n0.0\n\n\n...\n...\n...\n...\n\n\n1095\n1096\n3323\n0.0\n\n\n1096\n1097\n1639\n0.0\n\n\n1097\n1098\n3312\n0.0\n\n\n1098\n1099\n5327\n0.0\n\n\n1099\n1100\n1592\n0.0\n\n\n\n\n100 rows × 3 columns\n\n\n\nNote que, em alguns casos, o tempo estimado foi zero. Isso aconteceu porque o tempo desde a distribuição é maior do que o tempo estimado de encerramento. Nesses casos, consideramos o tempo desde a distribuição como o tempo estimado de encerramento.\nAgora, podemos juntar as informações de probabilidade de perda, valor da indenização e tempo de encerramento do processo para tomar decisões de provisionamento.\n\nX_provisionar_tempo = X_provisionar.merge(X_ativos_survival[['id', 'tempo_restante']], on='id')\n\nX_provisionar_tempo\n\n\n\n\n\n\n\n\nid\nperda\nvalor_final\ntempo_restante\n\n\n\n\n0\n1003\n0.93\n84485.949135\n651.0\n\n\n1\n1005\n0.93\n72403.530666\n0.0\n\n\n2\n1009\n0.73\n18765.443725\n0.0\n\n\n3\n1012\n0.96\n86351.066654\n385.0\n\n\n4\n1017\n0.60\n14336.943351\n0.0\n\n\n5\n1021\n0.93\n65636.248002\n0.0\n\n\n6\n1028\n0.95\n82979.440932\n0.0\n\n\n7\n1030\n0.95\n81366.649377\n0.0\n\n\n8\n1031\n0.90\n2672.454445\n0.0\n\n\n9\n1035\n0.87\n8325.816117\n0.0\n\n\n10\n1039\n0.87\n6308.679590\n0.0\n\n\n11\n1040\n0.92\n23570.829377\n0.0\n\n\n12\n1042\n0.87\n78797.329211\n0.0\n\n\n13\n1045\n0.93\n85439.301809\n0.0\n\n\n14\n1049\n0.92\n83953.100667\n489.0\n\n\n15\n1050\n0.84\n2718.351957\n0.0\n\n\n16\n1051\n0.91\n86124.536815\n0.0\n\n\n17\n1052\n0.96\n53316.948681\n1552.0\n\n\n18\n1058\n0.64\n36376.152361\n552.0\n\n\n19\n1063\n0.89\n77536.403123\n0.0\n\n\n20\n1066\n0.93\n72570.340488\n0.0\n\n\n21\n1071\n0.82\n24704.700949\n0.0\n\n\n22\n1072\n0.94\n14100.567213\n0.0\n\n\n23\n1074\n0.77\n2690.415733\n0.0\n\n\n24\n1080\n0.89\n87280.525331\n183.0\n\n\n25\n1081\n0.95\n78355.151460\n642.0\n\n\n26\n1088\n0.97\n15154.164214\n0.0\n\n\n27\n1094\n0.91\n50685.261076\n0.0\n\n\n28\n1096\n0.91\n17256.654609\n0.0\n\n\n29\n1100\n0.82\n31761.527306\n0.0\n\n\n\n\n\n\n\nAgora, podemos considerar um valor acumulado da provisão ao longo do tempo, considerando o tempo restante. Isso nos dá uma ideia de quanto a empresa deve provisionar no tempo para cobrir as perdas futuras.\n\nX_provisionar_tempo = X_provisionar_tempo.sort_values('tempo_restante', ascending=True)\n\n# pegando o valor acumulado e dividindo por 1000 para facilitar a visualização\nX_provisionar_tempo['valor_acu'] = X_provisionar_tempo['valor_final'].cumsum() / 1000\n\nX_provisionar_tempo\n\n\n\n\n\n\n\n\nid\nperda\nvalor_final\ntempo_restante\nvalor_acu\n\n\n\n\n1\n1005\n0.93\n72403.530666\n0.0\n72.403531\n\n\n2\n1009\n0.73\n18765.443725\n0.0\n91.168974\n\n\n5\n1021\n0.93\n65636.248002\n0.0\n156.805222\n\n\n4\n1017\n0.60\n14336.943351\n0.0\n171.142166\n\n\n6\n1028\n0.95\n82979.440932\n0.0\n254.121607\n\n\n7\n1030\n0.95\n81366.649377\n0.0\n335.488256\n\n\n9\n1035\n0.87\n8325.816117\n0.0\n343.814072\n\n\n8\n1031\n0.90\n2672.454445\n0.0\n346.486527\n\n\n12\n1042\n0.87\n78797.329211\n0.0\n425.283856\n\n\n13\n1045\n0.93\n85439.301809\n0.0\n510.723158\n\n\n10\n1039\n0.87\n6308.679590\n0.0\n517.031837\n\n\n11\n1040\n0.92\n23570.829377\n0.0\n540.602667\n\n\n15\n1050\n0.84\n2718.351957\n0.0\n543.321019\n\n\n21\n1071\n0.82\n24704.700949\n0.0\n568.025720\n\n\n19\n1063\n0.89\n77536.403123\n0.0\n645.562123\n\n\n16\n1051\n0.91\n86124.536815\n0.0\n731.686659\n\n\n29\n1100\n0.82\n31761.527306\n0.0\n763.448187\n\n\n28\n1096\n0.91\n17256.654609\n0.0\n780.704841\n\n\n26\n1088\n0.97\n15154.164214\n0.0\n795.859006\n\n\n20\n1066\n0.93\n72570.340488\n0.0\n868.429346\n\n\n22\n1072\n0.94\n14100.567213\n0.0\n882.529913\n\n\n23\n1074\n0.77\n2690.415733\n0.0\n885.220329\n\n\n27\n1094\n0.91\n50685.261076\n0.0\n935.905590\n\n\n24\n1080\n0.89\n87280.525331\n183.0\n1023.186115\n\n\n3\n1012\n0.96\n86351.066654\n385.0\n1109.537182\n\n\n14\n1049\n0.92\n83953.100667\n489.0\n1193.490283\n\n\n18\n1058\n0.64\n36376.152361\n552.0\n1229.866435\n\n\n25\n1081\n0.95\n78355.151460\n642.0\n1308.221587\n\n\n0\n1003\n0.93\n84485.949135\n651.0\n1392.707536\n\n\n17\n1052\n0.96\n53316.948681\n1552.0\n1446.024484\n\n\n\n\n\n\n\nColocando isso em um gráfico, temos:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.lineplot(\n  data=X_provisionar_tempo,\n  x='tempo_restante',\n  y='valor_acu',\n  drawstyle='steps-post',\n  errorbar=('ci', False)\n)\nfor i in range(1, 6):\n  plt.axvline(i * 365.25, color='lightgray', linestyle='--')\nplt.xlabel('Tempo restante (dias)')\nplt.ylabel('Valor acumulado ($ mil)')\n\nText(0, 0.5, 'Valor acumulado ($ mil)')\n\n\n\n\n\n\n\n\n\nConsiderando esse gráfico, a empresa pode considerar provisionar um valor acumulado de cerca de 1 milhão para cobrir as perdas do próximo exercício, e incluir uma provisão adicional para cobrir os processos com tempo de encerramento mais longo, classificando esses casos como perda possível para fins contábeis, indicando que não é possível fazer estimativa confiável nos casos muito longos.\nVale enfatizar, novamente, que esse tipo de estratégia não é algo praticado por todas as empresas. Cada empresa tem sua própria política de provisionamento e deve seguir as normas contábeis vigentes.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Aplicações corporativas</span>"
    ]
  },
  {
    "objectID": "10-corporativo.html#estratégia-de-acordo",
    "href": "10-corporativo.html#estratégia-de-acordo",
    "title": "10  Aplicações corporativas",
    "section": "10.2 Estratégia de acordo",
    "text": "10.2 Estratégia de acordo\nÉ comum tentar realizar acordos nos casos em que a empresa acredita que a probabilidade de perda é alta. Isso pode ser feito para evitar custos com o processo, para evitar a exposição da empresa ou para evitar a perda de tempo e recursos com um processo que a empresa acredita que vai perder. Sem considerar, é claro, os ganhos sociais, já que a parte contrária também pode ter interesse em um acordo e o acordo economiza tempo e recursos do judiciário.\nNo entanto, realizar acordos nem sempre é tão simples. Quando fazemos acordo, estamos antecipando um custo, que só seria realizado no futuro se o processo fosse desfavorável. Claro que o sistema corrige essa antecipação através de taxas de atualização (o valor de indenização geralmente sofre atualizações monetárias até o pagamento efetivo), mas ainda assim é um custo que a empresa está antecipando. Pode ser simplesmente que a empresa não tenha caixa suficiente para fazer acordos em todos os casos que ela gostaria de fazer.\nOutro aspecto importante a considerar ao fazer acordos é a probabilidade de sucesso do acordo. Em certos casos, pode ser que a parte contrária considere o valor do acordo muito baixo e decida levar o caso até o fim. Nesses casos, a empresa terá gastos extras com a tentativa de acordo e ainda terá que arcar com os custos do processo.\nA ciência de dados pode ser utilizada para ajudar a empresa a decidir quais casos devem ser alvo de acordo e qual o valor do acordo. A ideia é criar um modelo que consiga predizer a probabilidade de sucesso do acordo para cada valor ofertado. Com base nessas previsões e no orçamento da empresa, é possível decidir quais casos devem ser alvo de acordo e qual o valor ofertado em cada caso.\n\n10.2.1 Definições\nUma variável importante a ser considerada em uma estratégia de acordo é a alçada. A alçada geralmente é a proporção do valor pedido que a empresa está disposta a pagar para fazer um acordo. Por exemplo, se a empresa está disposta a pagar até 50% do valor pedido para fazer um acordo, a alçada é de 50%. Se a empresa está disposta a pagar até 80% do valor pedido para fazer um acordo, a alçada é de 80%. O valor de alçada é a multiplicação do valor pedido pelo valor de alçada. Por exemplo, se o valor pedido é de R$ 100.000,00 e a alçada é de 50%, o valor de alçada é de R$ 50.000,00.\nQuanto maior a alçada, mais agressiva é a estratégia de acordo. Por um lado, uma alçada alta aumenta a probabilidade de sucesso do acordo, já que a empresa está disposta a pagar um valor maior. Por outro lado, uma alçada alta aumenta o custo do acordo, já que a empresa está pagando um valor maior para fazer o acordo.\nIdealmente, cada processo teria uma alçada diferente, otimizada para maximizar a economia da empresa. No entanto, isso pode ser inviável na prática, já que as empresas geralmente não têm informações sobre tentativas frustradas de acordo, que seriam ideais para calibrar a alçada. Por isso, é comum que as empresas adotem uma alçada fixa para todos os processos, ou mude somente de acordo com algumas variáveis predefinidas, como tipo de processo (consumidor, trabalhista, tributário, etc.) e separação entre casos de massa e casos estratégicos.\nOutra variável que vamos considerar é o orçamento. O orçamento é o valor total que a empresa está disposta a gastar com acordos em um determinado período. Isso geralmente é uma decisão estratégica da empresa e pode variar de acordo com a situação atual da empresa. Uma prática comum no mercado é considerar sobras de provisão de exercícios anteriores como orçamento para acordos no exercício seguinte. Por exemplo, se a empresa provisionou R$ 1.000.000,00 para cobrir perdas futuras e gastou somente R$ 800.000,00 em acordos no exercício anterior, ela pode considerar os R$ 200.000,00 restantes como orçamento para acordos no exercício seguinte.\nFinalmente, também é importante considerar um número máximo de tentativas por mês. Isso é importante porque, em uma empresa com muitos processos, pode ser que a empresa não tenha recursos humanos suficientes para fazer acordos em todos os processos. Nesse caso, é importante priorizar os processos mais importantes e deixar os menos importantes para o mês seguinte.\n\n\n10.2.2 Modelo de simulação\nUma forma de estudar estratégias de acordo é simulando cenários. Por exemplo, podemos estudar os gastos ao longo do tempo em três cenários: 1) sem estratégia de acordo, 2) com estratégia de acordo branda e 3) com estratégia de acordo agressiva. O esperado é que a estratégia de acordo agressiva resulte em menos gastos com processos judiciais, já que a empresa está fazendo mais acordos. Por outro lado, a estratégia de acordo agressiva pode resultar em uma antecipação de gastos maiores com acordos, já que a empresa traria mais processos a valor presente.\nPara fazer a simulação precisamos, além da alçada e do orçamento, uma base de dados com os processos ativos, acompanhada de estimativas de probabilidade de sucesso do acordo em função da alçada, da probabilidade de derrota do processo (para fins de ordenação), e do tempo esperado para encerramento do processo.\nA função da probabilidade acordo de acordo com a alçada pode ser estimada com base nos dados de acordos passados da empresa através de um modelo de regressão logística, ou então pode ser definida manualmente pela empresa, caso não existam dados suficientes. Já o tempo esperado para encerramento do processo pode ser estimado com um modelo de regressão de sobrevivência, como vimos anteriormente.\nVamos simular uma base de dados com 100 processos ativos e criar um modelo de regressão para prever a probabilidade de sucesso do acordo em função da alçada e do tempo esperado para encerramento do processo. Para isso, vamos considerar a seguinte função de probabilidade de acordo:\n\\[\n\\log \\left( \\frac{P(\\text{acordo})}{1 - P(\\text{acordo})} \\right) = -10 + 15 \\cdot \\text{alçada}\n\\]\nou, equivalentemente,\n\\[\nP(\\text{acordo}) = \\frac{1}{1 + e^{10 - 15 \\cdot \\text{alçada} }}\n\\]\nOs valores de 10 (intercepto) e 20 (coeficiente angular) foram escolhidos de forma arbitrária. Na prática, esse valor seria estimado com base nos dados de acordos passados da empresa.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef p_acordo(alcada, alpha = 15):\n  return (1/(1+np.exp(10 - alcada * alpha)))\n\nalcadas = np.linspace(0, 1, 100)\nprob_acordo = p_acordo(alcadas)\n\nplt.plot(alcadas, prob_acordo)\n\n\n\n\n\n\n\n\nVamos considerar esses três cenários:\n\nSem estratégia de acordo: a empresa não faz acordo em nenhum processo.\nEstratégia de acordo branda: a empresa faz acordo usando a alçada de 50% em todos os processos.\nEstratégia de acordo agressiva: a empresa faz acordo usando a alçada de 80% em todos os processos.\n\n\nprint(p_acordo(0.5), p_acordo(0.8))\n\n0.07585818002124355 0.8807970779778823\n\n\nIsso nos leva ao seguinte código:\n\nimport numpy as np\nimport pandas as pd\n\n# Configurações para a simulação\nnp.random.seed(42)\nn_processos = 100\nvalores_causa = np.random.uniform(5000, 100000, n_processos)\nprob_derrota = np.random.uniform(0.1, 0.9, n_processos)\ntempos_esperados = np.random.uniform(1, 5, n_processos)\n\ndef p_acordo(alcada, alpha = 15):\n  return (1/(1+np.exp(10 - alcada * alpha)))\n\ndef calcular_valor_alcada(valor_causa, alcada):\n    return valor_causa * alcada\n\nalcada_branda = 0.50\nalcada_agressiva = 0.80\n\nvalores_alcada_branda = calcular_valor_alcada(valores_causa, alcada_branda)\nvalores_alcada_agressiva = calcular_valor_alcada(valores_causa, alcada_agressiva)\n\n# Criação do dataframe\ndf = pd.DataFrame({\n    'id': np.arange(1, n_processos + 1),\n    'valor': valores_causa,\n    'valor_alcada_branda': valores_alcada_branda,\n    'valor_alcada_agressiva': valores_alcada_agressiva,\n    'prob_derrota': prob_derrota,\n    'tempo': tempos_esperados\n})\n\n\ndf\n\n\n\n\n\n\n\n\nid\nvalor\nvalor_alcada_branda\nvalor_alcada_agressiva\nprob_derrota\ntempo\n\n\n\n\n0\n1\n40581.311290\n20290.655645\n32465.049032\n0.125143\n3.568127\n\n\n1\n2\n95317.859109\n47658.929554\n76254.287287\n0.609128\n1.336560\n\n\n2\n3\n74539.424472\n37269.712236\n59631.539578\n0.351485\n1.646515\n\n\n3\n4\n61872.555999\n30936.277999\n49498.044799\n0.506857\n4.594217\n\n\n4\n5\n19821.770842\n9910.885421\n15857.416674\n0.826053\n3.425716\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n96\n51910.581655\n25955.290827\n41528.465324\n0.379368\n3.088973\n\n\n96\n97\n54659.618791\n27329.809396\n43727.695033\n0.680765\n4.079974\n\n\n97\n98\n45616.396744\n22808.198372\n36493.117395\n0.817688\n1.863284\n\n\n98\n99\n7414.817041\n3707.408520\n5931.853633\n0.809669\n3.491562\n\n\n99\n100\n15249.685564\n7624.842782\n12199.748451\n0.723900\n1.341390\n\n\n\n\n100 rows × 6 columns\n\n\n\nDigamos que a empresa adote a seguinte estratégia: vamos tentar fazer acordo em todos os processos com probabilidade de derrota maior do que 50%, utilizando uma alçada, até que nosso orçamento seja atingido. O orçamento considerado é de 2 milhões e o limite de tentativas de acordo por mês é 30, por questões de alocação de funcionários. Se a proposta de acordo for rejeitada, passamos para o próximo processo. Se a proposta de acordo for aceita, o processo é encerrado e o valor do acordo é provisionado.\nVamos simular essas estratégias e ver como elas se comportam ao longo do tempo. Para cada estratégia, vamos fazer 100 simulações, para considerar a aleatoriedade dos processos judiciais. Vamos construir um aplicativo utilizando o streamlit para visualizar os resultados, com o auxílio do ChatGPT.\nO menu do streamlit possui as seguintes configurações:\n\nNúmero de tentativas por mês: número de tentativas de acordo por mês.\nOrçamento: orçamento disponível para acordos.\nAlçada: valor da alçada para a estratégia de acordo branda.\nProbabilidade mínima de derrota: probabilidade mínima de derrota para fazer acordo.\nNúmero de simulações: número de simulações a serem realizadas.\n\nLink do app: https://app-acordo.streamlit.app/ Código do app: https://github.com/jtrecenti/main-cdad2/blob/main/apostilas/10-corporativo/app_acordo.py\nA análise produz dois gráficos: o primeiro mostra os gastos acumulados ao longo do tempo para a estratégia com acordo, comparada com a estratégia sem acordo. O segundo gráfico mostra a diferença entre os gastos acumulados ao longo do tempo para a estratégia com acordo e a estratégia sem acordo.\n\n\n\nimage.png\n\n\n\n\n\nimage-2.png\n\n\nVeja que, nos primeiros 40 meses, a estratégia de acordo é mais custosa que a estratégia sem acordo. Isso acontece porque a empresa está antecipando custos com acordos que só seriam realizados no futuro. No entanto, a partir do mês 40, a estratégia de acordo começa a ser mais barata que a estratégia sem acordo.\nA empresa fica até 600 mil no negativo entre o mês 10 e mês 20, no que seria o break-even desse investimento. A partir do mês 40, a empresa começa a economizar dinheiro com a estratégia de acordo. No final do período, a empresa economiza cerca de 600 mil com a estratégia de acordo.\nAgora, mudamos o cenário para uma alçada de 80%. Com essa configuração, o gasto inicial é maior, e o gasto com acordos também. No final do período, a empresa acaba não economizando.\n\n\n\nimage-3.png\n\n\n\n\n\nimage-4.png\n\n\nFinalmente, fazemos uma simulação com maior orçamento. Consideramos orçamento de 6 milhões e 50 tentativas por mês, com a alçada de 50%. Nesse caso, a empresa economiza cerca de 1 milhão no final do período, mas com um valor negativo máximo de mais de 1 milhão ao comparar as estratégias.\n\n\n\nimage-5.png\n\n\n\n\n\nimage-6.png\n\n\n\n\n10.2.3 Efeito bumerangue\nA estratégia de acordo, na prática, apresenta ainda mais desafios. Um deles é a litigiosidade. Se a empresa adota uma estratégia de acordo que fica conhecida entre pessoas e advogados, pode ser que ela seja alvo de mais processos judiciais. Isso acontece porque as pessoas podem achar que a empresa é mais propensa a fazer acordo e, portanto, podem entrar com processos judiciais com valores mais altos. Para lidar com isso, a empresa pode adotar mecanismos de aleatorização: por exemplo, fazer acordo em apenas 50% dos processos elencados, escolhendo esses casos de forma aleatória. Isso pode reduzir a litigiosidade e aumentar a eficiência da estratégia de acordo.\n\n\n10.2.4 Parcerias com tribunais\nUma oportunidade para fazer mais acordos é realizar parcerias com os tribunais. Como os tribunais são interessados em acordos, periodicamente são realizados mutirões de conciliação, onde as partes são chamadas para tentar fazer acordo. A empresa pode se inscrever nesses mutirões e tentar fazer acordo com os autores dos processos. Nesse cenário é possível que as partes estejam mais propensas a fazer acordos, o que reduz o custo de aplicar a estratégia e evita o efeito bumerangue, já que a entidade acionadora do acordo é o tribunal, e não a empresa.\n\n\n10.2.5 Conclusão\nA estratégia de acordo é uma ferramenta poderosa para empresas que querem economizar dinheiro com processos judiciais e reduzir seu acervo de processos. No entanto, é importante considerar a alçada, o orçamento e disposição da empresa em investir nesses acordos. A ciência de dados pode ser utilizada para ajudar a empresa a decidir quais casos devem ser alvo de acordo e quais os valores praticados. No entanto, é importante lembrar que a ciência de dados é um guia e não a solução completa. A decisão final deve ser tomada uma equipe multidisciplinar, considerando todos os aspectos do problema.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Aplicações corporativas</span>"
    ]
  },
  {
    "objectID": "11-openai.html",
    "href": "11-openai.html",
    "title": "11  Classificação de documentos usando OpenAI GPT-4o-mini",
    "section": "",
    "text": "11.1 Controlando a temperatura\nO modelo GPT-4o-mini tem um parâmetro chamado “temperatura” que controla a aleatoriedade das previsões do modelo. Quanto maior a temperatura, mais aleatórias as previsões do modelo serão. Vamos ver um exemplo com temperatura igual a 1, rodando duas vezes o mesmo pedido.\nteste1 = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n      \"role\": \"user\",\n      \"content\": \"Qual o tesouro do jurista (ou seja, o diferencial quando comparado a um profissional da estatística, por exemplo) em pesquisas que envolvem ciência de dados no direito? Responda em uma frase.\"\n    }\n  ],\n  temperature=1.0\n)\n\nprint(teste1.choices[0].message.content)\n\nO tesouro do jurista em pesquisas que envolvem ciência de dados no direito reside na sua expertise em interpretar e aplicar normas jurídicas, contextualizando os resultados estatísticos dentro do arcabouço legal e ético pertinente.\nAgora, rodamos novamente o mesmo pedido. Veja que o resultado é diferente!\nteste2 = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n      \"role\": \"user\",\n      \"content\": \"Qual o tesouro do jurista (ou seja, o diferencial quando comparado a um profissional da estatística, por exemplo) em pesquisas que envolvem ciência de dados no direito? Responda em uma frase.\"\n    }\n  ],\n  temperature=1.0\n)\n\nprint(teste2.choices[0].message.content)\n\nO tesouro do jurista em pesquisas que envolvem ciência de dados no direito reside na sua capacidade de interpretar normas legais, contextos jurídicos e implicações éticas, além de aplicar conhecimentos jurídicos para analisar e contextualizar dados de forma crítica e fundamentada.\nAgora, vamos rodar o mesmo pedido, mas com temperatura igual a 0, duas vezes. O que você acha que vai acontecer?\nteste3 = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n      \"role\": \"user\",\n      \"content\": \"Qual o tesouro do jurista (ou seja, o diferencial quando comparado a um profissional da estatística, por exemplo) em pesquisas que envolvem ciência de dados no direito? Responda em uma frase.\"\n    }\n  ],\n  temperature=0.0\n)\n\nprint(teste3.choices[0].message.content)\n\nO tesouro do jurista em pesquisas que envolvem ciência de dados no direito reside na sua capacidade de interpretar e contextualizar dados dentro do arcabouço legal e ético, garantindo que as análises respeitem os princípios jurídicos e os direitos fundamentais.\nteste4 = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n      \"role\": \"user\",\n      \"content\": \"Qual o tesouro do jurista (ou seja, o diferencial quando comparado a um profissional da estatística, por exemplo) em pesquisas que envolvem ciência de dados no direito? Responda em uma frase.\"\n    }\n  ],\n  temperature=0.0\n)\n\nprint(teste4.choices[0].message.content)\n\nO tesouro do jurista em pesquisas que envolvem ciência de dados no direito reside na sua capacidade de interpretar e contextualizar normas jurídicas, garantindo que a análise estatística seja aplicada de forma ética e relevante ao sistema legal.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Classificação de documentos usando OpenAI GPT-4o-mini</span>"
    ]
  },
  {
    "objectID": "11-openai.html#desenvolvendo-prompts-para-anotação-de-documentos-jurídicos",
    "href": "11-openai.html#desenvolvendo-prompts-para-anotação-de-documentos-jurídicos",
    "title": "11  Classificação de documentos usando OpenAI GPT-4o-mini",
    "section": "11.2 Desenvolvendo prompts para anotação de documentos jurídicos",
    "text": "11.2 Desenvolvendo prompts para anotação de documentos jurídicos\nA anotação automática de documentos jurídicos utilizando modelos de linguagem aumenta significativamente a eficiência na anotação de documentos. No entanto, a qualidade dos resultados pode variar dependendo da formulação do prompt utilizado.\nTrata-se de uma área em desenvolvimento, então ainda não temos uma resposta final sobre como formular os prompts. No entanto, algumas dicas podem ser úteis:\n\nEscreva instruções claras e objetivas: O prompt deve ser o mais claro e específico possível. Experimente verificar com você ou outra pessoa se as instruções são compreensíveis e diretas.\nDeixe claro quem é o assistente e qual seu papel: O modelo de linguagem é um assistente, e é importante deixar claro qual é o papel dele na tarefa. Por exemplo, “você é um assistente jurídico que realiza a análise de petições iniciais em processos cíveis, resumindo as informações mais importantes para posterior avaliação de profissionais da advocacia”.\nDescreva o que o modelo deve esperar de entrada: Sempre que necessário, forneça contexto relevante ao modelo. Isso pode incluir a descrição do tipo de documento, o objetivo da anotação e as características específicas do caso em questão. Por exemplo, uma petição inicial de um processo é diferente de um tweet.\nEspecifique o formato de saída: Indicar o formato exato da resposta esperada é uma boa prática. Geralmente, pediremos para que a resposta seja em um JSON, e temos um parâmetro para forçar esse comportamento no modelo.\nInclua exemplos: Fornecer exemplos no prompt ajuda o modelo a entender o formato e o conteúdo esperado na resposta. O nome técnico disso é “one shot” ou “few shot” learning.\nTeste com alguns casos: Sempre verifique se o prompt está funcionando com alguns casos antes de rodar para uma amostra grande, e faça ajustes caso necessário.\nPedir um resumo: Para tarefas complexas, pode ser útil pedir para o modelo de linguagem resumir a resposta em um parágrafo ou algumas frases, antes de realizar uma anotação em categorias. Isso pode aumentar a qualidade da resposta final.\n\nObs: você pode usar o ChatGPT para ajudar na criação de prompts. Isso economiza tempo e cria prompts eficazes.\n\nprompt_ruim = \"\"\"\nClassifique o documento abaixo nessas variáveis:\n\n- faz parte do escopo?\n- quais drogas estão envolvidas?\n- quantidade de maconha em gramas\n- quantidade de cocaína em gramas\n- quantidade de crack em gramas\n- sexo da pessoa acusada\n- reincidente\n- decisão\n- tipo de pena\n- tempo da pena\n\"\"\"\n\n\nprompt_otimizado = \"\"\"\nVocê é um assistente de inteligência artificial que auxilia na anotação de sentenças judiciais do Tribunal de Justiça de São Paulo em processos envolvendo tráfico de drogas.\n\nVocê receberá o texto da sentença e deverá retornar um arquivo JSON, seguindo as regras abaixo:\n\n- O processo faz parte do escopo da pesquisa? O caso deve: Ser um caso relacionado a tráfico de drogas; Envolver porte de maconha, crack ou cocaína; Envolver apenas uma pessoa acusada.\n- Quantidade de maconha em gramas: Preencha apenas os números. Coloque 0 se o caso não envolve essa droga. Use \",\" como separador decimal. Se a decisão não menciona a quantidade em gramas, faça a conversão, seguindo a regra: 1 porção = 2 gramas.\n- Quantidade de cocaína em gramas: Preencha apenas os números. Coloque 0 se o caso não envolve essa droga. Use \",\" como separador decimal. Se a decisão não menciona a quantidade em gramas, faça a conversão, seguindo a regra: 1 porção ou pino = 0,5 grama.\n- Quantidade de crack em gramas: Preencha apenas os números. Coloque 0 se o caso não envolve essa droga. Use \",\" como separador decimal. Se a decisão não menciona a quantidade em gramas, faça a conversão, seguindo a regra: 1 porção = 0,1 grama.\n- Decisão: pode ser procedente (condenação), improcedente / punibilidade extinta, ou parcialmente procedente / advertência.\n- Tipo de pena: pode ser fechado, semiaberto ou aberto.\n- Tempo da pena (em meses): Preencha apenas os números. Converta o tempo em meses. Por exemplo, 2 anos e 7 meses = 31 meses.\n\nRetorne um arquivo JSON com as seguintes informações:\n\n{\n \"escopo\": \"sim/não\",\n \"maconha\": \"sim/não\",\n \"cocaina\": \"sim/não\",\n \"crack\": \"sim/não\",\n \"qtd_maconha\": 0,\n \"qtd_cocaina\": 0,\n \"qtd_crack\": 0,\n \"sexo\": \"masculino/feminino/não informado\",\n \"reincidente\": \"sim/não/não informado\",\n \"decisao\": \"procedente/improcedente/parcialmente procedente\",\n \"tipo_pena\": \"fechado/semiaberto/aberto\",\n \"tempo\": 0\n}\n\"\"\"\n\n\n11.2.1 Aplicando prompt em um caso\n\ndrogas = pd.read_csv(\"https://github.com/jtrecenti/main-cdad2/releases/download/data/drogas.csv\")\n\ndrogas.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 991 entries, 0 to 990\nData columns (total 14 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   file              991 non-null    object \n 1   processo          991 non-null    object \n 2   pagina            991 non-null    int64  \n 3   hora_coleta       0 non-null      float64\n 4   duplicado         991 non-null    bool   \n 5   classe            990 non-null    object \n 6   assunto           991 non-null    object \n 7   magistrado        991 non-null    object \n 8   comarca           991 non-null    object \n 9   foro              991 non-null    object \n 10  vara              991 non-null    object \n 11  disponibilizacao  991 non-null    object \n 12  julgado           991 non-null    object \n 13  cd_doc            991 non-null    object \ndtypes: bool(1), float64(1), int64(1), object(11)\nmemory usage: 101.7+ KB\n\n\n\nprocesso = '15005860220238260569'\n\njulgado = drogas[drogas['processo']==processo]['julgado'].iloc[0]\n\nprint(julgado[:2000])\n\nTRIBUNAL DE JUSTIÇA DO ESTADO DE SÃO PAULO COMARCA de Boituva Foro de Boituva 1ª Vara Rua Manoel dos Santos Freire, 161, Centro - CEP 18550-000, Fone: (15) 3263-2120, Boituva-SP - E-mail: boituva1@tjsp.jus.br 1500586-02.2023.8.26.0569 - lauda SENTENÇA Processo nº: 1500586-02.2023.8.26.0569 Classe - Assunto Ação Penal - Procedimento Ordinário - Tráfico de Drogas e Condutas Afins Autor: Justiça Pública Réu: ROSANA LEMES DE SOUZA Juiz(a) de Direito: Dr(a). Liliana Regina de Araujo Heidorn Abdala Vistos. ROSANA LEMES DE SOUZA foi denunciada como incursa no artigo 33, caput, c/c art. 40, inciso III, ambos da Lei 11.343/06, porque no dia 01 de julho de 2023, aproximadamente às 08h00min, na Estrada Tatuí/Iperó, Área Rural, Estabelecimento Prisional, na cidade de Iperó e comarca de Boituva, ROSANA LEMES DE SOUZA, qualificada a fls. 14 e indiciada a fls. 62, adquiriu e trazia consigo, para fornecer e entregar ao consumo de terceiros, 98,13g (noventa e oito gramas e treze centigramas) líquidos de maconha, acondicionados em 1 (uma) porção, conforme laudo pericial de fls. 31/33 e 106/108, substância entorpecente que determina dependência física e psíquica, sem autorização legal e regulamentar para tanto. Segundo o apurado, a denunciada estava no estabelecimento prisional de Iperó para realização de visitas, quando foi submetida à revista. Ao verificar o que tinha no inteiro de um pacote que a denunciada trazia consigo, foi localizada 1 (uma) porção de maconha no interior de 1 (um) absorvente. As circunstâncias em que se deram o flagrante, evidenciam a prática do tráfico de entorpecentes. Presa em flagrante, a ré foi beneficiada com a liberdade provisória mediante o cumprimento das medidas cautelares previstas no artigo 319, do C.P.P. (fl.89/91). Regularmente notificada (fl. 158), a ré apresentou defesa preliminar (fls.141/144). A denúncia foi recebida (fl. 150). Regularmente citada, foi realizada a instrução processual, com oitiva de duas testemunhas em comum e o interrogatório\n\n\n\nprint(julgado[-2000:])\n\nprisional, nos termos do artigo 40, inciso III, da Lei 11.343/2006, fixando-a definitivamente em 01 (um) ano, 11 (onze) meses e 10 (dez) dias, e 193 (cento e noventa e três) dias-multa. Vislumbro que a acusada faz jus ao benefício previsto nos artigos 43 e nos seguintes do Código Penal, substituo a pena privativa de liberdade em prestação de serviço à comunidade e 10 dias multa, nos termos do artigo 44, § 2º, do Código Penal, que será executada perante o Juízo das Execuções Criminais. Em caso de descumprimento, fixo o regime aberto para o início do cumprimento da pena. Pelo exposto JULGO PROCEDENTE a presente pretensão punitiva, e o faço para CONDENAR a ré ROSANA LEMES DE SOUZA à pena de 01 (um) ano, 11 (onze) meses e 10 (dez) dias, e 193 (cento e noventa e três) dias-multa, substituída por prestação de serviço à comunidade, na forma acima disposta, por infração ao artigo 33, caput, c/c art. 40, inciso III, ambos da Lei 11.343/06. Defiro recurso em liberdade. São parcas as informações acerca da situação financeira da ré, por isso arbitro o valor do dia-multa no seu mínimo legal, calculado sobre o salário mínimo da data dos fatos e atualizado até o dia do efetivo pagamento. Nos termos do artigo 32, parágrafos 1º e 2º, da Lei 11.343/06, AUTORIZO a incineração dos entorpecentes apreendidos, se ainda não providenciado. Condeno a acusada ao pagamento das custas equivalentes a 100 UFESP's, nos termos do artigo 4º, inciso III, item 5, § 9º, alínea “a” da Lei nº 11.608, de 29 de dezembro de 2003, obrigação que fica suspensa em atenção ao disposto nos artigos 11 e 12 da Lei nº 1.060/50, posto que beneficiários da justiça gratuita, pois defendidos por força do convênio Defensoria Pública/OAB. Arbitro os honorários advocatícios no valor máximo previsto em convênio. Expeça-se a devida certidão. Publicada em audiência. Saem todos intimados. Cumpra-se. Boituva, 19 de agosto de 2024. DOCUMENTO ASSINADO DIGITALMENTE NOS TERMOS DA LEI 11.419/2006, CONFORME IMPRESSÃO À MARGEM DIREITA\n\n\n\ndef classificar_decisao (prompt, julgado):\n  completion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n      {\"role\": \"system\", \"content\": prompt},\n      {\"role\": \"user\", \"content\": julgado}\n    ],\n    temperature=0.0\n  )\n  return completion.choices[0].message.content\n\n\nres_ruim = classificar_decisao(prompt_ruim, julgado)\n\nprint(res_ruim)\n\n- faz parte do escopo? Sim\n- quais drogas estão envolvidas? Maconha\n- quantidade de maconha em gramas: 98,13g\n- quantidade de cocaína em gramas: 0g\n- quantidade de crack em gramas: 0g\n- sexo da pessoa acusada: Feminino\n- reincidente: Não\n- decisão: Condenação\n- tipo de pena: Prestação de serviço à comunidade\n- tempo da pena: 1 ano, 11 meses e 10 dias\n\n\n\nres_otimizado = classificar_decisao(prompt_otimizado, julgado)\n\nprint(res_otimizado)\n\n```json\n{\n  \"escopo\": \"sim\",\n  \"maconha\": \"sim\",\n  \"cocaina\": \"não\",\n  \"crack\": \"não\",\n  \"qtd_maconha\": 98,\n  \"qtd_cocaina\": 0,\n  \"qtd_crack\": 0,\n  \"sexo\": \"feminino\",\n  \"reincidente\": \"não\",\n  \"decisao\": \"procedente\",\n  \"tipo_pena\": \"aberto\",\n  \"tempo\": 23\n}\n```",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Classificação de documentos usando OpenAI GPT-4o-mini</span>"
    ]
  },
  {
    "objectID": "11-openai.html#exercício",
    "href": "11-openai.html#exercício",
    "title": "11  Classificação de documentos usando OpenAI GPT-4o-mini",
    "section": "11.3 Exercício:",
    "text": "11.3 Exercício:\nTeste o prompt otimizado para novos processos e compare com as classificações manuais\n\nprocesso = '&lt;processo aqui&gt;'\njulgado = drogas[drogas['processo']==processo]['julgado'].iloc[0]\n\nclassificar_decisao(prompt_otimizado, julgado)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Classificação de documentos usando OpenAI GPT-4o-mini</span>"
    ]
  },
  {
    "objectID": "11-openai.html#aula-05-até-aqui",
    "href": "11-openai.html#aula-05-até-aqui",
    "title": "11  Classificação de documentos usando OpenAI GPT-4o-mini",
    "section": "11.4 Aula 05 até aqui",
    "text": "11.4 Aula 05 até aqui",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Classificação de documentos usando OpenAI GPT-4o-mini</span>"
    ]
  },
  {
    "objectID": "11-openai.html#aula-06",
    "href": "11-openai.html#aula-06",
    "title": "11  Classificação de documentos usando OpenAI GPT-4o-mini",
    "section": "11.5 Aula 06",
    "text": "11.5 Aula 06\nDá para melhorar um pouco mais ainda. Vamos forçar o resultado a um formato específico, que podemos transformar em um DataFrame python diretamente.\n\nclass JsonSchema(BaseModel):\n  escopo: str\n  maconha: str\n  cocaina: str\n  crack: str\n  qtd_maconha: float\n  qtd_cocaina: float\n  qtd_crack: float\n  sexo: str\n  reincidente: str\n  decisao: str\n  tipo_pena: str\n  tempo: float\n\ndef classificar_decisao_df (prompt, julgado):\n  completion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-mini\",\n    messages=[\n      {\"role\": \"system\", \"content\": prompt},\n      {\"role\": \"user\", \"content\": julgado}\n    ],\n    temperature=0.0,\n    response_format = JsonSchema\n  )\n  return completion.choices[0].message.parsed.dict()\n\n\nres_otimizado_df = classificar_decisao_df(prompt_otimizado, julgado)\n\nprint(res_otimizado_df)\n\n{'escopo': 'sim', 'maconha': 'sim', 'cocaina': 'não', 'crack': 'não', 'qtd_maconha': 98.0, 'qtd_cocaina': 0.0, 'qtd_crack': 0.0, 'sexo': 'feminino', 'reincidente': 'não', 'decisao': 'procedente', 'tipo_pena': 'aberto', 'tempo': 22.0}\n\n\nÉ possível converter para um DataFrame diretamente, veja:\n\npd.DataFrame([res_otimizado_df])\n\n\n\n\n\n\n\n\nescopo\nmaconha\ncocaina\ncrack\nqtd_maconha\nqtd_cocaina\nqtd_crack\nsexo\nreincidente\ndecisao\ntipo_pena\ntempo\n\n\n\n\n0\nsim\nsim\nnão\nnão\n98.0\n0.0\n0.0\nfeminino\nnão\nprocedente\naberto\n22.0\n\n\n\n\n\n\n\nAgora, podemos fazer isso para vários casos, com um laço:\n\ncasos = [\n  '00029796520178260542',\n  '15289122520238260228',\n  '15071822120248260228',\n  '15003527020248260347',\n  '15027831120238260542',\n  '15010156220248260559'\n]\n\nresultados = []\n\nfor caso in casos:\n  print(caso)\n  txt_caso = drogas[drogas['processo'] == caso]['julgado'].iloc[0]\n  res = classificar_decisao_df(prompt_otimizado, txt_caso)\n  resultados.append(res)\n\ndf_resultados = pd.DataFrame(resultados)\n\n00029796520178260542\n15289122520238260228\n15071822120248260228\n15003527020248260347\n15027831120238260542\n15010156220248260559\n\n\n\ndf_resultados\n\n\n\n\n\n\n\n\nescopo\nmaconha\ncocaina\ncrack\nqtd_maconha\nqtd_cocaina\nqtd_crack\nsexo\nreincidente\ndecisao\ntipo_pena\ntempo\n\n\n\n\n0\nsim\nsim\nsim\nnão\n63.0\n7.0\n0.0\nmasculino\nsim\nparcialmente procedente\nfechado\n84.0\n\n\n1\nsim\nsim\nsim\nnão\n3.0\n11.0\n0.0\nmasculino\nnão\nprocedente\nfechado\n32.0\n\n\n2\nsim\nsim\nsim\nnão\n51.4\n18.0\n0.0\nmasculino\nnão\nprocedente\naberto\n20.0\n\n\n3\nsim\nnão\nsim\nnão\n0.0\n13.0\n0.0\nmasculino\nnão\nprocedente\naberto\n8.0\n\n\n4\nsim\nsim\nsim\nsim\n6.0\n9.0\n4.0\nmasculino\nnão\nprocedente\naberto\n20.0\n\n\n5\nsim\nsim\nsim\nnão\n37.0\n12.0\n0.0\nmasculino\nnão\nparcialmente procedente\naberto\n32.0\n\n\n\n\n\n\n\nFinalmente, podemos salvar o resultado em um arquivo CSV:\n\ndf_resultados.to_csv(\"resultados.csv\", index = False)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Classificação de documentos usando OpenAI GPT-4o-mini</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciência de Dados Aplicada ao Direito II",
    "section": "",
    "text": "Prefácio\n\nO livro está em construção!\n\nA ideia deste livro é funcionar como material de apoio e estudos para a disciplina de Ciência de Dados no Direito II do curso de Direito do Insper.\nQuando o livro ficar pronto, poderemos utilizá-lo em outros contextos fora da sala de aula.\nOutro link de interesse do qual tirei ideias e conteúdos: https://livro.abj.org.br",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Objetivo do Livro\nEste material é uma compilação dos conteúdos abordados na disciplina, organizada de forma progressiva para facilitar o aprendizado. O livro cobre aspectos essenciais, incluindo:\nCada capítulo contém conceitos teóricos, exemplos práticos e exercícios para reforçar o aprendizado. Além disso, enfatiza-se o uso de ferramentas computacionais, como Python e bibliotecas específicas para análise de dados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#objetivo-do-livro",
    "href": "intro.html#objetivo-do-livro",
    "title": "1  Introdução",
    "section": "",
    "text": "Organização de dados e cálculo de taxas;\nMedidas de posição e variabilidade;\nVisualização de dados com Seaborn;\nConceitos de amostras e populações;\nProbabilidade e distribuição normal;\nTestes de hipóteses;\nCorrelação e regressão;\nModelos de classificação;\nAplicabilidade dos métodos estatísticos no ambiente corporativo;\nUso da OpenAI e da inteligência artificial na análise de dados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#público-alvo",
    "href": "intro.html#público-alvo",
    "title": "1  Introdução",
    "section": "1.2 Público-Alvo",
    "text": "1.2 Público-Alvo\nO livro é destinado a estudantes, profissionais e pesquisadores que desejam aprofundar seus conhecimentos em análise de dados e estatística aplicada. Embora alguns conceitos matemáticos e estatísticos sejam explorados, o material foi elaborado para ser acessível mesmo para aqueles que não possuem experiência prévia na área.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#metodologia-e-abordagem",
    "href": "intro.html#metodologia-e-abordagem",
    "title": "1  Introdução",
    "section": "1.3 Metodologia e Abordagem",
    "text": "1.3 Metodologia e Abordagem\nA metodologia adotada neste livro segue uma abordagem prática e aplicada, utilizando estudos de caso e dados reais para ilustrar os conceitos. Além disso, são apresentados códigos comentados e exemplos para facilitar a compreensão dos métodos abordados.\nEspera-se que ao final do livro, os leitores estejam aptos a:\n\nCompreender e manipular dados de forma eficiente;\nAplicar métodos estatísticos para tomada de decisão;\nDesenvolver visualizações informativas e interpretáveis;\nConstruir modelos estatísticos e de aprendizado de máquina;\nUtilizar inteligência artificial como ferramenta auxiliar na análise de dados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "resumo.html",
    "href": "resumo.html",
    "title": "12  Resumo",
    "section": "",
    "text": "Introdução gerada pelo ChatGPT\n\nAo longo deste livro, exploramos os principais conceitos e técnicas da ciência de dados aplicada ao Direito, abordando ferramentas estatísticas, computacionais e de aprendizado de máquina utilizadas na análise de dados jurídicos. Cada capítulo trouxe uma abordagem prática utilizando Python e bibliotecas especializadas para manipulação, visualização e modelagem de dados.\nInicialmente, discutimos a organização e manipulação de dados jurídicos, utilizando pandas para estruturação de bases de dados e cálculos de taxas. Exploramos medidas de posição e variabilidade, utilizando numpy e scipy para cálculos estatísticos como média, mediana, variância e desvio-padrão. Para visualização de dados, utilizamos seaborn e matplotlib, criando gráficos como histogramas, boxplots e scatterplots para melhor compreensão dos padrões nos dados.\nNos capítulos sobre amostras e populações, abordamos conceitos de inferência estatística e o uso do statsmodels para estimativas e testes de hipóteses. A teoria da probabilidade e a distribuição normal foram exploradas utilizando scipy.stats, com aplicações na construção de distribuições e cálculo de probabilidades acumuladas.\nEm testes de hipóteses, abordamos o uso do teste t de Student, testes qui-quadrado e ANOVA, aplicados com scipy.stats e statsmodels. Também discutimos correlação e regressão, abordando regressão linear simples e múltipla com scikit-learn, incluindo interpretação de coeficientes e métricas de ajuste como R² e erro quadrático médio.\nNos modelos de classificação, exploramos algoritmos como regressão logística, árvores de decisão e random forests, utilizando scikit-learn para treinamento, ajuste de hiperparâmetros e avaliação de métricas como acurácia e matriz de confusão. Além disso, discutimos técnicas de validação cruzada para garantir a robustez dos modelos.\nA aplicação de inteligência artificial no Direito foi abordada com o uso da API da OpenAI para processamento de linguagem natural, demonstrando casos de aplicação como extração de informações de textos jurídicos e categorização automatizada de documentos. Também discutimos métodos para avaliação da performance dos modelos e ajustes necessários para uso em ambientes jurídicos reais.\nPor fim, analisamos a aplicabilidade desses métodos no setor jurídico e corporativo, demonstrando casos de uso reais e ferramentas que auxiliam na análise de grandes volumes de dados jurídicos. Utilizamos técnicas de análise exploratória e modelagem preditiva para embasar decisões baseadas em dados e otimizar fluxos de trabalho.\nEste resumo consolidou os principais tópicos abordados no livro, permitindo que os leitores tenham uma visão geral das ferramentas e técnicas aplicadas na interseção entre ciência de dados e Direito.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resumo</span>"
    ]
  }
]